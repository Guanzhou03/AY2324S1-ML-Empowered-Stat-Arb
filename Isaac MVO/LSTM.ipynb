{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c9e1027",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: yfinance in c:\\users\\user\\anaconda3\\lib\\site-packages (0.2.28)\n",
      "Requirement already satisfied: pandas>=1.3.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from yfinance) (1.4.4)\n",
      "Requirement already satisfied: numpy>=1.16.5 in c:\\users\\user\\appdata\\roaming\\python\\python39\\site-packages (from yfinance) (1.24.4)\n",
      "Requirement already satisfied: requests>=2.31 in c:\\users\\user\\anaconda3\\lib\\site-packages (from yfinance) (2.31.0)\n",
      "Requirement already satisfied: multitasking>=0.0.7 in c:\\users\\user\\anaconda3\\lib\\site-packages (from yfinance) (0.0.11)\n",
      "Requirement already satisfied: lxml>=4.9.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from yfinance) (4.9.1)\n",
      "Requirement already satisfied: appdirs>=1.4.4 in c:\\users\\user\\anaconda3\\lib\\site-packages (from yfinance) (1.4.4)\n",
      "Requirement already satisfied: pytz>=2022.5 in c:\\users\\user\\anaconda3\\lib\\site-packages (from yfinance) (2023.3)\n",
      "Requirement already satisfied: frozendict>=2.3.4 in c:\\users\\user\\anaconda3\\lib\\site-packages (from yfinance) (2.3.8)\n",
      "Requirement already satisfied: beautifulsoup4>=4.11.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from yfinance) (4.11.1)\n",
      "Requirement already satisfied: html5lib>=1.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from yfinance) (1.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from beautifulsoup4>=4.11.1->yfinance) (2.3.1)\n",
      "Requirement already satisfied: six>=1.9 in c:\\users\\user\\anaconda3\\lib\\site-packages (from html5lib>=1.1->yfinance) (1.16.0)\n",
      "Requirement already satisfied: webencodings in c:\\users\\user\\anaconda3\\lib\\site-packages (from html5lib>=1.1->yfinance) (0.5.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from pandas>=1.3.0->yfinance) (2.8.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests>=2.31->yfinance) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests>=2.31->yfinance) (3.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests>=2.31->yfinance) (1.26.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests>=2.31->yfinance) (2022.9.14)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install yfinance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "057bcf24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import numpy as np\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe52ce8",
   "metadata": {},
   "source": [
    "## Stock Feature Selection: Adj Close, Volume, RSI, ATR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a9d9c5b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed\n"
     ]
    }
   ],
   "source": [
    "tickers = pd.read_csv('tickers.csv')\n",
    "tickers.sort_values(by='Market Cap', ascending=False, inplace=True)\n",
    "top18_tickers = tickers[:20]\n",
    "top18_tickers.reset_index(inplace=True)\n",
    "top18_tickers = top18_tickers.drop([7,8])\n",
    "stocks = top18_tickers['Symbol'].to_list()\n",
    "start_date = '2013-01-01'\n",
    "end_date = '2019-01-01'\n",
    "data = yf.download(\"AAPL\", start=start_date, end=end_date)\n",
    "data = data.reset_index()\n",
    "dates = data['Date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5c7b4423",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_log_returns(list_of_stocks_tickers, start_date, end_date, interval='1d'):\n",
    "    stocks = list()\n",
    "    for ticker in list_of_stocks_tickers:\n",
    "        data = yf.download(ticker, start=start_date, end=end_date, interval=interval)\n",
    "        data = {ticker: np.log(data['Adj Close']) - np.log(data['Adj Close'].shift(1))}\n",
    "        log_return = pd.DataFrame(data=data)\n",
    "        stocks.append(log_return)\n",
    "    all_stocks = reduce(lambda df1, df2: pd.merge(df1, df2, on='Date'), stocks)\n",
    "    all_stocks['Date'] = log_return.index\n",
    "    return all_stocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6b6fd5bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AAPL',\n",
       " 'MSFT',\n",
       " 'GOOG',\n",
       " 'GOOGL',\n",
       " 'AMZN',\n",
       " 'NVDA',\n",
       " 'TSLA',\n",
       " 'META',\n",
       " 'HSBC',\n",
       " 'LLY',\n",
       " 'TSM',\n",
       " 'V',\n",
       " 'UNH',\n",
       " 'XOM',\n",
       " 'JPM',\n",
       " 'WMT',\n",
       " 'NVO',\n",
       " 'JNJ']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9e6f66a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n"
     ]
    }
   ],
   "source": [
    "returns = get_log_returns(stocks, start_date, end_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9129277d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AAPL</th>\n",
       "      <th>MSFT</th>\n",
       "      <th>GOOG</th>\n",
       "      <th>GOOGL</th>\n",
       "      <th>AMZN</th>\n",
       "      <th>NVDA</th>\n",
       "      <th>TSLA</th>\n",
       "      <th>META</th>\n",
       "      <th>HSBC</th>\n",
       "      <th>LLY</th>\n",
       "      <th>TSM</th>\n",
       "      <th>V</th>\n",
       "      <th>UNH</th>\n",
       "      <th>XOM</th>\n",
       "      <th>JPM</th>\n",
       "      <th>WMT</th>\n",
       "      <th>NVO</th>\n",
       "      <th>JNJ</th>\n",
       "      <th>Date</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2013-01-02</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2013-01-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-01-03</th>\n",
       "      <td>-0.012703</td>\n",
       "      <td>-0.013487</td>\n",
       "      <td>0.000581</td>\n",
       "      <td>0.000581</td>\n",
       "      <td>0.004537</td>\n",
       "      <td>0.000786</td>\n",
       "      <td>-0.016826</td>\n",
       "      <td>-0.008248</td>\n",
       "      <td>-0.017466</td>\n",
       "      <td>0.004232</td>\n",
       "      <td>-0.000552</td>\n",
       "      <td>0.000772</td>\n",
       "      <td>-0.047883</td>\n",
       "      <td>-0.001805</td>\n",
       "      <td>-0.002018</td>\n",
       "      <td>-0.006375</td>\n",
       "      <td>-0.002898</td>\n",
       "      <td>-0.001413</td>\n",
       "      <td>2013-01-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-01-04</th>\n",
       "      <td>-0.028250</td>\n",
       "      <td>-0.018893</td>\n",
       "      <td>0.019568</td>\n",
       "      <td>0.019568</td>\n",
       "      <td>0.002589</td>\n",
       "      <td>0.032460</td>\n",
       "      <td>-0.010699</td>\n",
       "      <td>0.035029</td>\n",
       "      <td>0.006167</td>\n",
       "      <td>0.036339</td>\n",
       "      <td>-0.007212</td>\n",
       "      <td>0.008134</td>\n",
       "      <td>0.001922</td>\n",
       "      <td>0.004619</td>\n",
       "      <td>0.017570</td>\n",
       "      <td>0.003772</td>\n",
       "      <td>0.005907</td>\n",
       "      <td>0.011386</td>\n",
       "      <td>2013-01-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-01-07</th>\n",
       "      <td>-0.005900</td>\n",
       "      <td>-0.001872</td>\n",
       "      <td>-0.004373</td>\n",
       "      <td>-0.004373</td>\n",
       "      <td>0.035295</td>\n",
       "      <td>-0.029323</td>\n",
       "      <td>-0.001746</td>\n",
       "      <td>0.022689</td>\n",
       "      <td>0.001489</td>\n",
       "      <td>-0.001359</td>\n",
       "      <td>-0.014583</td>\n",
       "      <td>0.007119</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.011646</td>\n",
       "      <td>0.001101</td>\n",
       "      <td>-0.009603</td>\n",
       "      <td>0.003420</td>\n",
       "      <td>-0.002098</td>\n",
       "      <td>2013-01-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-01-08</th>\n",
       "      <td>0.002688</td>\n",
       "      <td>-0.005259</td>\n",
       "      <td>-0.001975</td>\n",
       "      <td>-0.001975</td>\n",
       "      <td>-0.007778</td>\n",
       "      <td>-0.022170</td>\n",
       "      <td>-0.019407</td>\n",
       "      <td>-0.012312</td>\n",
       "      <td>-0.010847</td>\n",
       "      <td>0.007546</td>\n",
       "      <td>-0.009080</td>\n",
       "      <td>0.009267</td>\n",
       "      <td>-0.013335</td>\n",
       "      <td>0.006236</td>\n",
       "      <td>0.001980</td>\n",
       "      <td>0.002774</td>\n",
       "      <td>0.009775</td>\n",
       "      <td>0.000140</td>\n",
       "      <td>2013-01-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-24</th>\n",
       "      <td>-0.026215</td>\n",
       "      <td>-0.042635</td>\n",
       "      <td>-0.003395</td>\n",
       "      <td>-0.006660</td>\n",
       "      <td>-0.024613</td>\n",
       "      <td>-0.019404</td>\n",
       "      <td>-0.079305</td>\n",
       "      <td>-0.007148</td>\n",
       "      <td>-0.004695</td>\n",
       "      <td>-0.024704</td>\n",
       "      <td>-0.011831</td>\n",
       "      <td>-0.020571</td>\n",
       "      <td>-0.022917</td>\n",
       "      <td>-0.039068</td>\n",
       "      <td>-0.021792</td>\n",
       "      <td>-0.015149</td>\n",
       "      <td>-0.011077</td>\n",
       "      <td>-0.041851</td>\n",
       "      <td>2018-12-24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-26</th>\n",
       "      <td>0.068053</td>\n",
       "      <td>0.066078</td>\n",
       "      <td>0.062769</td>\n",
       "      <td>0.062189</td>\n",
       "      <td>0.090254</td>\n",
       "      <td>0.046284</td>\n",
       "      <td>0.098877</td>\n",
       "      <td>0.078417</td>\n",
       "      <td>0.021079</td>\n",
       "      <td>0.039851</td>\n",
       "      <td>0.040810</td>\n",
       "      <td>0.067497</td>\n",
       "      <td>0.043720</td>\n",
       "      <td>0.046673</td>\n",
       "      <td>0.040622</td>\n",
       "      <td>0.052103</td>\n",
       "      <td>0.024254</td>\n",
       "      <td>0.031018</td>\n",
       "      <td>2018-12-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-27</th>\n",
       "      <td>-0.006511</td>\n",
       "      <td>0.006146</td>\n",
       "      <td>0.004243</td>\n",
       "      <td>0.004808</td>\n",
       "      <td>-0.006315</td>\n",
       "      <td>-0.014607</td>\n",
       "      <td>-0.031020</td>\n",
       "      <td>0.002531</td>\n",
       "      <td>-0.012938</td>\n",
       "      <td>0.016073</td>\n",
       "      <td>0.000544</td>\n",
       "      <td>0.013576</td>\n",
       "      <td>0.010709</td>\n",
       "      <td>0.004361</td>\n",
       "      <td>0.011192</td>\n",
       "      <td>0.012967</td>\n",
       "      <td>0.011472</td>\n",
       "      <td>0.005509</td>\n",
       "      <td>2018-12-27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-28</th>\n",
       "      <td>0.000512</td>\n",
       "      <td>-0.007838</td>\n",
       "      <td>-0.006535</td>\n",
       "      <td>-0.005925</td>\n",
       "      <td>0.011144</td>\n",
       "      <td>0.018730</td>\n",
       "      <td>0.054598</td>\n",
       "      <td>-0.009861</td>\n",
       "      <td>0.009050</td>\n",
       "      <td>0.011537</td>\n",
       "      <td>0.009740</td>\n",
       "      <td>-0.008139</td>\n",
       "      <td>0.001097</td>\n",
       "      <td>-0.011232</td>\n",
       "      <td>-0.002166</td>\n",
       "      <td>0.005879</td>\n",
       "      <td>0.005250</td>\n",
       "      <td>-0.001100</td>\n",
       "      <td>2018-12-28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-31</th>\n",
       "      <td>0.009619</td>\n",
       "      <td>0.011685</td>\n",
       "      <td>-0.001418</td>\n",
       "      <td>-0.001645</td>\n",
       "      <td>0.016074</td>\n",
       "      <td>-0.001123</td>\n",
       "      <td>-0.003210</td>\n",
       "      <td>-0.015968</td>\n",
       "      <td>0.000974</td>\n",
       "      <td>0.013222</td>\n",
       "      <td>-0.006212</td>\n",
       "      <td>0.007608</td>\n",
       "      <td>0.011628</td>\n",
       "      <td>0.000293</td>\n",
       "      <td>0.008126</td>\n",
       "      <td>0.011011</td>\n",
       "      <td>0.005223</td>\n",
       "      <td>0.013889</td>\n",
       "      <td>2018-12-31</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1510 rows × 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                AAPL      MSFT      GOOG     GOOGL      AMZN      NVDA  \\\n",
       "Date                                                                     \n",
       "2013-01-02       NaN       NaN       NaN       NaN       NaN       NaN   \n",
       "2013-01-03 -0.012703 -0.013487  0.000581  0.000581  0.004537  0.000786   \n",
       "2013-01-04 -0.028250 -0.018893  0.019568  0.019568  0.002589  0.032460   \n",
       "2013-01-07 -0.005900 -0.001872 -0.004373 -0.004373  0.035295 -0.029323   \n",
       "2013-01-08  0.002688 -0.005259 -0.001975 -0.001975 -0.007778 -0.022170   \n",
       "...              ...       ...       ...       ...       ...       ...   \n",
       "2018-12-24 -0.026215 -0.042635 -0.003395 -0.006660 -0.024613 -0.019404   \n",
       "2018-12-26  0.068053  0.066078  0.062769  0.062189  0.090254  0.046284   \n",
       "2018-12-27 -0.006511  0.006146  0.004243  0.004808 -0.006315 -0.014607   \n",
       "2018-12-28  0.000512 -0.007838 -0.006535 -0.005925  0.011144  0.018730   \n",
       "2018-12-31  0.009619  0.011685 -0.001418 -0.001645  0.016074 -0.001123   \n",
       "\n",
       "                TSLA      META      HSBC       LLY       TSM         V  \\\n",
       "Date                                                                     \n",
       "2013-01-02       NaN       NaN       NaN       NaN       NaN       NaN   \n",
       "2013-01-03 -0.016826 -0.008248 -0.017466  0.004232 -0.000552  0.000772   \n",
       "2013-01-04 -0.010699  0.035029  0.006167  0.036339 -0.007212  0.008134   \n",
       "2013-01-07 -0.001746  0.022689  0.001489 -0.001359 -0.014583  0.007119   \n",
       "2013-01-08 -0.019407 -0.012312 -0.010847  0.007546 -0.009080  0.009267   \n",
       "...              ...       ...       ...       ...       ...       ...   \n",
       "2018-12-24 -0.079305 -0.007148 -0.004695 -0.024704 -0.011831 -0.020571   \n",
       "2018-12-26  0.098877  0.078417  0.021079  0.039851  0.040810  0.067497   \n",
       "2018-12-27 -0.031020  0.002531 -0.012938  0.016073  0.000544  0.013576   \n",
       "2018-12-28  0.054598 -0.009861  0.009050  0.011537  0.009740 -0.008139   \n",
       "2018-12-31 -0.003210 -0.015968  0.000974  0.013222 -0.006212  0.007608   \n",
       "\n",
       "                 UNH       XOM       JPM       WMT       NVO       JNJ  \\\n",
       "Date                                                                     \n",
       "2013-01-02       NaN       NaN       NaN       NaN       NaN       NaN   \n",
       "2013-01-03 -0.047883 -0.001805 -0.002018 -0.006375 -0.002898 -0.001413   \n",
       "2013-01-04  0.001922  0.004619  0.017570  0.003772  0.005907  0.011386   \n",
       "2013-01-07  0.000000 -0.011646  0.001101 -0.009603  0.003420 -0.002098   \n",
       "2013-01-08 -0.013335  0.006236  0.001980  0.002774  0.009775  0.000140   \n",
       "...              ...       ...       ...       ...       ...       ...   \n",
       "2018-12-24 -0.022917 -0.039068 -0.021792 -0.015149 -0.011077 -0.041851   \n",
       "2018-12-26  0.043720  0.046673  0.040622  0.052103  0.024254  0.031018   \n",
       "2018-12-27  0.010709  0.004361  0.011192  0.012967  0.011472  0.005509   \n",
       "2018-12-28  0.001097 -0.011232 -0.002166  0.005879  0.005250 -0.001100   \n",
       "2018-12-31  0.011628  0.000293  0.008126  0.011011  0.005223  0.013889   \n",
       "\n",
       "                 Date  \n",
       "Date                   \n",
       "2013-01-02 2013-01-02  \n",
       "2013-01-03 2013-01-03  \n",
       "2013-01-04 2013-01-04  \n",
       "2013-01-07 2013-01-07  \n",
       "2013-01-08 2013-01-08  \n",
       "...               ...  \n",
       "2018-12-24 2018-12-24  \n",
       "2018-12-26 2018-12-26  \n",
       "2018-12-27 2018-12-27  \n",
       "2018-12-28 2018-12-28  \n",
       "2018-12-31 2018-12-31  \n",
       "\n",
       "[1510 rows x 19 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6c775bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_rsi(prices, n=14):\n",
    "    deltas = np.diff(prices)\n",
    "    seed = deltas[:n+1]\n",
    "    up = seed[seed >= 0].sum()/n\n",
    "    down = -seed[seed < 0].sum()/n\n",
    "    rs = up/down\n",
    "    rsi = np.zeros_like(prices)\n",
    "    rsi[:n] = 100. - 100./(1.+rs)\n",
    "\n",
    "    for i in range(n, len(prices)):\n",
    "        delta = deltas[i-1]  # The diff is 1 shorter\n",
    "\n",
    "        if delta > 0:\n",
    "            upval = delta\n",
    "            downval = 0.\n",
    "        else:\n",
    "            upval = 0.\n",
    "            downval = -delta\n",
    "\n",
    "        up = (up*(n-1) + upval)/n\n",
    "        down = (down*(n-1) + downval)/n\n",
    "\n",
    "        rs = up/down\n",
    "        rsi[i] = 100. - 100./(1.+rs)\n",
    "\n",
    "    return rsi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a06ac46b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_rsi(data, period=14):\n",
    "    # Calculate price differences\n",
    "    price_diff = np.diff(data)\n",
    "    \n",
    "    # Calculate initial seed values\n",
    "    seed = price_diff[:period + 1]\n",
    "    positive_seed = seed[seed >= 0].sum()\n",
    "    negative_seed = -seed[seed < 0].sum()\n",
    "    \n",
    "    # Calculate initial RS and RSI\n",
    "    initial_rs = positive_seed / negative_seed\n",
    "    initial_rsi = 100 - 100 / (1 + initial_rs)\n",
    "    \n",
    "    # Initialize up and down values\n",
    "    up = positive_seed / period\n",
    "    down = negative_seed / period\n",
    "    \n",
    "    # Initialize RSI array\n",
    "    rsi = np.zeros_like(data)\n",
    "    rsi[:period] = initial_rsi\n",
    "\n",
    "    for i in range(period, len(data)):\n",
    "        # Calculate delta for the current period\n",
    "        delta = price_diff[i - 1]\n",
    "        \n",
    "        if delta > 0:\n",
    "            upval = delta\n",
    "            downval = 0.\n",
    "        else:\n",
    "            upval = 0.\n",
    "            downval = -delta\n",
    "        \n",
    "        # Update up and down values\n",
    "        up = (up * (period - 1) + upval) / period\n",
    "        down = (down * (period - 1) + downval) / period\n",
    "        \n",
    "        # Calculate RS and RSI for the current period\n",
    "        current_rs = up / down\n",
    "        rsi[i] = 100 - 100 / (1 + current_rs)\n",
    "\n",
    "    return rsi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5327a2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_atr(data, period=14):\n",
    "    data = np.array(data)\n",
    "    \n",
    "    high = data[:, 0]\n",
    "    low = data[:, 1]\n",
    "    close = data[:, 2]\n",
    "    \n",
    "    tr = np.maximum(high - low, np.abs(high - np.roll(close, 1)), np.abs(low - np.roll(close, 1)))\n",
    "    \n",
    "    atr = np.zeros_like(tr)\n",
    "    atr[period] = np.mean(tr[:period])\n",
    "    \n",
    "    for i in range(period + 1, len(tr)):\n",
    "        atr[i] = ((period - 1) * atr[i - 1] + tr[i]) / period\n",
    "    \n",
    "    return atr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf7e7307",
   "metadata": {},
   "source": [
    "## Model Param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "13669c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"BATCH_SIZE\": 50,\n",
    "    \"EPOCHS\": 10,\n",
    "    \"LR\": 0.00010000,\n",
    "    \"TIME_STEPS\": 60\n",
    "    }\n",
    "\n",
    "TIME_STEPS = params['TIME_STEPS']\n",
    "BATCH_SIZE = params['BATCH_SIZE']\n",
    "\n",
    "\n",
    "def build_timeseries(mat, y_col_index):\n",
    "    \n",
    "    dim_0 = mat.shape[0] - TIME_STEPS\n",
    "    dim_1 = mat.shape[1]\n",
    "\n",
    "    x = np.zeros((dim_0, TIME_STEPS, dim_1))\n",
    "    y = np.zeros((dim_0,))\n",
    "\n",
    "    print(\"Length of inputs\", dim_0)\n",
    "\n",
    "    for i in range(dim_0):\n",
    "        x[i] = mat[i:TIME_STEPS+i]\n",
    "        y[i] = mat[TIME_STEPS+i, y_col_index]\n",
    "\n",
    "    print(\"length of time-series - inputs\", x.shape)\n",
    "    print(\"length of time-series - outputs\", y.shape)\n",
    "\n",
    "\n",
    "    return x, y\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71371983",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a3ff5288",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras_tuner in c:\\users\\user\\anaconda3\\lib\\site-packages (1.4.0)\n",
      "Requirement already satisfied: keras-core in c:\\users\\user\\anaconda3\\lib\\site-packages (from keras_tuner) (0.1.7)\n",
      "Requirement already satisfied: packaging in c:\\users\\user\\anaconda3\\lib\\site-packages (from keras_tuner) (21.3)\n",
      "Requirement already satisfied: requests in c:\\users\\user\\anaconda3\\lib\\site-packages (from keras_tuner) (2.31.0)\n",
      "Requirement already satisfied: kt-legacy in c:\\users\\user\\anaconda3\\lib\\site-packages (from keras_tuner) (1.0.5)\n",
      "Requirement already satisfied: absl-py in c:\\users\\user\\anaconda3\\lib\\site-packages (from keras-core->keras_tuner) (1.4.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\user\\appdata\\roaming\\python\\python39\\site-packages (from keras-core->keras_tuner) (1.24.4)\n",
      "Requirement already satisfied: rich in c:\\users\\user\\anaconda3\\lib\\site-packages (from keras-core->keras_tuner) (13.5.3)\n",
      "Requirement already satisfied: namex in c:\\users\\user\\anaconda3\\lib\\site-packages (from keras-core->keras_tuner) (0.0.7)\n",
      "Requirement already satisfied: h5py in c:\\users\\user\\anaconda3\\lib\\site-packages (from keras-core->keras_tuner) (3.7.0)\n",
      "Requirement already satisfied: dm-tree in c:\\users\\user\\anaconda3\\lib\\site-packages (from keras-core->keras_tuner) (0.1.8)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from packaging->keras_tuner) (3.0.9)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests->keras_tuner) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests->keras_tuner) (3.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests->keras_tuner) (1.26.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests->keras_tuner) (2022.9.14)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from rich->keras-core->keras_tuner) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from rich->keras-core->keras_tuner) (2.16.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras-core->keras_tuner) (0.1.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install keras_tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "dd0cdadb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from keras.models import Model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, LSTM, Input, Activation,concatenate, Attention, Bidirectional,GlobalAveragePooling1D\n",
    "from keras import optimizers\n",
    "from keras.preprocessing.sequence import TimeseriesGenerator\n",
    "from keras.layers import LeakyReLU\n",
    "import matplotlib.pyplot as plt\n",
    "import keras_tuner as kt\n",
    "plt.style.use('fivethirtyeight')\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "96234952",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_builder(hyperparameters):\n",
    "    lstm_model = Sequential()\n",
    "    \n",
    "    hyperparameters_units = hyperparameters.Int('units', min_value=50, max_value=100, step=TIME_STEPS)\n",
    "    lstm_model.add(LSTM(units=hyperparameters_units, \n",
    "                        input_shape=(x_t.shape[1], x_t.shape[2]), \n",
    "                        return_sequences=True,\n",
    "                        kernel_initializer='he_normal'))\n",
    "    lstm_model.add(GlobalAveragePooling1D())\n",
    "    lstm_model.add(Dense(60,activation='relu'))\n",
    "    lstm_model.add(Dense(20,activation='relu'))\n",
    "    lstm_model.add(Dropout(0.05))\n",
    "    lstm_model.add(Dense(1, activation='linear'))\n",
    "    \n",
    "    hyperparameters_learning_rate = hyperparameters.Choice('learning_rate', values=[0.01, 0.05, 0.1])\n",
    "    \n",
    "    lstm_model.compile(loss='mean_squared_error', optimizer=keras.optimizers.Adam(learning_rate=hyperparameters_learning_rate))\n",
    "    \n",
    "    return lstm_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c9c990c4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed\n",
      "            RSI       ATR       Date     Volume   Returns\n",
      "1     50.885506  0.518319 2013-01-03  352965200 -0.012703\n",
      "2     45.305253  0.513950 2013-01-04  594333600 -0.028250\n",
      "3     44.232810  0.513208 2013-01-07  484156400 -0.005900\n",
      "4     44.872080  0.503693 2013-01-08  458707200  0.002688\n",
      "5     41.863144  0.490725 2013-01-09  407604400 -0.015752\n",
      "...         ...       ...        ...        ...       ...\n",
      "1505  22.983638  1.395621 2018-12-24  148676800 -0.026215\n",
      "1506  36.492605  1.483613 2018-12-26  234330000  0.068053\n",
      "1507  35.825023  1.497283 2018-12-27  212468400 -0.006511\n",
      "1508  35.924070  1.461227 2018-12-28  169165600  0.000512\n",
      "1509  37.872294  1.412747 2018-12-31  140014000  0.009619\n",
      "\n",
      "[1509 rows x 5 columns]\n",
      "1198\n",
      "[[ 5.08855061e+01  5.18319294e-01  3.52965200e+08 -1.27027104e-02]\n",
      " [ 4.53052532e+01  5.13949615e-01  5.94333600e+08 -2.82499616e-02]\n",
      " [ 4.42328105e+01  5.13208253e-01  4.84156400e+08 -5.89961289e-03]\n",
      " ...\n",
      " [ 4.67901947e+01  5.68823932e-01  8.59928000e+07  1.75808513e-04]\n",
      " [ 4.82719458e+01  5.52658122e-01  6.59208000e+07  2.80987110e-03]\n",
      " [ 4.32704472e+01  5.37646740e-01  1.03999600e+08 -1.08726587e-02]]\n",
      "Length of inputs 1198\n",
      "length of time-series - inputs (1198, 60, 4)\n",
      "length of time-series - outputs (1198,)\n",
      "Training Size (1198, 60, 4) (1198,)\n",
      "Length of inputs 251\n",
      "length of time-series - inputs (251, 60, 4)\n",
      "length of time-series - outputs (251,)\n",
      "Test Size (251, 60, 4) (251,)\n",
      "Reloading Tuner from .\\untitled_project\\tuner0.json\n",
      "WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_23460\\465918227.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_stock.Date = pd.to_datetime(df_stock.Date)\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\base.py:443: UserWarning: X has feature names, but MinMaxScaler was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).layer_with_weights-1.cell.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).layer_with_weights-1.cell.recurrent_kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).layer_with_weights-1.cell.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.19\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.20\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 60, 452)           826256    \n",
      "                                                                 \n",
      " global_average_pooling1d (  (None, 452)               0         \n",
      " GlobalAveragePooling1D)                                         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 60)                27180     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 20)                1220      \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 20)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 21        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 854677 (3.26 MB)\n",
      "Trainable params: 854677 (3.26 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "AAPL None\n",
      "Epoch 1/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 2.0493WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 5s 151ms/step - loss: 2.0493\n",
      "Epoch 2/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0944WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 3s 146ms/step - loss: 0.0944\n",
      "Epoch 3/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0901WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 4s 148ms/step - loss: 0.0901\n",
      "Epoch 4/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0412WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 4s 146ms/step - loss: 0.0412\n",
      "Epoch 5/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0436WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 3s 145ms/step - loss: 0.0436\n",
      "Epoch 6/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0440WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 3s 145ms/step - loss: 0.0440\n",
      "Epoch 7/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0368WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 3s 145ms/step - loss: 0.0368\n",
      "Epoch 8/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0371WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 4s 149ms/step - loss: 0.0371\n",
      "Epoch 9/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0354WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 4s 149ms/step - loss: 0.0354\n",
      "Epoch 10/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0337WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 4s 146ms/step - loss: 0.0337\n",
      "6/6 [==============================] - 0s 47ms/step\n",
      "[[0.31490162]\n",
      " [0.31437948]\n",
      " [0.31404272]\n",
      " [0.3144419 ]\n",
      " [0.31379738]\n",
      " [0.31379294]\n",
      " [0.3137618 ]\n",
      " [0.31376272]\n",
      " [0.3131887 ]\n",
      " [0.312196  ]\n",
      " [0.31280962]\n",
      " [0.3132714 ]\n",
      " [0.3134903 ]\n",
      " [0.31294972]\n",
      " [0.3132381 ]\n",
      " [0.3138081 ]\n",
      " [0.3151203 ]\n",
      " [0.31602135]\n",
      " [0.31582975]\n",
      " [0.317034  ]\n",
      " [0.31746808]\n",
      " [0.3182305 ]\n",
      " [0.3184417 ]\n",
      " [0.31813625]\n",
      " [0.31769606]\n",
      " [0.31767982]\n",
      " [0.31703675]\n",
      " [0.31622022]\n",
      " [0.31623173]\n",
      " [0.31645548]\n",
      " [0.3163178 ]\n",
      " [0.31607357]\n",
      " [0.31591696]\n",
      " [0.31619856]\n",
      " [0.31574613]\n",
      " [0.31501487]\n",
      " [0.31473485]\n",
      " [0.31485924]\n",
      " [0.31675792]\n",
      " [0.31698516]\n",
      " [0.3171506 ]\n",
      " [0.3174191 ]\n",
      " [0.31759226]\n",
      " [0.31793207]\n",
      " [0.3175158 ]\n",
      " [0.3176698 ]\n",
      " [0.3175796 ]\n",
      " [0.3172597 ]\n",
      " [0.3168329 ]\n",
      " [0.31665045]\n",
      " [0.31626916]\n",
      " [0.3161051 ]\n",
      " [0.31588668]\n",
      " [0.31592202]\n",
      " [0.31561106]\n",
      " [0.3144015 ]\n",
      " [0.3153531 ]\n",
      " [0.31511864]\n",
      " [0.31450456]\n",
      " [0.31453976]\n",
      " [0.31474447]\n",
      " [0.3148216 ]\n",
      " [0.31468287]\n",
      " [0.31436744]\n",
      " [0.31418243]\n",
      " [0.3139337 ]\n",
      " [0.31362525]\n",
      " [0.31341162]\n",
      " [0.3129555 ]\n",
      " [0.3140434 ]\n",
      " [0.31466806]\n",
      " [0.3142575 ]\n",
      " [0.3140512 ]\n",
      " [0.31398335]\n",
      " [0.31382868]\n",
      " [0.31453696]\n",
      " [0.3158471 ]\n",
      " [0.3155159 ]\n",
      " [0.31548464]\n",
      " [0.3157473 ]\n",
      " [0.31586418]\n",
      " [0.31514657]\n",
      " [0.31556702]\n",
      " [0.31789294]\n",
      " [0.31985942]\n",
      " [0.3204359 ]\n",
      " [0.32106948]\n",
      " [0.32142287]\n",
      " [0.32206964]\n",
      " [0.32230976]\n",
      " [0.32240754]\n",
      " [0.32246158]\n",
      " [0.3225211 ]\n",
      " [0.32231513]\n",
      " [0.3221719 ]\n",
      " [0.32199207]\n",
      " [0.32184777]\n",
      " [0.3217344 ]\n",
      " [0.3215878 ]\n",
      " [0.32127744]\n",
      " [0.32197785]\n",
      " [0.32185453]\n",
      " [0.32174686]\n",
      " [0.32139596]\n",
      " [0.32108426]\n",
      " [0.32078177]\n",
      " [0.32058516]\n",
      " [0.32023832]\n",
      " [0.3205169 ]\n",
      " [0.32039815]\n",
      " [0.32005036]\n",
      " [0.3194008 ]\n",
      " [0.31968734]\n",
      " [0.3192033 ]\n",
      " [0.3193133 ]\n",
      " [0.31937504]\n",
      " [0.32005104]\n",
      " [0.3208207 ]\n",
      " [0.32196274]\n",
      " [0.32199794]\n",
      " [0.32207143]\n",
      " [0.3222442 ]\n",
      " [0.32221967]\n",
      " [0.3225877 ]\n",
      " [0.32237253]\n",
      " [0.3226054 ]\n",
      " [0.32248756]\n",
      " [0.3223648 ]\n",
      " [0.3222629 ]\n",
      " [0.32194027]\n",
      " [0.32165447]\n",
      " [0.32129118]\n",
      " [0.32102922]\n",
      " [0.3205629 ]\n",
      " [0.32040653]\n",
      " [0.32058245]\n",
      " [0.32053155]\n",
      " [0.32086095]\n",
      " [0.32072026]\n",
      " [0.32046485]\n",
      " [0.32058576]\n",
      " [0.32095754]\n",
      " [0.3211597 ]\n",
      " [0.32178766]\n",
      " [0.3221787 ]\n",
      " [0.32232744]\n",
      " [0.3224878 ]\n",
      " [0.32227919]\n",
      " [0.3220029 ]\n",
      " [0.3216894 ]\n",
      " [0.32156393]\n",
      " [0.32106346]\n",
      " [0.32088304]\n",
      " [0.320405  ]\n",
      " [0.32030472]\n",
      " [0.31998223]\n",
      " [0.3197339 ]\n",
      " [0.3196003 ]\n",
      " [0.31934854]\n",
      " [0.31924585]\n",
      " [0.31883717]\n",
      " [0.31849205]\n",
      " [0.3179501 ]\n",
      " [0.31797945]\n",
      " [0.31801477]\n",
      " [0.31809896]\n",
      " [0.31781173]\n",
      " [0.31747052]\n",
      " [0.31700355]\n",
      " [0.31700408]\n",
      " [0.31686777]\n",
      " [0.3161225 ]\n",
      " [0.31630522]\n",
      " [0.31564832]\n",
      " [0.31503153]\n",
      " [0.31523508]\n",
      " [0.31561464]\n",
      " [0.3153776 ]\n",
      " [0.3157303 ]\n",
      " [0.31522116]\n",
      " [0.31621453]\n",
      " [0.3169277 ]\n",
      " [0.31724516]\n",
      " [0.3171803 ]\n",
      " [0.31787965]\n",
      " [0.3178679 ]\n",
      " [0.31871822]\n",
      " [0.3186285 ]\n",
      " [0.31870764]\n",
      " [0.3183981 ]\n",
      " [0.317674  ]\n",
      " [0.31796038]\n",
      " [0.3178961 ]\n",
      " [0.3172751 ]\n",
      " [0.31721148]\n",
      " [0.31702322]\n",
      " [0.31703305]\n",
      " [0.3168887 ]\n",
      " [0.3168273 ]\n",
      " [0.31689778]\n",
      " [0.31668448]\n",
      " [0.31636336]\n",
      " [0.31602386]\n",
      " [0.31733176]\n",
      " [0.31767234]\n",
      " [0.31822217]\n",
      " [0.3199924 ]\n",
      " [0.32104617]\n",
      " [0.32118464]\n",
      " [0.3206949 ]\n",
      " [0.320751  ]\n",
      " [0.32068878]\n",
      " [0.32032117]\n",
      " [0.32026154]\n",
      " [0.3201751 ]\n",
      " [0.31992972]\n",
      " [0.3198083 ]\n",
      " [0.31988487]\n",
      " [0.31987837]\n",
      " [0.32038745]\n",
      " [0.3203028 ]\n",
      " [0.32010266]\n",
      " [0.3198597 ]\n",
      " [0.31951702]\n",
      " [0.31931466]\n",
      " [0.31924984]\n",
      " [0.3195023 ]\n",
      " [0.3201558 ]\n",
      " [0.32017878]\n",
      " [0.31990448]\n",
      " [0.32011536]\n",
      " [0.32098675]\n",
      " [0.3210976 ]\n",
      " [0.32167065]\n",
      " [0.32198015]\n",
      " [0.32246503]\n",
      " [0.32259136]\n",
      " [0.32275534]\n",
      " [0.32306096]\n",
      " [0.32286853]\n",
      " [0.3228864 ]\n",
      " [0.32278782]\n",
      " [0.32248476]\n",
      " [0.32272232]\n",
      " [0.322578  ]\n",
      " [0.32271764]\n",
      " [0.3227222 ]\n",
      " [0.3225312 ]\n",
      " [0.32245407]\n",
      " [0.32235637]\n",
      " [0.32217488]]\n",
      "[0.31490162 0.31437948 0.31404272 0.3144419  0.31379738 0.31379294\n",
      " 0.3137618  0.31376272 0.3131887  0.312196   0.31280962 0.3132714\n",
      " 0.3134903  0.31294972 0.3132381  0.3138081  0.3151203  0.31602135\n",
      " 0.31582975 0.317034   0.31746808 0.3182305  0.3184417  0.31813625\n",
      " 0.31769606 0.31767982 0.31703675 0.31622022 0.31623173 0.31645548\n",
      " 0.3163178  0.31607357 0.31591696 0.31619856 0.31574613 0.31501487\n",
      " 0.31473485 0.31485924 0.31675792 0.31698516 0.3171506  0.3174191\n",
      " 0.31759226 0.31793207 0.3175158  0.3176698  0.3175796  0.3172597\n",
      " 0.3168329  0.31665045 0.31626916 0.3161051  0.31588668 0.31592202\n",
      " 0.31561106 0.3144015  0.3153531  0.31511864 0.31450456 0.31453976\n",
      " 0.31474447 0.3148216  0.31468287 0.31436744 0.31418243 0.3139337\n",
      " 0.31362525 0.31341162 0.3129555  0.3140434  0.31466806 0.3142575\n",
      " 0.3140512  0.31398335 0.31382868 0.31453696 0.3158471  0.3155159\n",
      " 0.31548464 0.3157473  0.31586418 0.31514657 0.31556702 0.31789294\n",
      " 0.31985942 0.3204359  0.32106948 0.32142287 0.32206964 0.32230976\n",
      " 0.32240754 0.32246158 0.3225211  0.32231513 0.3221719  0.32199207\n",
      " 0.32184777 0.3217344  0.3215878  0.32127744 0.32197785 0.32185453\n",
      " 0.32174686 0.32139596 0.32108426 0.32078177 0.32058516 0.32023832\n",
      " 0.3205169  0.32039815 0.32005036 0.3194008  0.31968734 0.3192033\n",
      " 0.3193133  0.31937504 0.32005104 0.3208207  0.32196274 0.32199794\n",
      " 0.32207143 0.3222442  0.32221967 0.3225877  0.32237253 0.3226054\n",
      " 0.32248756 0.3223648  0.3222629  0.32194027 0.32165447 0.32129118\n",
      " 0.32102922 0.3205629  0.32040653 0.32058245 0.32053155 0.32086095\n",
      " 0.32072026 0.32046485 0.32058576 0.32095754 0.3211597  0.32178766\n",
      " 0.3221787  0.32232744 0.3224878  0.32227919 0.3220029  0.3216894\n",
      " 0.32156393 0.32106346 0.32088304 0.320405   0.32030472 0.31998223\n",
      " 0.3197339  0.3196003  0.31934854 0.31924585 0.31883717 0.31849205\n",
      " 0.3179501  0.31797945 0.31801477 0.31809896 0.31781173 0.31747052\n",
      " 0.31700355 0.31700408 0.31686777 0.3161225  0.31630522 0.31564832\n",
      " 0.31503153 0.31523508 0.31561464 0.3153776  0.3157303  0.31522116\n",
      " 0.31621453 0.3169277  0.31724516 0.3171803  0.31787965 0.3178679\n",
      " 0.31871822 0.3186285  0.31870764 0.3183981  0.317674   0.31796038\n",
      " 0.3178961  0.3172751  0.31721148 0.31702322 0.31703305 0.3168887\n",
      " 0.3168273  0.31689778 0.31668448 0.31636336 0.31602386 0.31733176\n",
      " 0.31767234 0.31822217 0.3199924  0.32104617 0.32118464 0.3206949\n",
      " 0.320751   0.32068878 0.32032117 0.32026154 0.3201751  0.31992972\n",
      " 0.3198083  0.31988487 0.31987837 0.32038745 0.3203028  0.32010266\n",
      " 0.3198597  0.31951702 0.31931466 0.31924984 0.3195023  0.3201558\n",
      " 0.32017878 0.31990448 0.32011536 0.32098675 0.3210976  0.32167065\n",
      " 0.32198015 0.32246503 0.32259136 0.32275534 0.32306096 0.32286853\n",
      " 0.3228864  0.32278782 0.32248476 0.32272232 0.322578   0.32271764\n",
      " 0.3227222  0.3225312  0.32245407 0.32235637 0.32217488]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AAPL 0.4132395671727298\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "            RSI       ATR       Date    Volume   Returns\n",
      "1     50.596832  0.513319 2013-01-03  48294400 -0.013487\n",
      "2     45.127571  0.520225 2013-01-04  52521100 -0.018893\n",
      "3     44.618429  0.500209 2013-01-07  37110400 -0.001872\n",
      "4     43.150168  0.488051 2013-01-08  44703100 -0.005259\n",
      "5     45.229713  0.467476 2013-01-09  49047900  0.005634\n",
      "...         ...       ...        ...       ...       ...\n",
      "1505  29.476148  3.722587 2018-12-24  43935200 -0.042635\n",
      "1506  43.112585  3.937402 2018-12-26  51634800  0.066078\n",
      "1507  44.232308  3.998302 2018-12-27  49498500  0.006146\n",
      "1508  43.069064  3.919138 2018-12-28  38196300 -0.007838\n",
      "1509  45.379682  3.782771 2018-12-31  33173800  0.011685\n",
      "\n",
      "[1509 rows x 5 columns]\n",
      "1198\n",
      "[[ 5.05968318e+01  5.13319424e-01  4.82944000e+07 -1.34866373e-02]\n",
      " [ 4.51275712e+01  5.20225223e-01  5.25211000e+07 -1.88929878e-02]\n",
      " [ 4.46184291e+01  5.00209120e-01  3.71104000e+07 -1.87150602e-03]\n",
      " ...\n",
      " [ 5.88216209e+01  1.17761943e+00  1.46780000e+07  3.62352434e-03]\n",
      " [ 5.88745105e+01  1.12064642e+00  1.05943000e+07  1.16669558e-04]\n",
      " [ 5.74444093e+01  1.07988618e+00  1.87174000e+07 -2.10213749e-03]]\n",
      "Length of inputs 1198\n",
      "length of time-series - inputs (1198, 60, 4)\n",
      "length of time-series - outputs (1198,)\n",
      "Training Size (1198, 60, 4) (1198,)\n",
      "Length of inputs 251\n",
      "length of time-series - inputs (251, 60, 4)\n",
      "length of time-series - outputs (251,)\n",
      "Test Size (251, 60, 4) (251,)\n",
      "Reloading Tuner from .\\untitled_project\\tuner0.json\n",
      "WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_23460\\465918227.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_stock.Date = pd.to_datetime(df_stock.Date)\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\base.py:443: UserWarning: X has feature names, but MinMaxScaler was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).layer_with_weights-1.cell.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).layer_with_weights-1.cell.recurrent_kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).layer_with_weights-1.cell.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.19\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.20\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 60, 452)           826256    \n",
      "                                                                 \n",
      " global_average_pooling1d (  (None, 452)               0         \n",
      " GlobalAveragePooling1D)                                         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 60)                27180     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 20)                1220      \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 20)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 21        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 854677 (3.26 MB)\n",
      "Trainable params: 854677 (3.26 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "MSFT None\n",
      "Epoch 1/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 1.3124WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 5s 148ms/step - loss: 1.3124\n",
      "Epoch 2/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.1155WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 4s 148ms/step - loss: 0.1155\n",
      "Epoch 3/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0869WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 4s 146ms/step - loss: 0.0869\n",
      "Epoch 4/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0512WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 4s 147ms/step - loss: 0.0512\n",
      "Epoch 5/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0593WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 4s 146ms/step - loss: 0.0593\n",
      "Epoch 6/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0704WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 4s 147ms/step - loss: 0.0704\n",
      "Epoch 7/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0664WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 4s 150ms/step - loss: 0.0664\n",
      "Epoch 8/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0456WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 4s 150ms/step - loss: 0.0456\n",
      "Epoch 9/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0657WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 4s 151ms/step - loss: 0.0657\n",
      "Epoch 10/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0680WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 4s 148ms/step - loss: 0.0680\n",
      "6/6 [==============================] - 1s 53ms/step\n",
      "[[0.52381325]\n",
      " [0.5238999 ]\n",
      " [0.52394676]\n",
      " [0.5238103 ]\n",
      " [0.52373165]\n",
      " [0.5238959 ]\n",
      " [0.52399004]\n",
      " [0.5239807 ]\n",
      " [0.52379817]\n",
      " [0.52378446]\n",
      " [0.52376294]\n",
      " [0.52387327]\n",
      " [0.52389354]\n",
      " [0.52396774]\n",
      " [0.523769  ]\n",
      " [0.5235627 ]\n",
      " [0.52440584]\n",
      " [0.5257885 ]\n",
      " [0.5257118 ]\n",
      " [0.525596  ]\n",
      " [0.5255902 ]\n",
      " [0.5257179 ]\n",
      " [0.5255727 ]\n",
      " [0.52551097]\n",
      " [0.52542084]\n",
      " [0.52531755]\n",
      " [0.52514017]\n",
      " [0.5250202 ]\n",
      " [0.5250016 ]\n",
      " [0.52464706]\n",
      " [0.5244743 ]\n",
      " [0.5240343 ]\n",
      " [0.52391344]\n",
      " [0.5242088 ]\n",
      " [0.52405185]\n",
      " [0.52410954]\n",
      " [0.5239951 ]\n",
      " [0.52424926]\n",
      " [0.5237555 ]\n",
      " [0.5238273 ]\n",
      " [0.5241809 ]\n",
      " [0.52315116]\n",
      " [0.5239263 ]\n",
      " [0.52440476]\n",
      " [0.524308  ]\n",
      " [0.52479154]\n",
      " [0.52506   ]\n",
      " [0.5251068 ]\n",
      " [0.5250288 ]\n",
      " [0.5247132 ]\n",
      " [0.5247544 ]\n",
      " [0.5252778 ]\n",
      " [0.5250442 ]\n",
      " [0.52498436]\n",
      " [0.52493995]\n",
      " [0.5248851 ]\n",
      " [0.5247165 ]\n",
      " [0.52459776]\n",
      " [0.524449  ]\n",
      " [0.5240735 ]\n",
      " [0.52399755]\n",
      " [0.52396286]\n",
      " [0.52424383]\n",
      " [0.5244714 ]\n",
      " [0.524596  ]\n",
      " [0.5245924 ]\n",
      " [0.524367  ]\n",
      " [0.52431583]\n",
      " [0.5246062 ]\n",
      " [0.5245874 ]\n",
      " [0.5249756 ]\n",
      " [0.52506447]\n",
      " [0.52483374]\n",
      " [0.5252165 ]\n",
      " [0.5252851 ]\n",
      " [0.5253134 ]\n",
      " [0.5254153 ]\n",
      " [0.52552515]\n",
      " [0.5257337 ]\n",
      " [0.5252644 ]\n",
      " [0.5254104 ]\n",
      " [0.5254708 ]\n",
      " [0.52400297]\n",
      " [0.5226906 ]\n",
      " [0.52358013]\n",
      " [0.5224565 ]\n",
      " [0.5214824 ]\n",
      " [0.5225054 ]\n",
      " [0.5227498 ]\n",
      " [0.5229349 ]\n",
      " [0.5231658 ]\n",
      " [0.5233904 ]\n",
      " [0.52337927]\n",
      " [0.52346796]\n",
      " [0.5233976 ]\n",
      " [0.52357936]\n",
      " [0.52410036]\n",
      " [0.524175  ]\n",
      " [0.5238566 ]\n",
      " [0.5237244 ]\n",
      " [0.523537  ]\n",
      " [0.5236999 ]\n",
      " [0.5238939 ]\n",
      " [0.5239241 ]\n",
      " [0.5241741 ]\n",
      " [0.5244361 ]\n",
      " [0.5248587 ]\n",
      " [0.5245878 ]\n",
      " [0.52373606]\n",
      " [0.52384776]\n",
      " [0.5241757 ]\n",
      " [0.5240734 ]\n",
      " [0.52368987]\n",
      " [0.5240498 ]\n",
      " [0.52357465]\n",
      " [0.5222254 ]\n",
      " [0.5218302 ]\n",
      " [0.52261066]\n",
      " [0.52122456]\n",
      " [0.5216409 ]\n",
      " [0.5217429 ]\n",
      " [0.520958  ]\n",
      " [0.5216247 ]\n",
      " [0.5219334 ]\n",
      " [0.5215894 ]\n",
      " [0.52120477]\n",
      " [0.52185357]\n",
      " [0.5223526 ]\n",
      " [0.5222965 ]\n",
      " [0.5228086 ]\n",
      " [0.5229057 ]\n",
      " [0.52322006]\n",
      " [0.5234871 ]\n",
      " [0.5235874 ]\n",
      " [0.52353543]\n",
      " [0.5234063 ]\n",
      " [0.52320814]\n",
      " [0.5226694 ]\n",
      " [0.5226662 ]\n",
      " [0.5228325 ]\n",
      " [0.52271587]\n",
      " [0.5223323 ]\n",
      " [0.5227502 ]\n",
      " [0.5225372 ]\n",
      " [0.5228351 ]\n",
      " [0.52317286]\n",
      " [0.5233747 ]\n",
      " [0.52346146]\n",
      " [0.5237988 ]\n",
      " [0.5240929 ]\n",
      " [0.5241957 ]\n",
      " [0.5243625 ]\n",
      " [0.5242966 ]\n",
      " [0.52452564]\n",
      " [0.5242983 ]\n",
      " [0.52462125]\n",
      " [0.5250284 ]\n",
      " [0.5249405 ]\n",
      " [0.5251574 ]\n",
      " [0.5249397 ]\n",
      " [0.5250891 ]\n",
      " [0.52495694]\n",
      " [0.5253352 ]\n",
      " [0.5251715 ]\n",
      " [0.5256876 ]\n",
      " [0.5258346 ]\n",
      " [0.52595997]\n",
      " [0.52594715]\n",
      " [0.5253317 ]\n",
      " [0.52565026]\n",
      " [0.5254444 ]\n",
      " [0.52549577]\n",
      " [0.5252287 ]\n",
      " [0.52522695]\n",
      " [0.5243727 ]\n",
      " [0.52514684]\n",
      " [0.5250938 ]\n",
      " [0.525378  ]\n",
      " [0.52509856]\n",
      " [0.5244945 ]\n",
      " [0.5237281 ]\n",
      " [0.5242336 ]\n",
      " [0.5234939 ]\n",
      " [0.524275  ]\n",
      " [0.5241859 ]\n",
      " [0.52479887]\n",
      " [0.5243971 ]\n",
      " [0.524799  ]\n",
      " [0.5252359 ]\n",
      " [0.5253952 ]\n",
      " [0.5253908 ]\n",
      " [0.5252879 ]\n",
      " [0.5256783 ]\n",
      " [0.52584887]\n",
      " [0.5256835 ]\n",
      " [0.52581733]\n",
      " [0.52547395]\n",
      " [0.525044  ]\n",
      " [0.5253397 ]\n",
      " [0.5257505 ]\n",
      " [0.52553076]\n",
      " [0.52583563]\n",
      " [0.5250666 ]\n",
      " [0.5241911 ]\n",
      " [0.5238423 ]\n",
      " [0.524352  ]\n",
      " [0.52434635]\n",
      " [0.52464384]\n",
      " [0.5248579 ]\n",
      " [0.52502024]\n",
      " [0.52536243]\n",
      " [0.52557284]\n",
      " [0.5256336 ]\n",
      " [0.52534723]\n",
      " [0.52513415]\n",
      " [0.52545166]\n",
      " [0.52472836]\n",
      " [0.52496046]\n",
      " [0.5249619 ]\n",
      " [0.5246316 ]\n",
      " [0.52432036]\n",
      " [0.52491397]\n",
      " [0.52506286]\n",
      " [0.525292  ]\n",
      " [0.5255038 ]\n",
      " [0.52556354]\n",
      " [0.52578557]\n",
      " [0.52575547]\n",
      " [0.52577704]\n",
      " [0.5254993 ]\n",
      " [0.52420145]\n",
      " [0.5246792 ]\n",
      " [0.52449876]\n",
      " [0.52493656]\n",
      " [0.52535886]\n",
      " [0.52542007]\n",
      " [0.52560014]\n",
      " [0.5256624 ]\n",
      " [0.52520305]\n",
      " [0.5254225 ]\n",
      " [0.5248196 ]\n",
      " [0.5253097 ]\n",
      " [0.52491766]\n",
      " [0.5253393 ]\n",
      " [0.52531844]\n",
      " [0.52529526]\n",
      " [0.5255035 ]\n",
      " [0.52549165]\n",
      " [0.525758  ]\n",
      " [0.5255714 ]\n",
      " [0.5254656 ]]\n",
      "[0.52381325 0.5238999  0.52394676 0.5238103  0.52373165 0.5238959\n",
      " 0.52399004 0.5239807  0.52379817 0.52378446 0.52376294 0.52387327\n",
      " 0.52389354 0.52396774 0.523769   0.5235627  0.52440584 0.5257885\n",
      " 0.5257118  0.525596   0.5255902  0.5257179  0.5255727  0.52551097\n",
      " 0.52542084 0.52531755 0.52514017 0.5250202  0.5250016  0.52464706\n",
      " 0.5244743  0.5240343  0.52391344 0.5242088  0.52405185 0.52410954\n",
      " 0.5239951  0.52424926 0.5237555  0.5238273  0.5241809  0.52315116\n",
      " 0.5239263  0.52440476 0.524308   0.52479154 0.52506    0.5251068\n",
      " 0.5250288  0.5247132  0.5247544  0.5252778  0.5250442  0.52498436\n",
      " 0.52493995 0.5248851  0.5247165  0.52459776 0.524449   0.5240735\n",
      " 0.52399755 0.52396286 0.52424383 0.5244714  0.524596   0.5245924\n",
      " 0.524367   0.52431583 0.5246062  0.5245874  0.5249756  0.52506447\n",
      " 0.52483374 0.5252165  0.5252851  0.5253134  0.5254153  0.52552515\n",
      " 0.5257337  0.5252644  0.5254104  0.5254708  0.52400297 0.5226906\n",
      " 0.52358013 0.5224565  0.5214824  0.5225054  0.5227498  0.5229349\n",
      " 0.5231658  0.5233904  0.52337927 0.52346796 0.5233976  0.52357936\n",
      " 0.52410036 0.524175   0.5238566  0.5237244  0.523537   0.5236999\n",
      " 0.5238939  0.5239241  0.5241741  0.5244361  0.5248587  0.5245878\n",
      " 0.52373606 0.52384776 0.5241757  0.5240734  0.52368987 0.5240498\n",
      " 0.52357465 0.5222254  0.5218302  0.52261066 0.52122456 0.5216409\n",
      " 0.5217429  0.520958   0.5216247  0.5219334  0.5215894  0.52120477\n",
      " 0.52185357 0.5223526  0.5222965  0.5228086  0.5229057  0.52322006\n",
      " 0.5234871  0.5235874  0.52353543 0.5234063  0.52320814 0.5226694\n",
      " 0.5226662  0.5228325  0.52271587 0.5223323  0.5227502  0.5225372\n",
      " 0.5228351  0.52317286 0.5233747  0.52346146 0.5237988  0.5240929\n",
      " 0.5241957  0.5243625  0.5242966  0.52452564 0.5242983  0.52462125\n",
      " 0.5250284  0.5249405  0.5251574  0.5249397  0.5250891  0.52495694\n",
      " 0.5253352  0.5251715  0.5256876  0.5258346  0.52595997 0.52594715\n",
      " 0.5253317  0.52565026 0.5254444  0.52549577 0.5252287  0.52522695\n",
      " 0.5243727  0.52514684 0.5250938  0.525378   0.52509856 0.5244945\n",
      " 0.5237281  0.5242336  0.5234939  0.524275   0.5241859  0.52479887\n",
      " 0.5243971  0.524799   0.5252359  0.5253952  0.5253908  0.5252879\n",
      " 0.5256783  0.52584887 0.5256835  0.52581733 0.52547395 0.525044\n",
      " 0.5253397  0.5257505  0.52553076 0.52583563 0.5250666  0.5241911\n",
      " 0.5238423  0.524352   0.52434635 0.52464384 0.5248579  0.52502024\n",
      " 0.52536243 0.52557284 0.5256336  0.52534723 0.52513415 0.52545166\n",
      " 0.52472836 0.52496046 0.5249619  0.5246316  0.52432036 0.52491397\n",
      " 0.52506286 0.525292   0.5255038  0.52556354 0.52578557 0.52575547\n",
      " 0.52577704 0.5254993  0.52420145 0.5246792  0.52449876 0.52493656\n",
      " 0.52535886 0.52542007 0.52560014 0.5256624  0.52520305 0.5254225\n",
      " 0.5248196  0.5253097  0.52491766 0.5253393  0.52531844 0.52529526\n",
      " 0.5255035  0.52549165 0.525758   0.5255714  0.5254656 ]\n",
      "MSFT 1.4764956823323152\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed\n",
      "            RSI       ATR       Date     Volume   Returns\n",
      "1     63.806858  0.303167 2013-01-03   93075567  0.000581\n",
      "2     69.709819  0.313179 2013-01-04  110954331  0.019568\n",
      "3     67.057713  0.306465 2013-01-07   66476239 -0.004373\n",
      "4     65.842868  0.305692 2013-01-08   67295297 -0.001975\n",
      "5     67.923125  0.301203 2013-01-09   81291563  0.006552\n",
      "...         ...       ...        ...        ...       ...\n",
      "1505  34.556117  1.742417 2018-12-24   31806000 -0.003395\n",
      "1506  49.665784  1.845744 2018-12-26   47466000  0.062769\n",
      "1507  50.525556  1.881370 2018-12-27   42196000  0.004243\n",
      "1508  49.135022  1.827200 2018-12-28   28296000 -0.006535\n",
      "1509  48.822206  1.800650 2018-12-31   29866000 -0.001418\n",
      "\n",
      "[1509 rows x 5 columns]\n",
      "1198\n",
      "[[ 6.38068579e+01  3.03167180e-01  9.30755670e+07  5.80599965e-04]\n",
      " [ 6.97098194e+01  3.13179409e-01  1.10954331e+08  1.95676820e-02]\n",
      " [ 6.70577125e+01  3.06464969e-01  6.64762390e+07 -4.37280770e-03]\n",
      " ...\n",
      " [ 5.41542902e+01  6.88277742e-01  2.54380000e+07 -6.99878116e-03]\n",
      " [ 5.34264521e+01  6.74757943e-01  1.67420000e+07 -1.17275707e-03]\n",
      " [ 5.23543999e+01  6.43703924e-01  1.77500000e+07 -1.66149741e-03]]\n",
      "Length of inputs 1198\n",
      "length of time-series - inputs (1198, 60, 4)\n",
      "length of time-series - outputs (1198,)\n",
      "Training Size (1198, 60, 4) (1198,)\n",
      "Length of inputs 251\n",
      "length of time-series - inputs (251, 60, 4)\n",
      "length of time-series - outputs (251,)\n",
      "Test Size (251, 60, 4) (251,)\n",
      "Reloading Tuner from .\\untitled_project\\tuner0.json\n",
      "WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_23460\\465918227.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_stock.Date = pd.to_datetime(df_stock.Date)\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\base.py:443: UserWarning: X has feature names, but MinMaxScaler was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).layer_with_weights-1.cell.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).layer_with_weights-1.cell.recurrent_kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).layer_with_weights-1.cell.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.19\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.20\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 60, 92)            35696     \n",
      "                                                                 \n",
      " global_average_pooling1d (  (None, 92)                0         \n",
      " GlobalAveragePooling1D)                                         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 60)                5580      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 20)                1220      \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 20)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 21        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 42517 (166.08 KB)\n",
      "Trainable params: 42517 (166.08 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "GOOG None\n",
      "Epoch 1/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.2257WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 2s 24ms/step - loss: 0.2257\n",
      "Epoch 2/10\n",
      "22/24 [==========================>...] - ETA: 0s - loss: 0.0496WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 1s 24ms/step - loss: 0.0473\n",
      "Epoch 3/10\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 0.0490WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 1s 24ms/step - loss: 0.0481\n",
      "Epoch 4/10\n",
      "22/24 [==========================>...] - ETA: 0s - loss: 0.0555WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 1s 23ms/step - loss: 0.0518\n",
      "Epoch 5/10\n",
      "22/24 [==========================>...] - ETA: 0s - loss: 0.0517WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 1s 25ms/step - loss: 0.0485\n",
      "Epoch 6/10\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 0.0428WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 1s 25ms/step - loss: 0.0422\n",
      "Epoch 7/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0428WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 1s 24ms/step - loss: 0.0428\n",
      "Epoch 8/10\n",
      "22/24 [==========================>...] - ETA: 0s - loss: 0.0374WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 1s 23ms/step - loss: 0.0355\n",
      "Epoch 9/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0386WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 1s 24ms/step - loss: 0.0386\n",
      "Epoch 10/10\n",
      "22/24 [==========================>...] - ETA: 0s - loss: 0.0299WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 1s 24ms/step - loss: 0.0307\n",
      "6/6 [==============================] - 0s 7ms/step\n",
      "[[0.30810645]\n",
      " [0.3079535 ]\n",
      " [0.30789435]\n",
      " [0.3079843 ]\n",
      " [0.308166  ]\n",
      " [0.3083002 ]\n",
      " [0.3086083 ]\n",
      " [0.30893022]\n",
      " [0.30919516]\n",
      " [0.3095098 ]\n",
      " [0.30992356]\n",
      " [0.31044835]\n",
      " [0.31087026]\n",
      " [0.3113336 ]\n",
      " [0.31137002]\n",
      " [0.31121904]\n",
      " [0.3113575 ]\n",
      " [0.31093162]\n",
      " [0.31079823]\n",
      " [0.3107269 ]\n",
      " [0.31067166]\n",
      " [0.3106509 ]\n",
      " [0.31060004]\n",
      " [0.31103522]\n",
      " [0.3116614 ]\n",
      " [0.3123464 ]\n",
      " [0.31360608]\n",
      " [0.315105  ]\n",
      " [0.31666243]\n",
      " [0.31841016]\n",
      " [0.320279  ]\n",
      " [0.3219608 ]\n",
      " [0.32387143]\n",
      " [0.32580817]\n",
      " [0.32769948]\n",
      " [0.33018482]\n",
      " [0.3326866 ]\n",
      " [0.3350851 ]\n",
      " [0.3372643 ]\n",
      " [0.33938354]\n",
      " [0.34111732]\n",
      " [0.34285408]\n",
      " [0.3446387 ]\n",
      " [0.3464111 ]\n",
      " [0.3481124 ]\n",
      " [0.35005838]\n",
      " [0.3520068 ]\n",
      " [0.35391283]\n",
      " [0.35573745]\n",
      " [0.3576026 ]\n",
      " [0.35948798]\n",
      " [0.36170793]\n",
      " [0.3640489 ]\n",
      " [0.3665116 ]\n",
      " [0.3690098 ]\n",
      " [0.3716659 ]\n",
      " [0.37450254]\n",
      " [0.37737042]\n",
      " [0.38022316]\n",
      " [0.3831426 ]\n",
      " [0.3859288 ]\n",
      " [0.3887302 ]\n",
      " [0.3915447 ]\n",
      " [0.39430505]\n",
      " [0.39718878]\n",
      " [0.4001165 ]\n",
      " [0.40303022]\n",
      " [0.40577298]\n",
      " [0.40850884]\n",
      " [0.41118068]\n",
      " [0.41366392]\n",
      " [0.4160837 ]\n",
      " [0.41832465]\n",
      " [0.42059976]\n",
      " [0.4228863 ]\n",
      " [0.42506468]\n",
      " [0.42732084]\n",
      " [0.42966163]\n",
      " [0.4322992 ]\n",
      " [0.43463194]\n",
      " [0.4365648 ]\n",
      " [0.43728   ]\n",
      " [0.43409824]\n",
      " [0.4345091 ]\n",
      " [0.43635118]\n",
      " [0.4351362 ]\n",
      " [0.43582284]\n",
      " [0.43743843]\n",
      " [0.4373932 ]\n",
      " [0.43737185]\n",
      " [0.4376439 ]\n",
      " [0.43758887]\n",
      " [0.43730038]\n",
      " [0.43717623]\n",
      " [0.43689203]\n",
      " [0.4365486 ]\n",
      " [0.43634975]\n",
      " [0.4353618 ]\n",
      " [0.43377495]\n",
      " [0.43293852]\n",
      " [0.43209815]\n",
      " [0.43208653]\n",
      " [0.43139446]\n",
      " [0.4303692 ]\n",
      " [0.42948693]\n",
      " [0.42852545]\n",
      " [0.42761296]\n",
      " [0.42619908]\n",
      " [0.42483652]\n",
      " [0.42410684]\n",
      " [0.42204046]\n",
      " [0.41892403]\n",
      " [0.4156381 ]\n",
      " [0.41394293]\n",
      " [0.41052133]\n",
      " [0.40749323]\n",
      " [0.40681934]\n",
      " [0.40618533]\n",
      " [0.40353107]\n",
      " [0.40378428]\n",
      " [0.40286386]\n",
      " [0.40121078]\n",
      " [0.40086752]\n",
      " [0.3998425 ]\n",
      " [0.3985139 ]\n",
      " [0.3973435 ]\n",
      " [0.3969487 ]\n",
      " [0.3962562 ]\n",
      " [0.39526498]\n",
      " [0.39499152]\n",
      " [0.39427364]\n",
      " [0.3938878 ]\n",
      " [0.3933053 ]\n",
      " [0.39200577]\n",
      " [0.39108258]\n",
      " [0.3893363 ]\n",
      " [0.38815457]\n",
      " [0.3869583 ]\n",
      " [0.3873491 ]\n",
      " [0.38669986]\n",
      " [0.38550323]\n",
      " [0.38503772]\n",
      " [0.38499498]\n",
      " [0.38432533]\n",
      " [0.3844326 ]\n",
      " [0.3846256 ]\n",
      " [0.38453877]\n",
      " [0.38464466]\n",
      " [0.38500506]\n",
      " [0.3852722 ]\n",
      " [0.38555455]\n",
      " [0.38590986]\n",
      " [0.38630676]\n",
      " [0.38683107]\n",
      " [0.3872636 ]\n",
      " [0.38761282]\n",
      " [0.388096  ]\n",
      " [0.38862497]\n",
      " [0.38922656]\n",
      " [0.38978136]\n",
      " [0.39008552]\n",
      " [0.39017996]\n",
      " [0.39026344]\n",
      " [0.39015985]\n",
      " [0.39005542]\n",
      " [0.38987395]\n",
      " [0.3897959 ]\n",
      " [0.3896777 ]\n",
      " [0.38965353]\n",
      " [0.38977122]\n",
      " [0.38998887]\n",
      " [0.39025217]\n",
      " [0.39064157]\n",
      " [0.39116865]\n",
      " [0.3915788 ]\n",
      " [0.39211076]\n",
      " [0.39264417]\n",
      " [0.39310294]\n",
      " [0.39350188]\n",
      " [0.39371008]\n",
      " [0.39341295]\n",
      " [0.39316416]\n",
      " [0.39266557]\n",
      " [0.39254427]\n",
      " [0.39209622]\n",
      " [0.3917356 ]\n",
      " [0.3912059 ]\n",
      " [0.3912077 ]\n",
      " [0.39099348]\n",
      " [0.39069653]\n",
      " [0.39028293]\n",
      " [0.3899672 ]\n",
      " [0.38998502]\n",
      " [0.3897883 ]\n",
      " [0.3898203 ]\n",
      " [0.3900744 ]\n",
      " [0.39035195]\n",
      " [0.39059514]\n",
      " [0.39090192]\n",
      " [0.39136916]\n",
      " [0.39169544]\n",
      " [0.39194417]\n",
      " [0.39216685]\n",
      " [0.3921613 ]\n",
      " [0.39266944]\n",
      " [0.39326662]\n",
      " [0.3939695 ]\n",
      " [0.39460397]\n",
      " [0.39553297]\n",
      " [0.39656034]\n",
      " [0.3977067 ]\n",
      " [0.39891738]\n",
      " [0.40023112]\n",
      " [0.4014594 ]\n",
      " [0.40284526]\n",
      " [0.40418983]\n",
      " [0.40536058]\n",
      " [0.4067483 ]\n",
      " [0.40806788]\n",
      " [0.409499  ]\n",
      " [0.4108544 ]\n",
      " [0.41229826]\n",
      " [0.41370422]\n",
      " [0.41526234]\n",
      " [0.41691464]\n",
      " [0.4185053 ]\n",
      " [0.42024034]\n",
      " [0.42185867]\n",
      " [0.4231503 ]\n",
      " [0.42435908]\n",
      " [0.42547232]\n",
      " [0.42658752]\n",
      " [0.42765123]\n",
      " [0.42873734]\n",
      " [0.43005252]\n",
      " [0.4311788 ]\n",
      " [0.43262005]\n",
      " [0.4337805 ]\n",
      " [0.4348653 ]\n",
      " [0.43611348]\n",
      " [0.43728507]\n",
      " [0.43835652]\n",
      " [0.43920523]\n",
      " [0.44060332]\n",
      " [0.4418189 ]\n",
      " [0.44296676]\n",
      " [0.4443515 ]\n",
      " [0.44547546]\n",
      " [0.4465534 ]\n",
      " [0.44751543]\n",
      " [0.4476807 ]]\n",
      "[0.30810645 0.3079535  0.30789435 0.3079843  0.308166   0.3083002\n",
      " 0.3086083  0.30893022 0.30919516 0.3095098  0.30992356 0.31044835\n",
      " 0.31087026 0.3113336  0.31137002 0.31121904 0.3113575  0.31093162\n",
      " 0.31079823 0.3107269  0.31067166 0.3106509  0.31060004 0.31103522\n",
      " 0.3116614  0.3123464  0.31360608 0.315105   0.31666243 0.31841016\n",
      " 0.320279   0.3219608  0.32387143 0.32580817 0.32769948 0.33018482\n",
      " 0.3326866  0.3350851  0.3372643  0.33938354 0.34111732 0.34285408\n",
      " 0.3446387  0.3464111  0.3481124  0.35005838 0.3520068  0.35391283\n",
      " 0.35573745 0.3576026  0.35948798 0.36170793 0.3640489  0.3665116\n",
      " 0.3690098  0.3716659  0.37450254 0.37737042 0.38022316 0.3831426\n",
      " 0.3859288  0.3887302  0.3915447  0.39430505 0.39718878 0.4001165\n",
      " 0.40303022 0.40577298 0.40850884 0.41118068 0.41366392 0.4160837\n",
      " 0.41832465 0.42059976 0.4228863  0.42506468 0.42732084 0.42966163\n",
      " 0.4322992  0.43463194 0.4365648  0.43728    0.43409824 0.4345091\n",
      " 0.43635118 0.4351362  0.43582284 0.43743843 0.4373932  0.43737185\n",
      " 0.4376439  0.43758887 0.43730038 0.43717623 0.43689203 0.4365486\n",
      " 0.43634975 0.4353618  0.43377495 0.43293852 0.43209815 0.43208653\n",
      " 0.43139446 0.4303692  0.42948693 0.42852545 0.42761296 0.42619908\n",
      " 0.42483652 0.42410684 0.42204046 0.41892403 0.4156381  0.41394293\n",
      " 0.41052133 0.40749323 0.40681934 0.40618533 0.40353107 0.40378428\n",
      " 0.40286386 0.40121078 0.40086752 0.3998425  0.3985139  0.3973435\n",
      " 0.3969487  0.3962562  0.39526498 0.39499152 0.39427364 0.3938878\n",
      " 0.3933053  0.39200577 0.39108258 0.3893363  0.38815457 0.3869583\n",
      " 0.3873491  0.38669986 0.38550323 0.38503772 0.38499498 0.38432533\n",
      " 0.3844326  0.3846256  0.38453877 0.38464466 0.38500506 0.3852722\n",
      " 0.38555455 0.38590986 0.38630676 0.38683107 0.3872636  0.38761282\n",
      " 0.388096   0.38862497 0.38922656 0.38978136 0.39008552 0.39017996\n",
      " 0.39026344 0.39015985 0.39005542 0.38987395 0.3897959  0.3896777\n",
      " 0.38965353 0.38977122 0.38998887 0.39025217 0.39064157 0.39116865\n",
      " 0.3915788  0.39211076 0.39264417 0.39310294 0.39350188 0.39371008\n",
      " 0.39341295 0.39316416 0.39266557 0.39254427 0.39209622 0.3917356\n",
      " 0.3912059  0.3912077  0.39099348 0.39069653 0.39028293 0.3899672\n",
      " 0.38998502 0.3897883  0.3898203  0.3900744  0.39035195 0.39059514\n",
      " 0.39090192 0.39136916 0.39169544 0.39194417 0.39216685 0.3921613\n",
      " 0.39266944 0.39326662 0.3939695  0.39460397 0.39553297 0.39656034\n",
      " 0.3977067  0.39891738 0.40023112 0.4014594  0.40284526 0.40418983\n",
      " 0.40536058 0.4067483  0.40806788 0.409499   0.4108544  0.41229826\n",
      " 0.41370422 0.41526234 0.41691464 0.4185053  0.42024034 0.42185867\n",
      " 0.4231503  0.42435908 0.42547232 0.42658752 0.42765123 0.42873734\n",
      " 0.43005252 0.4311788  0.43262005 0.4337805  0.4348653  0.43611348\n",
      " 0.43728507 0.43835652 0.43920523 0.44060332 0.4418189  0.44296676\n",
      " 0.4443515  0.44547546 0.4465534  0.44751543 0.4476807 ]\n",
      "GOOG 0.5298893191756915\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed\n",
      "            RSI       ATR       Date     Volume   Returns\n",
      "1     63.806890  0.304608 2013-01-03   92635272  0.000581\n",
      "2     69.709873  0.314668 2013-01-04  110429460  0.019568\n",
      "3     67.057689  0.307922 2013-01-07   66161772 -0.004373\n",
      "4     65.842906  0.307145 2013-01-08   66976956 -0.001975\n",
      "5     67.923160  0.302634 2013-01-09   80907012  0.006552\n",
      "...         ...       ...        ...        ...       ...\n",
      "1505  34.590236  1.786591 2018-12-24   36360000 -0.006660\n",
      "1506  49.279060  1.886763 2018-12-26   46318000  0.062189\n",
      "1507  50.240917  1.917494 2018-12-27   45996000  0.004808\n",
      "1508  49.008156  1.859923 2018-12-28   34398000 -0.005925\n",
      "1509  48.652654  1.834036 2018-12-31   33110000 -0.001645\n",
      "\n",
      "[1509 rows x 5 columns]\n",
      "1198\n",
      "[[ 6.38068900e+01  3.04608112e-01  9.26352720e+07  5.80592580e-04]\n",
      " [ 6.97098726e+01  3.14667962e-01  1.10429460e+08  1.95677297e-02]\n",
      " [ 6.70576891e+01  3.07921583e-01  6.61617720e+07 -4.37292695e-03]\n",
      " ...\n",
      " [ 5.37599002e+01  6.99177784e-01  2.23240000e+07 -5.31505640e-03]\n",
      " [ 5.13225752e+01  6.90165218e-01  1.98840000e+07 -4.01669067e-03]\n",
      " [ 4.98618376e+01  6.59974976e-01  2.36060000e+07 -2.41785394e-03]]\n",
      "Length of inputs 1198\n",
      "length of time-series - inputs (1198, 60, 4)\n",
      "length of time-series - outputs (1198,)\n",
      "Training Size (1198, 60, 4) (1198,)\n",
      "Length of inputs 251\n",
      "length of time-series - inputs (251, 60, 4)\n",
      "length of time-series - outputs (251,)\n",
      "Test Size (251, 60, 4) (251,)\n",
      "Reloading Tuner from .\\untitled_project\\tuner0.json\n",
      "WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_23460\\465918227.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_stock.Date = pd.to_datetime(df_stock.Date)\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\base.py:443: UserWarning: X has feature names, but MinMaxScaler was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).layer_with_weights-1.cell.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).layer_with_weights-1.cell.recurrent_kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).layer_with_weights-1.cell.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.19\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.20\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 60, 452)           826256    \n",
      "                                                                 \n",
      " global_average_pooling1d (  (None, 452)               0         \n",
      " GlobalAveragePooling1D)                                         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 60)                27180     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 20)                1220      \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 20)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 21        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 854677 (3.26 MB)\n",
      "Trainable params: 854677 (3.26 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "GOOGL None\n",
      "Epoch 1/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.9110WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 5s 153ms/step - loss: 0.9110\n",
      "Epoch 2/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0640WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 4s 156ms/step - loss: 0.0640\n",
      "Epoch 3/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0697WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 4s 152ms/step - loss: 0.0697\n",
      "Epoch 4/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0712WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 4s 152ms/step - loss: 0.0712\n",
      "Epoch 5/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0338WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 4s 150ms/step - loss: 0.0338\n",
      "Epoch 6/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0415WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 4s 154ms/step - loss: 0.0415\n",
      "Epoch 7/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0391WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 4s 154ms/step - loss: 0.0391\n",
      "Epoch 8/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0391WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 4s 154ms/step - loss: 0.0391\n",
      "Epoch 9/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0397WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 4s 159ms/step - loss: 0.0397\n",
      "Epoch 10/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0388WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 4s 155ms/step - loss: 0.0388\n",
      "6/6 [==============================] - 1s 59ms/step\n",
      "[[0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]\n",
      " [0.33547541]]\n",
      "[0.33547541 0.33547541 0.33547541 0.33547541 0.33547541 0.33547541\n",
      " 0.33547541 0.33547541 0.33547541 0.33547541 0.33547541 0.33547541\n",
      " 0.33547541 0.33547541 0.33547541 0.33547541 0.33547541 0.33547541\n",
      " 0.33547541 0.33547541 0.33547541 0.33547541 0.33547541 0.33547541\n",
      " 0.33547541 0.33547541 0.33547541 0.33547541 0.33547541 0.33547541\n",
      " 0.33547541 0.33547541 0.33547541 0.33547541 0.33547541 0.33547541\n",
      " 0.33547541 0.33547541 0.33547541 0.33547541 0.33547541 0.33547541\n",
      " 0.33547541 0.33547541 0.33547541 0.33547541 0.33547541 0.33547541\n",
      " 0.33547541 0.33547541 0.33547541 0.33547541 0.33547541 0.33547541\n",
      " 0.33547541 0.33547541 0.33547541 0.33547541 0.33547541 0.33547541\n",
      " 0.33547541 0.33547541 0.33547541 0.33547541 0.33547541 0.33547541\n",
      " 0.33547541 0.33547541 0.33547541 0.33547541 0.33547541 0.33547541\n",
      " 0.33547541 0.33547541 0.33547541 0.33547541 0.33547541 0.33547541\n",
      " 0.33547541 0.33547541 0.33547541 0.33547541 0.33547541 0.33547541\n",
      " 0.33547541 0.33547541 0.33547541 0.33547541 0.33547541 0.33547541\n",
      " 0.33547541 0.33547541 0.33547541 0.33547541 0.33547541 0.33547541\n",
      " 0.33547541 0.33547541 0.33547541 0.33547541 0.33547541 0.33547541\n",
      " 0.33547541 0.33547541 0.33547541 0.33547541 0.33547541 0.33547541\n",
      " 0.33547541 0.33547541 0.33547541 0.33547541 0.33547541 0.33547541\n",
      " 0.33547541 0.33547541 0.33547541 0.33547541 0.33547541 0.33547541\n",
      " 0.33547541 0.33547541 0.33547541 0.33547541 0.33547541 0.33547541\n",
      " 0.33547541 0.33547541 0.33547541 0.33547541 0.33547541 0.33547541\n",
      " 0.33547541 0.33547541 0.33547541 0.33547541 0.33547541 0.33547541\n",
      " 0.33547541 0.33547541 0.33547541 0.33547541 0.33547541 0.33547541\n",
      " 0.33547541 0.33547541 0.33547541 0.33547541 0.33547541 0.33547541\n",
      " 0.33547541 0.33547541 0.33547541 0.33547541 0.33547541 0.33547541\n",
      " 0.33547541 0.33547541 0.33547541 0.33547541 0.33547541 0.33547541\n",
      " 0.33547541 0.33547541 0.33547541 0.33547541 0.33547541 0.33547541\n",
      " 0.33547541 0.33547541 0.33547541 0.33547541 0.33547541 0.33547541\n",
      " 0.33547541 0.33547541 0.33547541 0.33547541 0.33547541 0.33547541\n",
      " 0.33547541 0.33547541 0.33547541 0.33547541 0.33547541 0.33547541\n",
      " 0.33547541 0.33547541 0.33547541 0.33547541 0.33547541 0.33547541\n",
      " 0.33547541 0.33547541 0.33547541 0.33547541 0.33547541 0.33547541\n",
      " 0.33547541 0.33547541 0.33547541 0.33547541 0.33547541 0.33547541\n",
      " 0.33547541 0.33547541 0.33547541 0.33547541 0.33547541 0.33547541\n",
      " 0.33547541 0.33547541 0.33547541 0.33547541 0.33547541 0.33547541\n",
      " 0.33547541 0.33547541 0.33547541 0.33547541 0.33547541 0.33547541\n",
      " 0.33547541 0.33547541 0.33547541 0.33547541 0.33547541 0.33547541\n",
      " 0.33547541 0.33547541 0.33547541 0.33547541 0.33547541 0.33547541\n",
      " 0.33547541 0.33547541 0.33547541 0.33547541 0.33547541 0.33547541\n",
      " 0.33547541 0.33547541 0.33547541 0.33547541 0.33547541 0.33547541\n",
      " 0.33547541 0.33547541 0.33547541 0.33547541 0.33547541]\n",
      "GOOGL 0.5928807228620258\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed\n",
      "            RSI       ATR       Date     Volume   Returns\n",
      "1     58.842540  0.278285 2013-01-03   55018000  0.004537\n",
      "2     59.455583  0.269658 2013-01-04   37484000  0.002589\n",
      "3     66.845444  0.288182 2013-01-07   98200000  0.035295\n",
      "4     64.037190  0.286919 2013-01-08   60214000 -0.007778\n",
      "5     63.995427  0.281068 2013-01-09   45312000 -0.000113\n",
      "...         ...       ...        ...        ...       ...\n",
      "1505  29.222786  3.722038 2018-12-24  144400000 -0.024613\n",
      "1506  42.165661  3.917606 2018-12-26  208236000  0.090254\n",
      "1507  41.568493  3.918813 2018-12-27  194440000 -0.006315\n",
      "1508  43.103510  3.869148 2018-12-28  176580000  0.011144\n",
      "1509  45.363596  3.745423 2018-12-31  139090000  0.016074\n",
      "\n",
      "[1509 rows x 5 columns]\n",
      "1198\n",
      "[[ 5.88425399e+01  2.78285219e-01  5.50180000e+07  4.53668633e-03]\n",
      " [ 5.94555828e+01  2.69657655e-01  3.74840000e+07  2.58877498e-03]\n",
      " [ 6.68454441e+01  2.88182060e-01  9.82000000e+07  3.52948721e-02]\n",
      " ...\n",
      " [ 6.07273695e+01  8.37492993e-01  3.73440000e+07  4.66292289e-03]\n",
      " [ 6.20195553e+01  8.05672216e-01  3.68340000e+07  3.24277580e-03]\n",
      " [ 5.37685562e+01  8.07052826e-01  5.37680000e+07 -1.41199765e-02]]\n",
      "Length of inputs 1198\n",
      "length of time-series - inputs (1198, 60, 4)\n",
      "length of time-series - outputs (1198,)\n",
      "Training Size (1198, 60, 4) (1198,)\n",
      "Length of inputs 251\n",
      "length of time-series - inputs (251, 60, 4)\n",
      "length of time-series - outputs (251,)\n",
      "Test Size (251, 60, 4) (251,)\n",
      "Reloading Tuner from .\\untitled_project\\tuner0.json\n",
      "WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_23460\\465918227.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_stock.Date = pd.to_datetime(df_stock.Date)\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\base.py:443: UserWarning: X has feature names, but MinMaxScaler was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).layer_with_weights-1.cell.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).layer_with_weights-1.cell.recurrent_kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).layer_with_weights-1.cell.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.19\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.20\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 60, 512)           1058816   \n",
      "                                                                 \n",
      " global_average_pooling1d (  (None, 512)               0         \n",
      " GlobalAveragePooling1D)                                         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 60)                30780     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 20)                1220      \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 20)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 21        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1090837 (4.16 MB)\n",
      "Trainable params: 1090837 (4.16 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "AMZN None\n",
      "Epoch 1/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.7088WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 6s 216ms/step - loss: 0.7088\n",
      "Epoch 2/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.1496WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 5s 209ms/step - loss: 0.1496\n",
      "Epoch 3/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0482WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 5s 205ms/step - loss: 0.0482\n",
      "Epoch 4/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0579WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 5s 201ms/step - loss: 0.0579\n",
      "Epoch 5/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0601WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 5s 201ms/step - loss: 0.0601\n",
      "Epoch 6/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0590WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 5s 203ms/step - loss: 0.0590\n",
      "Epoch 7/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0586WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 5s 207ms/step - loss: 0.0586\n",
      "Epoch 8/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0594WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 5s 201ms/step - loss: 0.0594\n",
      "Epoch 9/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0590WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 5s 201ms/step - loss: 0.0590\n",
      "Epoch 10/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0594WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 5s 204ms/step - loss: 0.0594\n",
      "6/6 [==============================] - 1s 61ms/step\n",
      "[[0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]\n",
      " [0.3420307]]\n",
      "[0.3420307 0.3420307 0.3420307 0.3420307 0.3420307 0.3420307 0.3420307\n",
      " 0.3420307 0.3420307 0.3420307 0.3420307 0.3420307 0.3420307 0.3420307\n",
      " 0.3420307 0.3420307 0.3420307 0.3420307 0.3420307 0.3420307 0.3420307\n",
      " 0.3420307 0.3420307 0.3420307 0.3420307 0.3420307 0.3420307 0.3420307\n",
      " 0.3420307 0.3420307 0.3420307 0.3420307 0.3420307 0.3420307 0.3420307\n",
      " 0.3420307 0.3420307 0.3420307 0.3420307 0.3420307 0.3420307 0.3420307\n",
      " 0.3420307 0.3420307 0.3420307 0.3420307 0.3420307 0.3420307 0.3420307\n",
      " 0.3420307 0.3420307 0.3420307 0.3420307 0.3420307 0.3420307 0.3420307\n",
      " 0.3420307 0.3420307 0.3420307 0.3420307 0.3420307 0.3420307 0.3420307\n",
      " 0.3420307 0.3420307 0.3420307 0.3420307 0.3420307 0.3420307 0.3420307\n",
      " 0.3420307 0.3420307 0.3420307 0.3420307 0.3420307 0.3420307 0.3420307\n",
      " 0.3420307 0.3420307 0.3420307 0.3420307 0.3420307 0.3420307 0.3420307\n",
      " 0.3420307 0.3420307 0.3420307 0.3420307 0.3420307 0.3420307 0.3420307\n",
      " 0.3420307 0.3420307 0.3420307 0.3420307 0.3420307 0.3420307 0.3420307\n",
      " 0.3420307 0.3420307 0.3420307 0.3420307 0.3420307 0.3420307 0.3420307\n",
      " 0.3420307 0.3420307 0.3420307 0.3420307 0.3420307 0.3420307 0.3420307\n",
      " 0.3420307 0.3420307 0.3420307 0.3420307 0.3420307 0.3420307 0.3420307\n",
      " 0.3420307 0.3420307 0.3420307 0.3420307 0.3420307 0.3420307 0.3420307\n",
      " 0.3420307 0.3420307 0.3420307 0.3420307 0.3420307 0.3420307 0.3420307\n",
      " 0.3420307 0.3420307 0.3420307 0.3420307 0.3420307 0.3420307 0.3420307\n",
      " 0.3420307 0.3420307 0.3420307 0.3420307 0.3420307 0.3420307 0.3420307\n",
      " 0.3420307 0.3420307 0.3420307 0.3420307 0.3420307 0.3420307 0.3420307\n",
      " 0.3420307 0.3420307 0.3420307 0.3420307 0.3420307 0.3420307 0.3420307\n",
      " 0.3420307 0.3420307 0.3420307 0.3420307 0.3420307 0.3420307 0.3420307\n",
      " 0.3420307 0.3420307 0.3420307 0.3420307 0.3420307 0.3420307 0.3420307\n",
      " 0.3420307 0.3420307 0.3420307 0.3420307 0.3420307 0.3420307 0.3420307\n",
      " 0.3420307 0.3420307 0.3420307 0.3420307 0.3420307 0.3420307 0.3420307\n",
      " 0.3420307 0.3420307 0.3420307 0.3420307 0.3420307 0.3420307 0.3420307\n",
      " 0.3420307 0.3420307 0.3420307 0.3420307 0.3420307 0.3420307 0.3420307\n",
      " 0.3420307 0.3420307 0.3420307 0.3420307 0.3420307 0.3420307 0.3420307\n",
      " 0.3420307 0.3420307 0.3420307 0.3420307 0.3420307 0.3420307 0.3420307\n",
      " 0.3420307 0.3420307 0.3420307 0.3420307 0.3420307 0.3420307 0.3420307\n",
      " 0.3420307 0.3420307 0.3420307 0.3420307 0.3420307 0.3420307 0.3420307\n",
      " 0.3420307 0.3420307 0.3420307 0.3420307 0.3420307 0.3420307 0.3420307\n",
      " 0.3420307 0.3420307 0.3420307 0.3420307 0.3420307 0.3420307 0.3420307\n",
      " 0.3420307 0.3420307 0.3420307 0.3420307 0.3420307 0.3420307]\n",
      "AMZN 3.428805740759474\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed\n",
      "            RSI       ATR       Date    Volume   Returns\n",
      "1     60.423352  0.075814 2013-01-03  29888800  0.000786\n",
      "2     67.742509  0.078970 2013-01-04  52496800  0.032460\n",
      "3     57.399505  0.082258 2013-01-07  61073200 -0.029323\n",
      "4     51.196695  0.084240 2013-01-08  46642400 -0.022170\n",
      "5     45.859672  0.087508 2013-01-09  69502000 -0.022673\n",
      "...         ...       ...        ...       ...       ...\n",
      "1505  28.272512  2.321551 2018-12-24  46384000 -0.019404\n",
      "1506  34.058154  2.310726 2018-12-26  69510000  0.046284\n",
      "1507  33.135366  2.274245 2018-12-27  63704400 -0.014607\n",
      "1508  35.551779  2.238228 2018-12-28  62872800  0.018730\n",
      "1509  35.468279  2.157819 2018-12-31  46514000 -0.001123\n",
      "\n",
      "[1509 rows x 5 columns]\n",
      "1198\n",
      "[[ 6.04233522e+01  7.58142446e-02  2.98888000e+07  7.85856890e-04]\n",
      " [ 6.77425091e+01  7.89703618e-02  5.24968000e+07  3.24602797e-02]\n",
      " [ 5.73995046e+01  8.22581931e-02  6.10732000e+07 -2.93229324e-02]\n",
      " ...\n",
      " [ 5.02752665e+01  1.31175102e+00  3.29488000e+07 -1.36828670e-03]\n",
      " [ 5.06034339e+01  1.25787615e+00  2.40248000e+07  1.16575641e-03]\n",
      " [ 4.51608622e+01  1.25659940e+00  2.79964000e+07 -1.99546418e-02]]\n",
      "Length of inputs 1198\n",
      "length of time-series - inputs (1198, 60, 4)\n",
      "length of time-series - outputs (1198,)\n",
      "Training Size (1198, 60, 4) (1198,)\n",
      "Length of inputs 251\n",
      "length of time-series - inputs (251, 60, 4)\n",
      "length of time-series - outputs (251,)\n",
      "Test Size (251, 60, 4) (251,)\n",
      "Reloading Tuner from .\\untitled_project\\tuner0.json\n",
      "WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_23460\\465918227.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_stock.Date = pd.to_datetime(df_stock.Date)\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\base.py:443: UserWarning: X has feature names, but MinMaxScaler was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).layer_with_weights-1.cell.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).layer_with_weights-1.cell.recurrent_kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).layer_with_weights-1.cell.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.19\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.20\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 60, 452)           826256    \n",
      "                                                                 \n",
      " global_average_pooling1d (  (None, 452)               0         \n",
      " GlobalAveragePooling1D)                                         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 60)                27180     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 20)                1220      \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 20)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 21        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 854677 (3.26 MB)\n",
      "Trainable params: 854677 (3.26 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "NVDA None\n",
      "Epoch 1/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.8946WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 5s 155ms/step - loss: 0.8946\n",
      "Epoch 2/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.1074WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 4s 158ms/step - loss: 0.1074\n",
      "Epoch 3/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.1750WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 4s 158ms/step - loss: 0.1750\n",
      "Epoch 4/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0672WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 4s 154ms/step - loss: 0.0672\n",
      "Epoch 5/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0778WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 4s 155ms/step - loss: 0.0778\n",
      "Epoch 6/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0780WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 4s 154ms/step - loss: 0.0780\n",
      "Epoch 7/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0841WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 4s 153ms/step - loss: 0.0841\n",
      "Epoch 8/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.1025WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 4s 153ms/step - loss: 0.1025\n",
      "Epoch 9/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0694WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 4s 152ms/step - loss: 0.0694\n",
      "Epoch 10/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0693WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 4s 154ms/step - loss: 0.0693\n",
      "6/6 [==============================] - 1s 55ms/step\n",
      "[[0.1860246 ]\n",
      " [0.18600096]\n",
      " [0.18599637]\n",
      " [0.1860199 ]\n",
      " [0.1859948 ]\n",
      " [0.18597908]\n",
      " [0.18595228]\n",
      " [0.18594436]\n",
      " [0.18592595]\n",
      " [0.18593696]\n",
      " [0.18595012]\n",
      " [0.18594146]\n",
      " [0.18592039]\n",
      " [0.18589094]\n",
      " [0.18601204]\n",
      " [0.1859538 ]\n",
      " [0.18594733]\n",
      " [0.18595119]\n",
      " [0.18593043]\n",
      " [0.18595216]\n",
      " [0.18595564]\n",
      " [0.18591937]\n",
      " [0.18590115]\n",
      " [0.1858764 ]\n",
      " [0.18595164]\n",
      " [0.18600605]\n",
      " [0.1860408 ]\n",
      " [0.18606117]\n",
      " [0.18604653]\n",
      " [0.1860576 ]\n",
      " [0.18604288]\n",
      " [0.18604308]\n",
      " [0.18602236]\n",
      " [0.18600054]\n",
      " [0.1859938 ]\n",
      " [0.18595853]\n",
      " [0.18599057]\n",
      " [0.18605268]\n",
      " [0.18621016]\n",
      " [0.18612458]\n",
      " [0.18618341]\n",
      " [0.18632482]\n",
      " [0.18624555]\n",
      " [0.18621269]\n",
      " [0.18618326]\n",
      " [0.18617873]\n",
      " [0.18614577]\n",
      " [0.18617825]\n",
      " [0.18620089]\n",
      " [0.18616152]\n",
      " [0.18614182]\n",
      " [0.18613459]\n",
      " [0.18613163]\n",
      " [0.1861152 ]\n",
      " [0.1861068 ]\n",
      " [0.18610594]\n",
      " [0.18609965]\n",
      " [0.18609256]\n",
      " [0.18607928]\n",
      " [0.1860865 ]\n",
      " [0.18607397]\n",
      " [0.18611437]\n",
      " [0.18611942]\n",
      " [0.18611921]\n",
      " [0.18613182]\n",
      " [0.18612802]\n",
      " [0.18613186]\n",
      " [0.18611598]\n",
      " [0.1861175 ]\n",
      " [0.18615109]\n",
      " [0.18614368]\n",
      " [0.18613635]\n",
      " [0.18613333]\n",
      " [0.18612793]\n",
      " [0.1861267 ]\n",
      " [0.18614228]\n",
      " [0.18612555]\n",
      " [0.18612322]\n",
      " [0.18613426]\n",
      " [0.18615519]\n",
      " [0.18615255]\n",
      " [0.18617831]\n",
      " [0.18624341]\n",
      " [0.18645188]\n",
      " [0.1863248 ]\n",
      " [0.18634948]\n",
      " [0.18647191]\n",
      " [0.18638276]\n",
      " [0.18642844]\n",
      " [0.1863837 ]\n",
      " [0.18636201]\n",
      " [0.186349  ]\n",
      " [0.18636405]\n",
      " [0.18633676]\n",
      " [0.18637471]\n",
      " [0.18633069]\n",
      " [0.18629794]\n",
      " [0.1862874 ]\n",
      " [0.18628152]\n",
      " [0.18631044]\n",
      " [0.18638623]\n",
      " [0.18633819]\n",
      " [0.18634039]\n",
      " [0.18630096]\n",
      " [0.1863007 ]\n",
      " [0.18628164]\n",
      " [0.18625566]\n",
      " [0.18624996]\n",
      " [0.18626404]\n",
      " [0.18623726]\n",
      " [0.18621925]\n",
      " [0.1862162 ]\n",
      " [0.18628602]\n",
      " [0.1862222 ]\n",
      " [0.18623671]\n",
      " [0.18627925]\n",
      " [0.18634175]\n",
      " [0.18629307]\n",
      " [0.18652202]\n",
      " [0.18646629]\n",
      " [0.18640783]\n",
      " [0.18652181]\n",
      " [0.18644203]\n",
      " [0.18646242]\n",
      " [0.18649721]\n",
      " [0.18651278]\n",
      " [0.18645316]\n",
      " [0.18637797]\n",
      " [0.18640414]\n",
      " [0.18635449]\n",
      " [0.18637523]\n",
      " [0.18634775]\n",
      " [0.18631512]\n",
      " [0.1863235 ]\n",
      " [0.18635075]\n",
      " [0.18632083]\n",
      " [0.18635608]\n",
      " [0.18635823]\n",
      " [0.18637998]\n",
      " [0.18629716]\n",
      " [0.18630211]\n",
      " [0.18629582]\n",
      " [0.18625973]\n",
      " [0.1862578 ]\n",
      " [0.18623048]\n",
      " [0.18622738]\n",
      " [0.18622339]\n",
      " [0.18621886]\n",
      " [0.18620431]\n",
      " [0.18620996]\n",
      " [0.18623213]\n",
      " [0.18620501]\n",
      " [0.18626313]\n",
      " [0.18621956]\n",
      " [0.18620892]\n",
      " [0.18622094]\n",
      " [0.18623224]\n",
      " [0.18622458]\n",
      " [0.18619464]\n",
      " [0.18618952]\n",
      " [0.1861711 ]\n",
      " [0.18616779]\n",
      " [0.1861549 ]\n",
      " [0.18615419]\n",
      " [0.18614227]\n",
      " [0.1861457 ]\n",
      " [0.18613373]\n",
      " [0.1861292 ]\n",
      " [0.18613338]\n",
      " [0.18612687]\n",
      " [0.18611509]\n",
      " [0.18610172]\n",
      " [0.18609443]\n",
      " [0.1860951 ]\n",
      " [0.18609487]\n",
      " [0.18609025]\n",
      " [0.18611807]\n",
      " [0.18610436]\n",
      " [0.1861282 ]\n",
      " [0.1861676 ]\n",
      " [0.18627088]\n",
      " [0.18618469]\n",
      " [0.1862631 ]\n",
      " [0.18618006]\n",
      " [0.18623787]\n",
      " [0.18618572]\n",
      " [0.18623486]\n",
      " [0.18617386]\n",
      " [0.18616903]\n",
      " [0.18616442]\n",
      " [0.18615842]\n",
      " [0.18617105]\n",
      " [0.18614678]\n",
      " [0.18614414]\n",
      " [0.18613324]\n",
      " [0.1861368 ]\n",
      " [0.18613502]\n",
      " [0.18612008]\n",
      " [0.18611462]\n",
      " [0.18612336]\n",
      " [0.18612674]\n",
      " [0.1861187 ]\n",
      " [0.1861211 ]\n",
      " [0.18613832]\n",
      " [0.1861814 ]\n",
      " [0.1861481 ]\n",
      " [0.18614197]\n",
      " [0.18614323]\n",
      " [0.18613169]\n",
      " [0.18612336]\n",
      " [0.18611439]\n",
      " [0.18611075]\n",
      " [0.18610439]\n",
      " [0.18609968]\n",
      " [0.18610084]\n",
      " [0.18610233]\n",
      " [0.18612824]\n",
      " [0.18614769]\n",
      " [0.18623613]\n",
      " [0.18619478]\n",
      " [0.18618032]\n",
      " [0.1861871 ]\n",
      " [0.18619354]\n",
      " [0.18618354]\n",
      " [0.18618445]\n",
      " [0.18618931]\n",
      " [0.18618016]\n",
      " [0.186177  ]\n",
      " [0.18616636]\n",
      " [0.18617156]\n",
      " [0.18619202]\n",
      " [0.18620224]\n",
      " [0.18619853]\n",
      " [0.18618336]\n",
      " [0.1861917 ]\n",
      " [0.1862208 ]\n",
      " [0.1861962 ]\n",
      " [0.18619305]\n",
      " [0.18619534]\n",
      " [0.18620074]\n",
      " [0.1861853 ]\n",
      " [0.18621087]\n",
      " [0.18620971]\n",
      " [0.18618582]\n",
      " [0.18617523]\n",
      " [0.18617424]\n",
      " [0.18616618]\n",
      " [0.18618521]\n",
      " [0.18619725]\n",
      " [0.18620318]\n",
      " [0.18620346]]\n",
      "[0.1860246  0.18600096 0.18599637 0.1860199  0.1859948  0.18597908\n",
      " 0.18595228 0.18594436 0.18592595 0.18593696 0.18595012 0.18594146\n",
      " 0.18592039 0.18589094 0.18601204 0.1859538  0.18594733 0.18595119\n",
      " 0.18593043 0.18595216 0.18595564 0.18591937 0.18590115 0.1858764\n",
      " 0.18595164 0.18600605 0.1860408  0.18606117 0.18604653 0.1860576\n",
      " 0.18604288 0.18604308 0.18602236 0.18600054 0.1859938  0.18595853\n",
      " 0.18599057 0.18605268 0.18621016 0.18612458 0.18618341 0.18632482\n",
      " 0.18624555 0.18621269 0.18618326 0.18617873 0.18614577 0.18617825\n",
      " 0.18620089 0.18616152 0.18614182 0.18613459 0.18613163 0.1861152\n",
      " 0.1861068  0.18610594 0.18609965 0.18609256 0.18607928 0.1860865\n",
      " 0.18607397 0.18611437 0.18611942 0.18611921 0.18613182 0.18612802\n",
      " 0.18613186 0.18611598 0.1861175  0.18615109 0.18614368 0.18613635\n",
      " 0.18613333 0.18612793 0.1861267  0.18614228 0.18612555 0.18612322\n",
      " 0.18613426 0.18615519 0.18615255 0.18617831 0.18624341 0.18645188\n",
      " 0.1863248  0.18634948 0.18647191 0.18638276 0.18642844 0.1863837\n",
      " 0.18636201 0.186349   0.18636405 0.18633676 0.18637471 0.18633069\n",
      " 0.18629794 0.1862874  0.18628152 0.18631044 0.18638623 0.18633819\n",
      " 0.18634039 0.18630096 0.1863007  0.18628164 0.18625566 0.18624996\n",
      " 0.18626404 0.18623726 0.18621925 0.1862162  0.18628602 0.1862222\n",
      " 0.18623671 0.18627925 0.18634175 0.18629307 0.18652202 0.18646629\n",
      " 0.18640783 0.18652181 0.18644203 0.18646242 0.18649721 0.18651278\n",
      " 0.18645316 0.18637797 0.18640414 0.18635449 0.18637523 0.18634775\n",
      " 0.18631512 0.1863235  0.18635075 0.18632083 0.18635608 0.18635823\n",
      " 0.18637998 0.18629716 0.18630211 0.18629582 0.18625973 0.1862578\n",
      " 0.18623048 0.18622738 0.18622339 0.18621886 0.18620431 0.18620996\n",
      " 0.18623213 0.18620501 0.18626313 0.18621956 0.18620892 0.18622094\n",
      " 0.18623224 0.18622458 0.18619464 0.18618952 0.1861711  0.18616779\n",
      " 0.1861549  0.18615419 0.18614227 0.1861457  0.18613373 0.1861292\n",
      " 0.18613338 0.18612687 0.18611509 0.18610172 0.18609443 0.1860951\n",
      " 0.18609487 0.18609025 0.18611807 0.18610436 0.1861282  0.1861676\n",
      " 0.18627088 0.18618469 0.1862631  0.18618006 0.18623787 0.18618572\n",
      " 0.18623486 0.18617386 0.18616903 0.18616442 0.18615842 0.18617105\n",
      " 0.18614678 0.18614414 0.18613324 0.1861368  0.18613502 0.18612008\n",
      " 0.18611462 0.18612336 0.18612674 0.1861187  0.1861211  0.18613832\n",
      " 0.1861814  0.1861481  0.18614197 0.18614323 0.18613169 0.18612336\n",
      " 0.18611439 0.18611075 0.18610439 0.18609968 0.18610084 0.18610233\n",
      " 0.18612824 0.18614769 0.18623613 0.18619478 0.18618032 0.1861871\n",
      " 0.18619354 0.18618354 0.18618445 0.18618931 0.18618016 0.186177\n",
      " 0.18616636 0.18617156 0.18619202 0.18620224 0.18619853 0.18618336\n",
      " 0.1861917  0.1862208  0.1861962  0.18619305 0.18619534 0.18620074\n",
      " 0.1861853  0.18621087 0.18620971 0.18618582 0.18617523 0.18617424\n",
      " 0.18616618 0.18618521 0.18619725 0.18620318 0.18620346]\n",
      "NVDA 1.1037220853381982\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed\n",
      "            RSI       ATR       Date     Volume   Returns\n",
      "1     57.820695  0.066655 2013-01-03   11130000 -0.016826\n",
      "2     54.875638  0.066084 2013-01-04   10110000 -0.010699\n",
      "3     54.391837  0.065650 2013-01-07    6630000 -0.001746\n",
      "4     49.248294  0.067580 2013-01-08   19260000 -0.019407\n",
      "5     48.946272  0.066514 2013-01-09   10470000 -0.001188\n",
      "...         ...       ...        ...        ...       ...\n",
      "1505  31.334964  0.996541 2018-12-24   83398500 -0.079305\n",
      "1506  45.440901  1.081931 2018-12-26  122446500  0.098877\n",
      "1507  42.397807  1.103078 2018-12-27  128626500 -0.031020\n",
      "1508  48.954772  1.120049 2018-12-28  149085000  0.054598\n",
      "1509  48.595460  1.106474 2018-12-31   94534500 -0.003210\n",
      "\n",
      "[1509 rows x 5 columns]\n",
      "1198\n",
      "[[ 5.78206946e+01  6.66548773e-02  1.11300000e+07 -1.68260932e-02]\n",
      " [ 5.48756380e+01  6.60843107e-02  1.01100000e+07 -1.06985374e-02]\n",
      " [ 5.43918374e+01  6.56497130e-02  6.63000000e+06 -1.74568663e-03]\n",
      " ...\n",
      " [ 4.12403679e+01  5.79377609e-01  7.06815000e+07 -1.79675243e-02]\n",
      " [ 4.43219762e+01  5.67898268e-01  6.47445000e+07  1.18662207e-02]\n",
      " [ 4.17784515e+01  5.57857815e-01  5.66580000e+07 -1.27971933e-02]]\n",
      "Length of inputs 1198\n",
      "length of time-series - inputs (1198, 60, 4)\n",
      "length of time-series - outputs (1198,)\n",
      "Training Size (1198, 60, 4) (1198,)\n",
      "Length of inputs 251\n",
      "length of time-series - inputs (251, 60, 4)\n",
      "length of time-series - outputs (251,)\n",
      "Test Size (251, 60, 4) (251,)\n",
      "Reloading Tuner from .\\untitled_project\\tuner0.json\n",
      "WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_23460\\465918227.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_stock.Date = pd.to_datetime(df_stock.Date)\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\base.py:443: UserWarning: X has feature names, but MinMaxScaler was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).layer_with_weights-1.cell.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).layer_with_weights-1.cell.recurrent_kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).layer_with_weights-1.cell.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.19\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.20\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 60, 452)           826256    \n",
      "                                                                 \n",
      " global_average_pooling1d (  (None, 452)               0         \n",
      " GlobalAveragePooling1D)                                         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 60)                27180     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 20)                1220      \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 20)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 21        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 854677 (3.26 MB)\n",
      "Trainable params: 854677 (3.26 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "TSLA None\n",
      "Epoch 1/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.6706WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 5s 151ms/step - loss: 0.6706\n",
      "Epoch 2/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.1100WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 4s 149ms/step - loss: 0.1100\n",
      "Epoch 3/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0601WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 4s 155ms/step - loss: 0.0601\n",
      "Epoch 4/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0485WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 4s 148ms/step - loss: 0.0485\n",
      "Epoch 5/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0462WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 4s 147ms/step - loss: 0.0462\n",
      "Epoch 6/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0424WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 4s 151ms/step - loss: 0.0424\n",
      "Epoch 7/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0416WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 4s 149ms/step - loss: 0.0416\n",
      "Epoch 8/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0393WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 4s 148ms/step - loss: 0.0393\n",
      "Epoch 9/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0393WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 4s 147ms/step - loss: 0.0393\n",
      "Epoch 10/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0398WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 4s 150ms/step - loss: 0.0398\n",
      "6/6 [==============================] - 1s 51ms/step\n",
      "[[0.59488374]\n",
      " [0.5944003 ]\n",
      " [0.5984502 ]\n",
      " [0.5929904 ]\n",
      " [0.5940331 ]\n",
      " [0.5928219 ]\n",
      " [0.5919407 ]\n",
      " [0.5926285 ]\n",
      " [0.5896183 ]\n",
      " [0.5905521 ]\n",
      " [0.5927301 ]\n",
      " [0.5928104 ]\n",
      " [0.5940031 ]\n",
      " [0.5914463 ]\n",
      " [0.5961946 ]\n",
      " [0.5914409 ]\n",
      " [0.5927733 ]\n",
      " [0.5902463 ]\n",
      " [0.589293  ]\n",
      " [0.5990578 ]\n",
      " [0.60202575]\n",
      " [0.592882  ]\n",
      " [0.5946175 ]\n",
      " [0.5914675 ]\n",
      " [0.5918555 ]\n",
      " [0.591467  ]\n",
      " [0.59068465]\n",
      " [0.59086096]\n",
      " [0.59505105]\n",
      " [0.5928385 ]\n",
      " [0.5931746 ]\n",
      " [0.59534085]\n",
      " [0.5963151 ]\n",
      " [0.5923878 ]\n",
      " [0.593701  ]\n",
      " [0.5905598 ]\n",
      " [0.591082  ]\n",
      " [0.5911257 ]\n",
      " [0.595608  ]\n",
      " [0.5908713 ]\n",
      " [0.5908605 ]\n",
      " [0.5905699 ]\n",
      " [0.59003294]\n",
      " [0.58964056]\n",
      " [0.5910485 ]\n",
      " [0.5890809 ]\n",
      " [0.59043205]\n",
      " [0.5919991 ]\n",
      " [0.59345573]\n",
      " [0.5933249 ]\n",
      " [0.5922754 ]\n",
      " [0.5936554 ]\n",
      " [0.5947578 ]\n",
      " [0.59284925]\n",
      " [0.5908674 ]\n",
      " [0.59187657]\n",
      " [0.59166664]\n",
      " [0.5904131 ]\n",
      " [0.5876125 ]\n",
      " [0.5887382 ]\n",
      " [0.58700037]\n",
      " [0.5905762 ]\n",
      " [0.5922455 ]\n",
      " [0.58958924]\n",
      " [0.59091884]\n",
      " [0.5946536 ]\n",
      " [0.59272456]\n",
      " [0.59336174]\n",
      " [0.59322   ]\n",
      " [0.5928514 ]\n",
      " [0.5925069 ]\n",
      " [0.59348285]\n",
      " [0.5916399 ]\n",
      " [0.5926259 ]\n",
      " [0.5925581 ]\n",
      " [0.59441805]\n",
      " [0.5953928 ]\n",
      " [0.5920049 ]\n",
      " [0.5924778 ]\n",
      " [0.59369016]\n",
      " [0.5923209 ]\n",
      " [0.59442806]\n",
      " [0.59484684]\n",
      " [0.59685606]\n",
      " [0.59505224]\n",
      " [0.59523714]\n",
      " [0.6109389 ]\n",
      " [0.60726464]\n",
      " [0.60289395]\n",
      " [0.60126734]\n",
      " [0.6027441 ]\n",
      " [0.5995162 ]\n",
      " [0.6016188 ]\n",
      " [0.6009785 ]\n",
      " [0.600119  ]\n",
      " [0.59803164]\n",
      " [0.5984752 ]\n",
      " [0.5974754 ]\n",
      " [0.60030776]\n",
      " [0.6019561 ]\n",
      " [0.60421133]\n",
      " [0.5993376 ]\n",
      " [0.5999104 ]\n",
      " [0.60037315]\n",
      " [0.59774685]\n",
      " [0.5985067 ]\n",
      " [0.5982443 ]\n",
      " [0.5965912 ]\n",
      " [0.60340095]\n",
      " [0.60709333]\n",
      " [0.60298145]\n",
      " [0.6033323 ]\n",
      " [0.6039959 ]\n",
      " [0.6011703 ]\n",
      " [0.59881115]\n",
      " [0.6032405 ]\n",
      " [0.60438365]\n",
      " [0.6050736 ]\n",
      " [0.6134801 ]\n",
      " [0.61216617]\n",
      " [0.609406  ]\n",
      " [0.61338735]\n",
      " [0.6097358 ]\n",
      " [0.61170894]\n",
      " [0.6124965 ]\n",
      " [0.6154038 ]\n",
      " [0.6153882 ]\n",
      " [0.61179066]\n",
      " [0.6134453 ]\n",
      " [0.6129296 ]\n",
      " [0.60995173]\n",
      " [0.61197495]\n",
      " [0.6101166 ]\n",
      " [0.6073457 ]\n",
      " [0.60674113]\n",
      " [0.6097801 ]\n",
      " [0.60806966]\n",
      " [0.6051876 ]\n",
      " [0.60441947]\n",
      " [0.6009978 ]\n",
      " [0.59935117]\n",
      " [0.6005466 ]\n",
      " [0.59859425]\n",
      " [0.6014433 ]\n",
      " [0.60672307]\n",
      " [0.5992551 ]\n",
      " [0.59942645]\n",
      " [0.60030425]\n",
      " [0.59793997]\n",
      " [0.59914076]\n",
      " [0.59965324]\n",
      " [0.6020732 ]\n",
      " [0.6004174 ]\n",
      " [0.5957606 ]\n",
      " [0.5960306 ]\n",
      " [0.59894407]\n",
      " [0.59565926]\n",
      " [0.60073835]\n",
      " [0.5947394 ]\n",
      " [0.59463376]\n",
      " [0.5927967 ]\n",
      " [0.59272987]\n",
      " [0.5934361 ]\n",
      " [0.59583443]\n",
      " [0.59204185]\n",
      " [0.5922844 ]\n",
      " [0.5967779 ]\n",
      " [0.5961555 ]\n",
      " [0.6026036 ]\n",
      " [0.6004336 ]\n",
      " [0.6004236 ]\n",
      " [0.6035331 ]\n",
      " [0.60276186]\n",
      " [0.6015891 ]\n",
      " [0.60462946]\n",
      " [0.60415995]\n",
      " [0.61071867]\n",
      " [0.60579765]\n",
      " [0.61143184]\n",
      " [0.6120462 ]\n",
      " [0.6089593 ]\n",
      " [0.6074415 ]\n",
      " [0.608389  ]\n",
      " [0.60809183]\n",
      " [0.6112995 ]\n",
      " [0.61500597]\n",
      " [0.616795  ]\n",
      " [0.6141994 ]\n",
      " [0.6123276 ]\n",
      " [0.6094433 ]\n",
      " [0.60953486]\n",
      " [0.6091393 ]\n",
      " [0.608356  ]\n",
      " [0.6068891 ]\n",
      " [0.60873353]\n",
      " [0.6032764 ]\n",
      " [0.60515416]\n",
      " [0.606049  ]\n",
      " [0.60729074]\n",
      " [0.608997  ]\n",
      " [0.6074424 ]\n",
      " [0.60212207]\n",
      " [0.60491383]\n",
      " [0.6071502 ]\n",
      " [0.6058953 ]\n",
      " [0.60113096]\n",
      " [0.60534865]\n",
      " [0.603356  ]\n",
      " [0.6117505 ]\n",
      " [0.6130794 ]\n",
      " [0.6130233 ]\n",
      " [0.6169704 ]\n",
      " [0.61738884]\n",
      " [0.6147514 ]\n",
      " [0.6144159 ]\n",
      " [0.61462885]\n",
      " [0.6146153 ]\n",
      " [0.6139673 ]\n",
      " [0.6176374 ]\n",
      " [0.6142852 ]\n",
      " [0.61198044]\n",
      " [0.6120832 ]\n",
      " [0.6110389 ]\n",
      " [0.60909075]\n",
      " [0.61093235]\n",
      " [0.6099815 ]\n",
      " [0.60862064]\n",
      " [0.6060146 ]\n",
      " [0.6051427 ]\n",
      " [0.6082145 ]\n",
      " [0.6070381 ]\n",
      " [0.6065985 ]\n",
      " [0.6095218 ]\n",
      " [0.6008141 ]\n",
      " [0.6074408 ]\n",
      " [0.6022713 ]\n",
      " [0.6046551 ]\n",
      " [0.60298145]\n",
      " [0.6066253 ]\n",
      " [0.6107373 ]\n",
      " [0.6047981 ]\n",
      " [0.6075922 ]\n",
      " [0.60517824]\n",
      " [0.604243  ]\n",
      " [0.60334015]\n",
      " [0.6034736 ]\n",
      " [0.60890305]\n",
      " [0.613546  ]\n",
      " [0.60593194]\n",
      " [0.615163  ]\n",
      " [0.61452174]]\n",
      "[0.59488374 0.5944003  0.5984502  0.5929904  0.5940331  0.5928219\n",
      " 0.5919407  0.5926285  0.5896183  0.5905521  0.5927301  0.5928104\n",
      " 0.5940031  0.5914463  0.5961946  0.5914409  0.5927733  0.5902463\n",
      " 0.589293   0.5990578  0.60202575 0.592882   0.5946175  0.5914675\n",
      " 0.5918555  0.591467   0.59068465 0.59086096 0.59505105 0.5928385\n",
      " 0.5931746  0.59534085 0.5963151  0.5923878  0.593701   0.5905598\n",
      " 0.591082   0.5911257  0.595608   0.5908713  0.5908605  0.5905699\n",
      " 0.59003294 0.58964056 0.5910485  0.5890809  0.59043205 0.5919991\n",
      " 0.59345573 0.5933249  0.5922754  0.5936554  0.5947578  0.59284925\n",
      " 0.5908674  0.59187657 0.59166664 0.5904131  0.5876125  0.5887382\n",
      " 0.58700037 0.5905762  0.5922455  0.58958924 0.59091884 0.5946536\n",
      " 0.59272456 0.59336174 0.59322    0.5928514  0.5925069  0.59348285\n",
      " 0.5916399  0.5926259  0.5925581  0.59441805 0.5953928  0.5920049\n",
      " 0.5924778  0.59369016 0.5923209  0.59442806 0.59484684 0.59685606\n",
      " 0.59505224 0.59523714 0.6109389  0.60726464 0.60289395 0.60126734\n",
      " 0.6027441  0.5995162  0.6016188  0.6009785  0.600119   0.59803164\n",
      " 0.5984752  0.5974754  0.60030776 0.6019561  0.60421133 0.5993376\n",
      " 0.5999104  0.60037315 0.59774685 0.5985067  0.5982443  0.5965912\n",
      " 0.60340095 0.60709333 0.60298145 0.6033323  0.6039959  0.6011703\n",
      " 0.59881115 0.6032405  0.60438365 0.6050736  0.6134801  0.61216617\n",
      " 0.609406   0.61338735 0.6097358  0.61170894 0.6124965  0.6154038\n",
      " 0.6153882  0.61179066 0.6134453  0.6129296  0.60995173 0.61197495\n",
      " 0.6101166  0.6073457  0.60674113 0.6097801  0.60806966 0.6051876\n",
      " 0.60441947 0.6009978  0.59935117 0.6005466  0.59859425 0.6014433\n",
      " 0.60672307 0.5992551  0.59942645 0.60030425 0.59793997 0.59914076\n",
      " 0.59965324 0.6020732  0.6004174  0.5957606  0.5960306  0.59894407\n",
      " 0.59565926 0.60073835 0.5947394  0.59463376 0.5927967  0.59272987\n",
      " 0.5934361  0.59583443 0.59204185 0.5922844  0.5967779  0.5961555\n",
      " 0.6026036  0.6004336  0.6004236  0.6035331  0.60276186 0.6015891\n",
      " 0.60462946 0.60415995 0.61071867 0.60579765 0.61143184 0.6120462\n",
      " 0.6089593  0.6074415  0.608389   0.60809183 0.6112995  0.61500597\n",
      " 0.616795   0.6141994  0.6123276  0.6094433  0.60953486 0.6091393\n",
      " 0.608356   0.6068891  0.60873353 0.6032764  0.60515416 0.606049\n",
      " 0.60729074 0.608997   0.6074424  0.60212207 0.60491383 0.6071502\n",
      " 0.6058953  0.60113096 0.60534865 0.603356   0.6117505  0.6130794\n",
      " 0.6130233  0.6169704  0.61738884 0.6147514  0.6144159  0.61462885\n",
      " 0.6146153  0.6139673  0.6176374  0.6142852  0.61198044 0.6120832\n",
      " 0.6110389  0.60909075 0.61093235 0.6099815  0.60862064 0.6060146\n",
      " 0.6051427  0.6082145  0.6070381  0.6065985  0.6095218  0.6008141\n",
      " 0.6074408  0.6022713  0.6046551  0.60298145 0.6066253  0.6107373\n",
      " 0.6047981  0.6075922  0.60517824 0.604243   0.60334015 0.6034736\n",
      " 0.60890305 0.613546   0.60593194 0.615163   0.61452174]\n",
      "TSLA 0.21729045485681617\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed\n",
      "            RSI       ATR       Date     Volume   Returns\n",
      "1     59.089676  1.047264 2013-01-03   63140600 -0.008248\n",
      "2     63.705055  1.055316 2013-01-04   72715400  0.035029\n",
      "3     66.424559  1.061365 2013-01-07   83781800  0.022689\n",
      "4     63.624223  1.038411 2013-01-08   45871300 -0.012312\n",
      "5     69.507821  1.074239 2013-01-09  104787700  0.051311\n",
      "...         ...       ...        ...        ...       ...\n",
      "1505  31.108940  5.769428 2018-12-24   22066000 -0.007148\n",
      "1506  45.302163  6.084470 2018-12-26   39723400  0.078417\n",
      "1507  45.706889  6.029865 2018-12-27   31202500  0.002531\n",
      "1508  44.335349  5.864875 2018-12-28   22627600 -0.009861\n",
      "1509  42.157669  5.780956 2018-12-31   24625300 -0.015968\n",
      "\n",
      "[1509 rows x 5 columns]\n",
      "1198\n",
      "[[ 5.90896755e+01  1.04726384e+00  6.31406000e+07 -8.24819237e-03]\n",
      " [ 6.37050553e+01  1.05531641e+00  7.27154000e+07  3.50292230e-02]\n",
      " [ 6.64245590e+01  1.06136533e+00  8.37818000e+07  2.26891770e-02]\n",
      " ...\n",
      " [ 4.93130625e+01  2.63885149e+00  9.49610000e+06  9.21920289e-03]\n",
      " [ 5.01211163e+01  2.54464834e+00  1.22208000e+07  1.68759143e-03]\n",
      " [ 4.62563365e+01  2.53360198e+00  1.02615000e+07 -8.23974093e-03]]\n",
      "Length of inputs 1198\n",
      "length of time-series - inputs (1198, 60, 4)\n",
      "length of time-series - outputs (1198,)\n",
      "Training Size (1198, 60, 4) (1198,)\n",
      "Length of inputs 251\n",
      "length of time-series - inputs (251, 60, 4)\n",
      "length of time-series - outputs (251,)\n",
      "Test Size (251, 60, 4) (251,)\n",
      "Reloading Tuner from .\\untitled_project\\tuner0.json\n",
      "WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_23460\\465918227.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_stock.Date = pd.to_datetime(df_stock.Date)\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\base.py:443: UserWarning: X has feature names, but MinMaxScaler was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).layer_with_weights-1.cell.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).layer_with_weights-1.cell.recurrent_kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).layer_with_weights-1.cell.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.19\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.20\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 60, 392)           622496    \n",
      "                                                                 \n",
      " global_average_pooling1d (  (None, 392)               0         \n",
      " GlobalAveragePooling1D)                                         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 60)                23580     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 20)                1220      \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 20)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 21        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 647317 (2.47 MB)\n",
      "Trainable params: 647317 (2.47 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "META None\n",
      "Epoch 1/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.4559WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 4s 130ms/step - loss: 0.4559\n",
      "Epoch 2/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0913WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 3s 130ms/step - loss: 0.0913\n",
      "Epoch 3/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0505WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 3s 135ms/step - loss: 0.0505\n",
      "Epoch 4/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0465WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 3s 141ms/step - loss: 0.0465\n",
      "Epoch 5/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0493WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 3s 136ms/step - loss: 0.0493\n",
      "Epoch 6/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0559WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 3s 138ms/step - loss: 0.0559\n",
      "Epoch 7/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0496WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 3s 134ms/step - loss: 0.0496\n",
      "Epoch 8/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0412WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 3s 135ms/step - loss: 0.0412\n",
      "Epoch 9/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0400WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 3s 135ms/step - loss: 0.0400\n",
      "Epoch 10/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0358WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 3s 133ms/step - loss: 0.0358\n",
      "6/6 [==============================] - 0s 45ms/step\n",
      "[[0.41964012]\n",
      " [0.41965014]\n",
      " [0.41965336]\n",
      " [0.41964447]\n",
      " [0.41953528]\n",
      " [0.41949424]\n",
      " [0.41946107]\n",
      " [0.4194004 ]\n",
      " [0.41932613]\n",
      " [0.41921455]\n",
      " [0.41925228]\n",
      " [0.41913784]\n",
      " [0.41925436]\n",
      " [0.41921693]\n",
      " [0.41932675]\n",
      " [0.41945064]\n",
      " [0.41973972]\n",
      " [0.41977116]\n",
      " [0.4197392 ]\n",
      " [0.4196365 ]\n",
      " [0.4198522 ]\n",
      " [0.41988045]\n",
      " [0.4198112 ]\n",
      " [0.41973162]\n",
      " [0.4196456 ]\n",
      " [0.41959888]\n",
      " [0.41951045]\n",
      " [0.41944557]\n",
      " [0.41941854]\n",
      " [0.4194216 ]\n",
      " [0.41937438]\n",
      " [0.41930407]\n",
      " [0.4193158 ]\n",
      " [0.41937476]\n",
      " [0.41935283]\n",
      " [0.41939402]\n",
      " [0.41931456]\n",
      " [0.41922373]\n",
      " [0.4196842 ]\n",
      " [0.41971081]\n",
      " [0.4200883 ]\n",
      " [0.4205013 ]\n",
      " [0.42063937]\n",
      " [0.42065406]\n",
      " [0.4206244 ]\n",
      " [0.42071822]\n",
      " [0.42058557]\n",
      " [0.42060834]\n",
      " [0.4204604 ]\n",
      " [0.42042246]\n",
      " [0.42026445]\n",
      " [0.42017198]\n",
      " [0.42013907]\n",
      " [0.42010963]\n",
      " [0.4199828 ]\n",
      " [0.41989318]\n",
      " [0.4198964 ]\n",
      " [0.4198251 ]\n",
      " [0.41977414]\n",
      " [0.41989216]\n",
      " [0.42006236]\n",
      " [0.42010444]\n",
      " [0.42014033]\n",
      " [0.42012182]\n",
      " [0.42008778]\n",
      " [0.420049  ]\n",
      " [0.41998586]\n",
      " [0.41972864]\n",
      " [0.4200806 ]\n",
      " [0.4202482 ]\n",
      " [0.42038321]\n",
      " [0.42041522]\n",
      " [0.42048848]\n",
      " [0.42059582]\n",
      " [0.42058545]\n",
      " [0.42077702]\n",
      " [0.42068264]\n",
      " [0.42059475]\n",
      " [0.4208483 ]\n",
      " [0.42093745]\n",
      " [0.42109668]\n",
      " [0.42109197]\n",
      " [0.42160323]\n",
      " [0.42220372]\n",
      " [0.42209515]\n",
      " [0.42253387]\n",
      " [0.42283857]\n",
      " [0.42287302]\n",
      " [0.42297226]\n",
      " [0.4229676 ]\n",
      " [0.42276537]\n",
      " [0.4228211 ]\n",
      " [0.4228146 ]\n",
      " [0.4227417 ]\n",
      " [0.42263442]\n",
      " [0.4225309 ]\n",
      " [0.4222712 ]\n",
      " [0.42217898]\n",
      " [0.42231923]\n",
      " [0.4224154 ]\n",
      " [0.42247537]\n",
      " [0.42239586]\n",
      " [0.4222653 ]\n",
      " [0.42232352]\n",
      " [0.42215097]\n",
      " [0.4221853 ]\n",
      " [0.4218862 ]\n",
      " [0.42191666]\n",
      " [0.42199558]\n",
      " [0.42166474]\n",
      " [0.4214747 ]\n",
      " [0.4212647 ]\n",
      " [0.42162168]\n",
      " [0.42177582]\n",
      " [0.42213976]\n",
      " [0.422405  ]\n",
      " [0.42243007]\n",
      " [0.4226459 ]\n",
      " [0.42294696]\n",
      " [0.423028  ]\n",
      " [0.42313945]\n",
      " [0.4232237 ]\n",
      " [0.42320567]\n",
      " [0.42318198]\n",
      " [0.4231548 ]\n",
      " [0.42318898]\n",
      " [0.4231177 ]\n",
      " [0.42297745]\n",
      " [0.42306882]\n",
      " [0.42310208]\n",
      " [0.422934  ]\n",
      " [0.4227768 ]\n",
      " [0.42254522]\n",
      " [0.42254788]\n",
      " [0.4223172 ]\n",
      " [0.42227   ]\n",
      " [0.422287  ]\n",
      " [0.42253974]\n",
      " [0.42251253]\n",
      " [0.42236674]\n",
      " [0.42274356]\n",
      " [0.4227109 ]\n",
      " [0.42250216]\n",
      " [0.42240945]\n",
      " [0.4224952 ]\n",
      " [0.42227268]\n",
      " [0.42212677]\n",
      " [0.4219867 ]\n",
      " [0.4217944 ]\n",
      " [0.4217046 ]\n",
      " [0.4216143 ]\n",
      " [0.42151725]\n",
      " [0.42150736]\n",
      " [0.42136902]\n",
      " [0.4211906 ]\n",
      " [0.42113793]\n",
      " [0.4209751 ]\n",
      " [0.42099726]\n",
      " [0.42088658]\n",
      " [0.42086738]\n",
      " [0.42083177]\n",
      " [0.42079407]\n",
      " [0.42078352]\n",
      " [0.42076847]\n",
      " [0.4207031 ]\n",
      " [0.42068005]\n",
      " [0.42061698]\n",
      " [0.42067546]\n",
      " [0.4208547 ]\n",
      " [0.42080635]\n",
      " [0.42074153]\n",
      " [0.42066473]\n",
      " [0.42069164]\n",
      " [0.42063147]\n",
      " [0.4207099 ]\n",
      " [0.42075148]\n",
      " [0.4208992 ]\n",
      " [0.4208455 ]\n",
      " [0.4208634 ]\n",
      " [0.42084908]\n",
      " [0.4212532 ]\n",
      " [0.42106056]\n",
      " [0.42132783]\n",
      " [0.42127344]\n",
      " [0.42138958]\n",
      " [0.4212789 ]\n",
      " [0.42156434]\n",
      " [0.42136258]\n",
      " [0.42134902]\n",
      " [0.42132485]\n",
      " [0.42133802]\n",
      " [0.42132837]\n",
      " [0.42112482]\n",
      " [0.42105883]\n",
      " [0.4210255 ]\n",
      " [0.4209707 ]\n",
      " [0.42094988]\n",
      " [0.42093804]\n",
      " [0.42083508]\n",
      " [0.42080742]\n",
      " [0.42056024]\n",
      " [0.42092222]\n",
      " [0.4218918 ]\n",
      " [0.42287275]\n",
      " [0.42294735]\n",
      " [0.4229682 ]\n",
      " [0.42296195]\n",
      " [0.4229641 ]\n",
      " [0.42289346]\n",
      " [0.42280945]\n",
      " [0.42289868]\n",
      " [0.4227854 ]\n",
      " [0.42275584]\n",
      " [0.4226699 ]\n",
      " [0.42254078]\n",
      " [0.422432  ]\n",
      " [0.42251912]\n",
      " [0.4225915 ]\n",
      " [0.42253298]\n",
      " [0.42247105]\n",
      " [0.422337  ]\n",
      " [0.4222076 ]\n",
      " [0.42210612]\n",
      " [0.42190683]\n",
      " [0.42177042]\n",
      " [0.4217342 ]\n",
      " [0.42157704]\n",
      " [0.4214295 ]\n",
      " [0.42150894]\n",
      " [0.42160678]\n",
      " [0.4216769 ]\n",
      " [0.4218191 ]\n",
      " [0.42181954]\n",
      " [0.42172703]\n",
      " [0.42165428]\n",
      " [0.42164755]\n",
      " [0.4214835 ]\n",
      " [0.421369  ]\n",
      " [0.4213481 ]\n",
      " [0.42128575]\n",
      " [0.42116678]\n",
      " [0.42110068]\n",
      " [0.42133796]\n",
      " [0.4213103 ]\n",
      " [0.42144185]\n",
      " [0.4213858 ]\n",
      " [0.42145535]\n",
      " [0.4218136 ]\n",
      " [0.42184818]\n",
      " [0.4218365 ]\n",
      " [0.42171642]]\n",
      "[0.41964012 0.41965014 0.41965336 0.41964447 0.41953528 0.41949424\n",
      " 0.41946107 0.4194004  0.41932613 0.41921455 0.41925228 0.41913784\n",
      " 0.41925436 0.41921693 0.41932675 0.41945064 0.41973972 0.41977116\n",
      " 0.4197392  0.4196365  0.4198522  0.41988045 0.4198112  0.41973162\n",
      " 0.4196456  0.41959888 0.41951045 0.41944557 0.41941854 0.4194216\n",
      " 0.41937438 0.41930407 0.4193158  0.41937476 0.41935283 0.41939402\n",
      " 0.41931456 0.41922373 0.4196842  0.41971081 0.4200883  0.4205013\n",
      " 0.42063937 0.42065406 0.4206244  0.42071822 0.42058557 0.42060834\n",
      " 0.4204604  0.42042246 0.42026445 0.42017198 0.42013907 0.42010963\n",
      " 0.4199828  0.41989318 0.4198964  0.4198251  0.41977414 0.41989216\n",
      " 0.42006236 0.42010444 0.42014033 0.42012182 0.42008778 0.420049\n",
      " 0.41998586 0.41972864 0.4200806  0.4202482  0.42038321 0.42041522\n",
      " 0.42048848 0.42059582 0.42058545 0.42077702 0.42068264 0.42059475\n",
      " 0.4208483  0.42093745 0.42109668 0.42109197 0.42160323 0.42220372\n",
      " 0.42209515 0.42253387 0.42283857 0.42287302 0.42297226 0.4229676\n",
      " 0.42276537 0.4228211  0.4228146  0.4227417  0.42263442 0.4225309\n",
      " 0.4222712  0.42217898 0.42231923 0.4224154  0.42247537 0.42239586\n",
      " 0.4222653  0.42232352 0.42215097 0.4221853  0.4218862  0.42191666\n",
      " 0.42199558 0.42166474 0.4214747  0.4212647  0.42162168 0.42177582\n",
      " 0.42213976 0.422405   0.42243007 0.4226459  0.42294696 0.423028\n",
      " 0.42313945 0.4232237  0.42320567 0.42318198 0.4231548  0.42318898\n",
      " 0.4231177  0.42297745 0.42306882 0.42310208 0.422934   0.4227768\n",
      " 0.42254522 0.42254788 0.4223172  0.42227    0.422287   0.42253974\n",
      " 0.42251253 0.42236674 0.42274356 0.4227109  0.42250216 0.42240945\n",
      " 0.4224952  0.42227268 0.42212677 0.4219867  0.4217944  0.4217046\n",
      " 0.4216143  0.42151725 0.42150736 0.42136902 0.4211906  0.42113793\n",
      " 0.4209751  0.42099726 0.42088658 0.42086738 0.42083177 0.42079407\n",
      " 0.42078352 0.42076847 0.4207031  0.42068005 0.42061698 0.42067546\n",
      " 0.4208547  0.42080635 0.42074153 0.42066473 0.42069164 0.42063147\n",
      " 0.4207099  0.42075148 0.4208992  0.4208455  0.4208634  0.42084908\n",
      " 0.4212532  0.42106056 0.42132783 0.42127344 0.42138958 0.4212789\n",
      " 0.42156434 0.42136258 0.42134902 0.42132485 0.42133802 0.42132837\n",
      " 0.42112482 0.42105883 0.4210255  0.4209707  0.42094988 0.42093804\n",
      " 0.42083508 0.42080742 0.42056024 0.42092222 0.4218918  0.42287275\n",
      " 0.42294735 0.4229682  0.42296195 0.4229641  0.42289346 0.42280945\n",
      " 0.42289868 0.4227854  0.42275584 0.4226699  0.42254078 0.422432\n",
      " 0.42251912 0.4225915  0.42253298 0.42247105 0.422337   0.4222076\n",
      " 0.42210612 0.42190683 0.42177042 0.4217342  0.42157704 0.4214295\n",
      " 0.42150894 0.42160678 0.4216769  0.4218191  0.42181954 0.42172703\n",
      " 0.42165428 0.42164755 0.4214835  0.421369   0.4213481  0.42128575\n",
      " 0.42116678 0.42110068 0.42133796 0.4213103  0.42144185 0.4213858\n",
      " 0.42145535 0.4218136  0.42184818 0.4218365  0.42171642]\n",
      "META 0.3842658199836015\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed\n",
      "            RSI       ATR       Date   Volume   Returns\n",
      "1     64.003149  0.599988 2013-01-03  1339800 -0.017466\n",
      "2     66.209028  0.590703 2013-01-04  1457200  0.006167\n",
      "3     66.740955  0.572081 2013-01-07  1432900  0.001489\n",
      "4     59.434383  0.566218 2013-01-08  1892900 -0.010847\n",
      "5     63.312393  0.580060 2013-01-09  1459500  0.009731\n",
      "...         ...       ...        ...      ...       ...\n",
      "1505  42.509934  0.791440 2018-12-24  1771100 -0.004695\n",
      "1506  49.653455  0.822052 2018-12-26  2421200  0.021079\n",
      "1507  45.870616  0.824048 2018-12-27  3242700 -0.012938\n",
      "1508  48.802967  0.811616 2018-12-28  2190500  0.009050\n",
      "1509  49.123903  0.793644 2018-12-31  1719900  0.000974\n",
      "\n",
      "[1509 rows x 5 columns]\n",
      "1198\n",
      "[[ 6.40031489e+01  5.99987739e-01  1.33980000e+06 -1.74659142e-02]\n",
      " [ 6.62090282e+01  5.90702987e-01  1.45720000e+06  6.16651659e-03]\n",
      " [ 6.67409549e+01  5.72081204e-01  1.43290000e+06  1.48901530e-03]\n",
      " ...\n",
      " [ 6.44897567e+01  4.69127944e-01  1.45950000e+06 -7.76912042e-04]\n",
      " [ 6.48088388e+01  4.49190136e-01  1.20730000e+06  7.76912042e-04]\n",
      " [ 6.58019201e+01  4.41390851e-01  2.01340000e+06  2.32633701e-03]]\n",
      "Length of inputs 1198\n",
      "length of time-series - inputs (1198, 60, 4)\n",
      "length of time-series - outputs (1198,)\n",
      "Training Size (1198, 60, 4) (1198,)\n",
      "Length of inputs 251\n",
      "length of time-series - inputs (251, 60, 4)\n",
      "length of time-series - outputs (251,)\n",
      "Test Size (251, 60, 4) (251,)\n",
      "Reloading Tuner from .\\untitled_project\\tuner0.json\n",
      "WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_23460\\465918227.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_stock.Date = pd.to_datetime(df_stock.Date)\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\base.py:443: UserWarning: X has feature names, but MinMaxScaler was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).layer_with_weights-1.cell.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).layer_with_weights-1.cell.recurrent_kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).layer_with_weights-1.cell.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.19\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.20\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 60, 272)           301376    \n",
      "                                                                 \n",
      " global_average_pooling1d (  (None, 272)               0         \n",
      " GlobalAveragePooling1D)                                         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 60)                16380     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 20)                1220      \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 20)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 21        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 318997 (1.22 MB)\n",
      "Trainable params: 318997 (1.22 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "HSBC None\n",
      "Epoch 1/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.2459WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 3s 82ms/step - loss: 0.2459\n",
      "Epoch 2/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.1046WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 2s 81ms/step - loss: 0.1046\n",
      "Epoch 3/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0659WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 2s 81ms/step - loss: 0.0659\n",
      "Epoch 4/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0570WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 2s 82ms/step - loss: 0.0570\n",
      "Epoch 5/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0579WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 2s 81ms/step - loss: 0.0579\n",
      "Epoch 6/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0576WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 2s 80ms/step - loss: 0.0576\n",
      "Epoch 7/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0561WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 2s 79ms/step - loss: 0.0561\n",
      "Epoch 8/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0504WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 2s 79ms/step - loss: 0.0504\n",
      "Epoch 9/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0458WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 2s 79ms/step - loss: 0.0458\n",
      "Epoch 10/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0442WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 2s 80ms/step - loss: 0.0442\n",
      "6/6 [==============================] - 0s 22ms/step\n",
      "[[0.3545072 ]\n",
      " [0.3540405 ]\n",
      " [0.35369587]\n",
      " [0.35381484]\n",
      " [0.35365468]\n",
      " [0.35338598]\n",
      " [0.35321975]\n",
      " [0.3530991 ]\n",
      " [0.3529786 ]\n",
      " [0.3531289 ]\n",
      " [0.35303092]\n",
      " [0.35322642]\n",
      " [0.35307628]\n",
      " [0.35328   ]\n",
      " [0.35341942]\n",
      " [0.35338318]\n",
      " [0.35349107]\n",
      " [0.35338718]\n",
      " [0.35358757]\n",
      " [0.3537442 ]\n",
      " [0.35380828]\n",
      " [0.3540811 ]\n",
      " [0.3542008 ]\n",
      " [0.35440743]\n",
      " [0.3552649 ]\n",
      " [0.3560558 ]\n",
      " [0.35667405]\n",
      " [0.35720593]\n",
      " [0.35824537]\n",
      " [0.35929018]\n",
      " [0.35998505]\n",
      " [0.36085886]\n",
      " [0.3616612 ]\n",
      " [0.36251962]\n",
      " [0.36398822]\n",
      " [0.3645804 ]\n",
      " [0.3654604 ]\n",
      " [0.36664897]\n",
      " [0.3673679 ]\n",
      " [0.36807686]\n",
      " [0.3688988 ]\n",
      " [0.36953676]\n",
      " [0.37003356]\n",
      " [0.37042952]\n",
      " [0.37125987]\n",
      " [0.3720625 ]\n",
      " [0.3728599 ]\n",
      " [0.37334   ]\n",
      " [0.37412423]\n",
      " [0.3743541 ]\n",
      " [0.3743109 ]\n",
      " [0.37491497]\n",
      " [0.37518108]\n",
      " [0.37569028]\n",
      " [0.37647057]\n",
      " [0.37693536]\n",
      " [0.3772623 ]\n",
      " [0.37767035]\n",
      " [0.3780744 ]\n",
      " [0.37862122]\n",
      " [0.37949187]\n",
      " [0.37991387]\n",
      " [0.3804334 ]\n",
      " [0.38087916]\n",
      " [0.38119185]\n",
      " [0.38180897]\n",
      " [0.38285583]\n",
      " [0.38325292]\n",
      " [0.38364902]\n",
      " [0.38399225]\n",
      " [0.38462222]\n",
      " [0.3848424 ]\n",
      " [0.38510084]\n",
      " [0.3853461 ]\n",
      " [0.38549393]\n",
      " [0.38578463]\n",
      " [0.38590717]\n",
      " [0.3861258 ]\n",
      " [0.3859917 ]\n",
      " [0.3858704 ]\n",
      " [0.38571474]\n",
      " [0.38553762]\n",
      " [0.38505214]\n",
      " [0.38459998]\n",
      " [0.3850394 ]\n",
      " [0.38449392]\n",
      " [0.3842157 ]\n",
      " [0.38435298]\n",
      " [0.3838749 ]\n",
      " [0.38362634]\n",
      " [0.38354683]\n",
      " [0.3831116 ]\n",
      " [0.38261622]\n",
      " [0.3816933 ]\n",
      " [0.38145489]\n",
      " [0.38086593]\n",
      " [0.38029522]\n",
      " [0.37953806]\n",
      " [0.3785801 ]\n",
      " [0.37806475]\n",
      " [0.3776691 ]\n",
      " [0.37725177]\n",
      " [0.3768469 ]\n",
      " [0.37653634]\n",
      " [0.3761149 ]\n",
      " [0.37577015]\n",
      " [0.3756811 ]\n",
      " [0.3752429 ]\n",
      " [0.37474614]\n",
      " [0.37445122]\n",
      " [0.37450826]\n",
      " [0.37437785]\n",
      " [0.37395793]\n",
      " [0.3735503 ]\n",
      " [0.37309656]\n",
      " [0.37216663]\n",
      " [0.3720588 ]\n",
      " [0.37248522]\n",
      " [0.3719486 ]\n",
      " [0.37185794]\n",
      " [0.37158316]\n",
      " [0.37127846]\n",
      " [0.3712235 ]\n",
      " [0.37121546]\n",
      " [0.3710935 ]\n",
      " [0.3707496 ]\n",
      " [0.37110186]\n",
      " [0.37114206]\n",
      " [0.3710268 ]\n",
      " [0.37118864]\n",
      " [0.37105453]\n",
      " [0.37075853]\n",
      " [0.3707412 ]\n",
      " [0.37058526]\n",
      " [0.3709473 ]\n",
      " [0.3709012 ]\n",
      " [0.3709128 ]\n",
      " [0.37086654]\n",
      " [0.37086418]\n",
      " [0.3706578 ]\n",
      " [0.37055635]\n",
      " [0.37037826]\n",
      " [0.3702215 ]\n",
      " [0.3701343 ]\n",
      " [0.3701641 ]\n",
      " [0.36981946]\n",
      " [0.3696944 ]\n",
      " [0.3696699 ]\n",
      " [0.37026682]\n",
      " [0.37043172]\n",
      " [0.3705933 ]\n",
      " [0.37065166]\n",
      " [0.37070328]\n",
      " [0.3707809 ]\n",
      " [0.37084144]\n",
      " [0.37098742]\n",
      " [0.37110072]\n",
      " [0.37146723]\n",
      " [0.3712798 ]\n",
      " [0.3712781 ]\n",
      " [0.3712165 ]\n",
      " [0.37085438]\n",
      " [0.3713296 ]\n",
      " [0.3711416 ]\n",
      " [0.37135482]\n",
      " [0.3714348 ]\n",
      " [0.3712064 ]\n",
      " [0.37148052]\n",
      " [0.37145227]\n",
      " [0.37142366]\n",
      " [0.37138534]\n",
      " [0.37135112]\n",
      " [0.37137288]\n",
      " [0.37129462]\n",
      " [0.3710056 ]\n",
      " [0.37099302]\n",
      " [0.37072366]\n",
      " [0.37081504]\n",
      " [0.37032413]\n",
      " [0.3707559 ]\n",
      " [0.37018663]\n",
      " [0.37032402]\n",
      " [0.36967134]\n",
      " [0.36984783]\n",
      " [0.36976108]\n",
      " [0.3692028 ]\n",
      " [0.36917037]\n",
      " [0.36911488]\n",
      " [0.3689764 ]\n",
      " [0.3691463 ]\n",
      " [0.3687566 ]\n",
      " [0.36843586]\n",
      " [0.3688069 ]\n",
      " [0.3685882 ]\n",
      " [0.36856645]\n",
      " [0.3682779 ]\n",
      " [0.36852515]\n",
      " [0.36841086]\n",
      " [0.3686343 ]\n",
      " [0.36891842]\n",
      " [0.36935163]\n",
      " [0.3694294 ]\n",
      " [0.36926442]\n",
      " [0.3693663 ]\n",
      " [0.3693084 ]\n",
      " [0.3695095 ]\n",
      " [0.36922383]\n",
      " [0.36913735]\n",
      " [0.36953956]\n",
      " [0.36932403]\n",
      " [0.36963457]\n",
      " [0.36980963]\n",
      " [0.36980337]\n",
      " [0.36972755]\n",
      " [0.3696279 ]\n",
      " [0.36964816]\n",
      " [0.36934838]\n",
      " [0.36965334]\n",
      " [0.36964744]\n",
      " [0.36978608]\n",
      " [0.36992905]\n",
      " [0.37017795]\n",
      " [0.36996168]\n",
      " [0.37041056]\n",
      " [0.37121695]\n",
      " [0.3712146 ]\n",
      " [0.37164158]\n",
      " [0.37152237]\n",
      " [0.37179983]\n",
      " [0.37223446]\n",
      " [0.37237632]\n",
      " [0.3724755 ]\n",
      " [0.3725943 ]\n",
      " [0.3730926 ]\n",
      " [0.3734284 ]\n",
      " [0.3742416 ]\n",
      " [0.37538844]\n",
      " [0.37584126]\n",
      " [0.37630635]\n",
      " [0.3767866 ]\n",
      " [0.37807298]\n",
      " [0.3788734 ]\n",
      " [0.3793022 ]\n",
      " [0.37967497]\n",
      " [0.38021004]\n",
      " [0.3807779 ]\n",
      " [0.38125658]\n",
      " [0.38133967]\n",
      " [0.38189548]\n",
      " [0.38211846]\n",
      " [0.3829801 ]]\n",
      "[0.3545072  0.3540405  0.35369587 0.35381484 0.35365468 0.35338598\n",
      " 0.35321975 0.3530991  0.3529786  0.3531289  0.35303092 0.35322642\n",
      " 0.35307628 0.35328    0.35341942 0.35338318 0.35349107 0.35338718\n",
      " 0.35358757 0.3537442  0.35380828 0.3540811  0.3542008  0.35440743\n",
      " 0.3552649  0.3560558  0.35667405 0.35720593 0.35824537 0.35929018\n",
      " 0.35998505 0.36085886 0.3616612  0.36251962 0.36398822 0.3645804\n",
      " 0.3654604  0.36664897 0.3673679  0.36807686 0.3688988  0.36953676\n",
      " 0.37003356 0.37042952 0.37125987 0.3720625  0.3728599  0.37334\n",
      " 0.37412423 0.3743541  0.3743109  0.37491497 0.37518108 0.37569028\n",
      " 0.37647057 0.37693536 0.3772623  0.37767035 0.3780744  0.37862122\n",
      " 0.37949187 0.37991387 0.3804334  0.38087916 0.38119185 0.38180897\n",
      " 0.38285583 0.38325292 0.38364902 0.38399225 0.38462222 0.3848424\n",
      " 0.38510084 0.3853461  0.38549393 0.38578463 0.38590717 0.3861258\n",
      " 0.3859917  0.3858704  0.38571474 0.38553762 0.38505214 0.38459998\n",
      " 0.3850394  0.38449392 0.3842157  0.38435298 0.3838749  0.38362634\n",
      " 0.38354683 0.3831116  0.38261622 0.3816933  0.38145489 0.38086593\n",
      " 0.38029522 0.37953806 0.3785801  0.37806475 0.3776691  0.37725177\n",
      " 0.3768469  0.37653634 0.3761149  0.37577015 0.3756811  0.3752429\n",
      " 0.37474614 0.37445122 0.37450826 0.37437785 0.37395793 0.3735503\n",
      " 0.37309656 0.37216663 0.3720588  0.37248522 0.3719486  0.37185794\n",
      " 0.37158316 0.37127846 0.3712235  0.37121546 0.3710935  0.3707496\n",
      " 0.37110186 0.37114206 0.3710268  0.37118864 0.37105453 0.37075853\n",
      " 0.3707412  0.37058526 0.3709473  0.3709012  0.3709128  0.37086654\n",
      " 0.37086418 0.3706578  0.37055635 0.37037826 0.3702215  0.3701343\n",
      " 0.3701641  0.36981946 0.3696944  0.3696699  0.37026682 0.37043172\n",
      " 0.3705933  0.37065166 0.37070328 0.3707809  0.37084144 0.37098742\n",
      " 0.37110072 0.37146723 0.3712798  0.3712781  0.3712165  0.37085438\n",
      " 0.3713296  0.3711416  0.37135482 0.3714348  0.3712064  0.37148052\n",
      " 0.37145227 0.37142366 0.37138534 0.37135112 0.37137288 0.37129462\n",
      " 0.3710056  0.37099302 0.37072366 0.37081504 0.37032413 0.3707559\n",
      " 0.37018663 0.37032402 0.36967134 0.36984783 0.36976108 0.3692028\n",
      " 0.36917037 0.36911488 0.3689764  0.3691463  0.3687566  0.36843586\n",
      " 0.3688069  0.3685882  0.36856645 0.3682779  0.36852515 0.36841086\n",
      " 0.3686343  0.36891842 0.36935163 0.3694294  0.36926442 0.3693663\n",
      " 0.3693084  0.3695095  0.36922383 0.36913735 0.36953956 0.36932403\n",
      " 0.36963457 0.36980963 0.36980337 0.36972755 0.3696279  0.36964816\n",
      " 0.36934838 0.36965334 0.36964744 0.36978608 0.36992905 0.37017795\n",
      " 0.36996168 0.37041056 0.37121695 0.3712146  0.37164158 0.37152237\n",
      " 0.37179983 0.37223446 0.37237632 0.3724755  0.3725943  0.3730926\n",
      " 0.3734284  0.3742416  0.37538844 0.37584126 0.37630635 0.3767866\n",
      " 0.37807298 0.3788734  0.3793022  0.37967497 0.38021004 0.3807779\n",
      " 0.38125658 0.38133967 0.38189548 0.38211846 0.3829801 ]\n",
      "HSBC 0.056329563345672684\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed\n",
      "            RSI       ATR       Date    Volume   Returns\n",
      "1     56.656129  0.835728 2013-01-03   7448100  0.004232\n",
      "2     67.227326  0.931748 2013-01-04  13587200  0.036339\n",
      "3     66.562325  0.928766 2013-01-07   6989100 -0.001359\n",
      "4     68.435921  0.915283 2013-01-08   5246100  0.007546\n",
      "5     72.991097  0.934191 2013-01-09   7267200  0.020792\n",
      "...         ...       ...        ...       ...       ...\n",
      "1505  37.523475  2.825029 2018-12-24   3284000 -0.024704\n",
      "1506  48.992265  2.956098 2018-12-26   3792400  0.039851\n",
      "1507  52.857546  3.042805 2018-12-27   4101600  0.016073\n",
      "1508  55.500462  3.004033 2018-12-28   3984000  0.011537\n",
      "1509  58.413726  2.907317 2018-12-31   2731000  0.013222\n",
      "\n",
      "[1509 rows x 5 columns]\n",
      "1198\n",
      "[[ 5.66561291e+01  8.35728457e-01  7.44810000e+06  4.23244275e-03]\n",
      " [ 6.72273265e+01  9.31747875e-01  1.35872000e+07  3.63391519e-02]\n",
      " [ 6.65623249e+01  9.28766112e-01  6.98910000e+06 -1.35864120e-03]\n",
      " ...\n",
      " [ 4.81843928e+01  1.13312494e+00  1.33280000e+06  1.76324212e-03]\n",
      " [ 4.74814176e+01  1.09147338e+00  1.73170000e+06 -1.05757214e-03]\n",
      " [ 4.30477185e+01  1.08351123e+00  2.56380000e+06 -6.96127817e-03]]\n",
      "Length of inputs 1198\n",
      "length of time-series - inputs (1198, 60, 4)\n",
      "length of time-series - outputs (1198,)\n",
      "Training Size (1198, 60, 4) (1198,)\n",
      "Length of inputs 251\n",
      "length of time-series - inputs (251, 60, 4)\n",
      "length of time-series - outputs (251,)\n",
      "Test Size (251, 60, 4) (251,)\n",
      "Reloading Tuner from .\\untitled_project\\tuner0.json\n",
      "WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_23460\\465918227.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_stock.Date = pd.to_datetime(df_stock.Date)\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\base.py:443: UserWarning: X has feature names, but MinMaxScaler was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).layer_with_weights-1.cell.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).layer_with_weights-1.cell.recurrent_kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).layer_with_weights-1.cell.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.19\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.20\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 60, 452)           826256    \n",
      "                                                                 \n",
      " global_average_pooling1d (  (None, 452)               0         \n",
      " GlobalAveragePooling1D)                                         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 60)                27180     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 20)                1220      \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 20)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 21        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 854677 (3.26 MB)\n",
      "Trainable params: 854677 (3.26 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "LLY None\n",
      "Epoch 1/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 1.3497WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 5s 151ms/step - loss: 1.3497\n",
      "Epoch 2/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0685WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 4s 152ms/step - loss: 0.0685\n",
      "Epoch 3/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0518WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 4s 158ms/step - loss: 0.0518\n",
      "Epoch 4/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0625WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 4s 152ms/step - loss: 0.0625\n",
      "Epoch 5/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0503WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 4s 149ms/step - loss: 0.0503\n",
      "Epoch 6/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0400WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 4s 150ms/step - loss: 0.0400\n",
      "Epoch 7/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0442WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 4s 152ms/step - loss: 0.0442\n",
      "Epoch 8/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0584WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 4s 150ms/step - loss: 0.0584\n",
      "Epoch 9/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0458WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 4s 150ms/step - loss: 0.0458\n",
      "Epoch 10/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0549WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 4s 151ms/step - loss: 0.0549\n",
      "6/6 [==============================] - 1s 52ms/step\n",
      "[[0.32309645]\n",
      " [0.32314658]\n",
      " [0.32317612]\n",
      " [0.32324636]\n",
      " [0.3232903 ]\n",
      " [0.32317978]\n",
      " [0.3231491 ]\n",
      " [0.32312888]\n",
      " [0.32317212]\n",
      " [0.32311362]\n",
      " [0.3230429 ]\n",
      " [0.32299006]\n",
      " [0.323107  ]\n",
      " [0.32405415]\n",
      " [0.324428  ]\n",
      " [0.32451826]\n",
      " [0.32464328]\n",
      " [0.32472503]\n",
      " [0.3247053 ]\n",
      " [0.32472318]\n",
      " [0.32470763]\n",
      " [0.3245719 ]\n",
      " [0.32431495]\n",
      " [0.3239983 ]\n",
      " [0.32395232]\n",
      " [0.32384354]\n",
      " [0.32391882]\n",
      " [0.3238274 ]\n",
      " [0.32378983]\n",
      " [0.32384127]\n",
      " [0.32390103]\n",
      " [0.3236585 ]\n",
      " [0.32353014]\n",
      " [0.32347938]\n",
      " [0.32335788]\n",
      " [0.32320833]\n",
      " [0.32312018]\n",
      " [0.3229892 ]\n",
      " [0.32296538]\n",
      " [0.32289878]\n",
      " [0.32298523]\n",
      " [0.32296127]\n",
      " [0.3230573 ]\n",
      " [0.32323998]\n",
      " [0.32319176]\n",
      " [0.32310653]\n",
      " [0.3230081 ]\n",
      " [0.32292283]\n",
      " [0.3232394 ]\n",
      " [0.32365483]\n",
      " [0.32363164]\n",
      " [0.3237763 ]\n",
      " [0.32371792]\n",
      " [0.3238594 ]\n",
      " [0.32382706]\n",
      " [0.32381776]\n",
      " [0.32363945]\n",
      " [0.32346117]\n",
      " [0.32336384]\n",
      " [0.32329577]\n",
      " [0.323232  ]\n",
      " [0.32319778]\n",
      " [0.32309955]\n",
      " [0.32310823]\n",
      " [0.32310796]\n",
      " [0.3230585 ]\n",
      " [0.32297665]\n",
      " [0.32285985]\n",
      " [0.32302365]\n",
      " [0.3229921 ]\n",
      " [0.32316145]\n",
      " [0.32307553]\n",
      " [0.32309216]\n",
      " [0.32367563]\n",
      " [0.3238753 ]\n",
      " [0.3237291 ]\n",
      " [0.32357627]\n",
      " [0.32369512]\n",
      " [0.32380942]\n",
      " [0.32453722]\n",
      " [0.32629466]\n",
      " [0.32588893]\n",
      " [0.3266625 ]\n",
      " [0.32772845]\n",
      " [0.3279515 ]\n",
      " [0.32812712]\n",
      " [0.3283881 ]\n",
      " [0.3284199 ]\n",
      " [0.3285695 ]\n",
      " [0.3282696 ]\n",
      " [0.32779908]\n",
      " [0.3273723 ]\n",
      " [0.32724905]\n",
      " [0.32761824]\n",
      " [0.3271638 ]\n",
      " [0.3268602 ]\n",
      " [0.32629555]\n",
      " [0.3261714 ]\n",
      " [0.32680818]\n",
      " [0.32709247]\n",
      " [0.32688072]\n",
      " [0.32647914]\n",
      " [0.32629988]\n",
      " [0.3262514 ]\n",
      " [0.32580042]\n",
      " [0.32537177]\n",
      " [0.3251781 ]\n",
      " [0.32521373]\n",
      " [0.3250522 ]\n",
      " [0.3252165 ]\n",
      " [0.3249275 ]\n",
      " [0.32473123]\n",
      " [0.32544217]\n",
      " [0.32529932]\n",
      " [0.3252312 ]\n",
      " [0.3257113 ]\n",
      " [0.3259958 ]\n",
      " [0.32556176]\n",
      " [0.32558972]\n",
      " [0.32535845]\n",
      " [0.32541063]\n",
      " [0.32562923]\n",
      " [0.32524848]\n",
      " [0.32530886]\n",
      " [0.32528943]\n",
      " [0.325712  ]\n",
      " [0.32536274]\n",
      " [0.3253342 ]\n",
      " [0.32526863]\n",
      " [0.32491666]\n",
      " [0.3247273 ]\n",
      " [0.32457238]\n",
      " [0.32448173]\n",
      " [0.32426488]\n",
      " [0.32422197]\n",
      " [0.32412732]\n",
      " [0.3240758 ]\n",
      " [0.32425305]\n",
      " [0.3243193 ]\n",
      " [0.32437444]\n",
      " [0.32442123]\n",
      " [0.32509965]\n",
      " [0.32561183]\n",
      " [0.3253234 ]\n",
      " [0.32534266]\n",
      " [0.32520115]\n",
      " [0.3249593 ]\n",
      " [0.32462168]\n",
      " [0.32443273]\n",
      " [0.32428032]\n",
      " [0.32411647]\n",
      " [0.32397217]\n",
      " [0.32407108]\n",
      " [0.3239757 ]\n",
      " [0.32380944]\n",
      " [0.3236372 ]\n",
      " [0.32357234]\n",
      " [0.32349932]\n",
      " [0.3234697 ]\n",
      " [0.3233344 ]\n",
      " [0.3232883 ]\n",
      " [0.3231866 ]\n",
      " [0.32322043]\n",
      " [0.323052  ]\n",
      " [0.32312062]\n",
      " [0.32312024]\n",
      " [0.3231105 ]\n",
      " [0.32312453]\n",
      " [0.3230412 ]\n",
      " [0.32301456]\n",
      " [0.3230066 ]\n",
      " [0.3229428 ]\n",
      " [0.32289255]\n",
      " [0.32289034]\n",
      " [0.322861  ]\n",
      " [0.32295358]\n",
      " [0.32301718]\n",
      " [0.32297406]\n",
      " [0.32305628]\n",
      " [0.32297388]\n",
      " [0.3231303 ]\n",
      " [0.32320794]\n",
      " [0.32316083]\n",
      " [0.3232327 ]\n",
      " [0.32311928]\n",
      " [0.32311958]\n",
      " [0.32309258]\n",
      " [0.3229993 ]\n",
      " [0.3229718 ]\n",
      " [0.32287306]\n",
      " [0.32280156]\n",
      " [0.3227523 ]\n",
      " [0.32280993]\n",
      " [0.32280558]\n",
      " [0.32279754]\n",
      " [0.32280034]\n",
      " [0.32279548]\n",
      " [0.32288557]\n",
      " [0.3229156 ]\n",
      " [0.32291892]\n",
      " [0.3233736 ]\n",
      " [0.32363108]\n",
      " [0.3239964 ]\n",
      " [0.3239689 ]\n",
      " [0.32393235]\n",
      " [0.32393265]\n",
      " [0.32382596]\n",
      " [0.32398283]\n",
      " [0.32396474]\n",
      " [0.32406348]\n",
      " [0.3241964 ]\n",
      " [0.32408738]\n",
      " [0.3239367 ]\n",
      " [0.32383496]\n",
      " [0.32361805]\n",
      " [0.32357693]\n",
      " [0.32351348]\n",
      " [0.32348228]\n",
      " [0.3235317 ]\n",
      " [0.32347712]\n",
      " [0.323612  ]\n",
      " [0.32362664]\n",
      " [0.32353136]\n",
      " [0.32352018]\n",
      " [0.32367575]\n",
      " [0.32356787]\n",
      " [0.32351357]\n",
      " [0.3233915 ]\n",
      " [0.32345575]\n",
      " [0.32360828]\n",
      " [0.3235941 ]\n",
      " [0.32376325]\n",
      " [0.32387334]\n",
      " [0.3240814 ]\n",
      " [0.3240853 ]\n",
      " [0.32388932]\n",
      " [0.3237529 ]\n",
      " [0.32392675]\n",
      " [0.32390863]\n",
      " [0.32407224]\n",
      " [0.32432687]\n",
      " [0.3243371 ]\n",
      " [0.32431972]\n",
      " [0.32441908]\n",
      " [0.3242629 ]\n",
      " [0.3241967 ]\n",
      " [0.32404643]\n",
      " [0.3239346 ]\n",
      " [0.32372695]\n",
      " [0.32356066]\n",
      " [0.323543  ]]\n",
      "[0.32309645 0.32314658 0.32317612 0.32324636 0.3232903  0.32317978\n",
      " 0.3231491  0.32312888 0.32317212 0.32311362 0.3230429  0.32299006\n",
      " 0.323107   0.32405415 0.324428   0.32451826 0.32464328 0.32472503\n",
      " 0.3247053  0.32472318 0.32470763 0.3245719  0.32431495 0.3239983\n",
      " 0.32395232 0.32384354 0.32391882 0.3238274  0.32378983 0.32384127\n",
      " 0.32390103 0.3236585  0.32353014 0.32347938 0.32335788 0.32320833\n",
      " 0.32312018 0.3229892  0.32296538 0.32289878 0.32298523 0.32296127\n",
      " 0.3230573  0.32323998 0.32319176 0.32310653 0.3230081  0.32292283\n",
      " 0.3232394  0.32365483 0.32363164 0.3237763  0.32371792 0.3238594\n",
      " 0.32382706 0.32381776 0.32363945 0.32346117 0.32336384 0.32329577\n",
      " 0.323232   0.32319778 0.32309955 0.32310823 0.32310796 0.3230585\n",
      " 0.32297665 0.32285985 0.32302365 0.3229921  0.32316145 0.32307553\n",
      " 0.32309216 0.32367563 0.3238753  0.3237291  0.32357627 0.32369512\n",
      " 0.32380942 0.32453722 0.32629466 0.32588893 0.3266625  0.32772845\n",
      " 0.3279515  0.32812712 0.3283881  0.3284199  0.3285695  0.3282696\n",
      " 0.32779908 0.3273723  0.32724905 0.32761824 0.3271638  0.3268602\n",
      " 0.32629555 0.3261714  0.32680818 0.32709247 0.32688072 0.32647914\n",
      " 0.32629988 0.3262514  0.32580042 0.32537177 0.3251781  0.32521373\n",
      " 0.3250522  0.3252165  0.3249275  0.32473123 0.32544217 0.32529932\n",
      " 0.3252312  0.3257113  0.3259958  0.32556176 0.32558972 0.32535845\n",
      " 0.32541063 0.32562923 0.32524848 0.32530886 0.32528943 0.325712\n",
      " 0.32536274 0.3253342  0.32526863 0.32491666 0.3247273  0.32457238\n",
      " 0.32448173 0.32426488 0.32422197 0.32412732 0.3240758  0.32425305\n",
      " 0.3243193  0.32437444 0.32442123 0.32509965 0.32561183 0.3253234\n",
      " 0.32534266 0.32520115 0.3249593  0.32462168 0.32443273 0.32428032\n",
      " 0.32411647 0.32397217 0.32407108 0.3239757  0.32380944 0.3236372\n",
      " 0.32357234 0.32349932 0.3234697  0.3233344  0.3232883  0.3231866\n",
      " 0.32322043 0.323052   0.32312062 0.32312024 0.3231105  0.32312453\n",
      " 0.3230412  0.32301456 0.3230066  0.3229428  0.32289255 0.32289034\n",
      " 0.322861   0.32295358 0.32301718 0.32297406 0.32305628 0.32297388\n",
      " 0.3231303  0.32320794 0.32316083 0.3232327  0.32311928 0.32311958\n",
      " 0.32309258 0.3229993  0.3229718  0.32287306 0.32280156 0.3227523\n",
      " 0.32280993 0.32280558 0.32279754 0.32280034 0.32279548 0.32288557\n",
      " 0.3229156  0.32291892 0.3233736  0.32363108 0.3239964  0.3239689\n",
      " 0.32393235 0.32393265 0.32382596 0.32398283 0.32396474 0.32406348\n",
      " 0.3241964  0.32408738 0.3239367  0.32383496 0.32361805 0.32357693\n",
      " 0.32351348 0.32348228 0.3235317  0.32347712 0.323612   0.32362664\n",
      " 0.32353136 0.32352018 0.32367575 0.32356787 0.32351357 0.3233915\n",
      " 0.32345575 0.32360828 0.3235941  0.32376325 0.32387334 0.3240814\n",
      " 0.3240853  0.32388932 0.3237529  0.32392675 0.32390863 0.32407224\n",
      " 0.32432687 0.3243371  0.32431972 0.32441908 0.3242629  0.3241967\n",
      " 0.32404643 0.3239346  0.32372695 0.32356066 0.323543  ]\n",
      "LLY 0.07244299362884987\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed\n",
      "            RSI       ATR       Date    Volume   Returns\n",
      "1     75.597387  0.315118 2013-01-03  13148600 -0.000552\n",
      "2     71.156360  0.306895 2013-01-04   7464200 -0.007212\n",
      "3     63.164586  0.298545 2013-01-07   9429900 -0.014583\n",
      "4     58.788602  0.292935 2013-01-08   8112900 -0.009080\n",
      "5     58.788602  0.309868 2013-01-09  12977400  0.000000\n",
      "...         ...       ...        ...       ...       ...\n",
      "1505  37.537035  0.908531 2018-12-24   6288500 -0.011831\n",
      "1506  49.031579  0.949350 2018-12-26  10063400  0.040810\n",
      "1507  49.168606  0.941540 2018-12-27   8688600  0.000544\n",
      "1508  51.687011  0.938573 2018-12-28   7369000  0.009740\n",
      "1509  49.983181  0.930103 2018-12-31   4600600 -0.006212\n",
      "\n",
      "[1509 rows x 5 columns]\n",
      "1198\n",
      "[[ 7.55973867e+01  3.15117616e-01  1.31486000e+07 -5.52437490e-04]\n",
      " [ 7.11563600e+01  3.06894984e-01  7.46420000e+06 -7.21227695e-03]\n",
      " [ 6.31645856e+01  2.98545380e-01  9.42990000e+06 -1.45825059e-02]\n",
      " ...\n",
      " [ 3.99762365e+01  4.79369105e-01  3.09290000e+06  3.33203236e-03]\n",
      " [ 4.92678009e+01  5.02985423e-01  4.27950000e+06  1.67470987e-02]\n",
      " [ 4.81727686e+01  5.07772157e-01  3.87090000e+06 -2.26694442e-03]]\n",
      "Length of inputs 1198\n",
      "length of time-series - inputs (1198, 60, 4)\n",
      "length of time-series - outputs (1198,)\n",
      "Training Size (1198, 60, 4) (1198,)\n",
      "Length of inputs 251\n",
      "length of time-series - inputs (251, 60, 4)\n",
      "length of time-series - outputs (251,)\n",
      "Test Size (251, 60, 4) (251,)\n",
      "Reloading Tuner from .\\untitled_project\\tuner0.json\n",
      "WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_23460\\465918227.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_stock.Date = pd.to_datetime(df_stock.Date)\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\base.py:443: UserWarning: X has feature names, but MinMaxScaler was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).layer_with_weights-1.cell.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).layer_with_weights-1.cell.recurrent_kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).layer_with_weights-1.cell.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.19\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.20\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 60, 452)           826256    \n",
      "                                                                 \n",
      " global_average_pooling1d (  (None, 452)               0         \n",
      " GlobalAveragePooling1D)                                         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 60)                27180     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 20)                1220      \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 20)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 21        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 854677 (3.26 MB)\n",
      "Trainable params: 854677 (3.26 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "TSM None\n",
      "Epoch 1/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 1.9192WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 5s 149ms/step - loss: 1.9192\n",
      "Epoch 2/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0652WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 4s 150ms/step - loss: 0.0652\n",
      "Epoch 3/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0806WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 4s 148ms/step - loss: 0.0806\n",
      "Epoch 4/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0800WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 4s 151ms/step - loss: 0.0800\n",
      "Epoch 5/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0633WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 4s 153ms/step - loss: 0.0633\n",
      "Epoch 6/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.1529WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 4s 153ms/step - loss: 0.1529\n",
      "Epoch 7/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0532WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 4s 149ms/step - loss: 0.0532\n",
      "Epoch 8/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0913WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 4s 153ms/step - loss: 0.0913\n",
      "Epoch 9/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0632WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 4s 151ms/step - loss: 0.0632\n",
      "Epoch 10/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0690WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 4s 151ms/step - loss: 0.0690\n",
      "6/6 [==============================] - 1s 51ms/step\n",
      "[[0.6353348 ]\n",
      " [0.6354754 ]\n",
      " [0.63635904]\n",
      " [0.63620573]\n",
      " [0.63641316]\n",
      " [0.6357279 ]\n",
      " [0.63635457]\n",
      " [0.63642395]\n",
      " [0.6361348 ]\n",
      " [0.6357883 ]\n",
      " [0.6343884 ]\n",
      " [0.6350899 ]\n",
      " [0.6353596 ]\n",
      " [0.6355915 ]\n",
      " [0.63488936]\n",
      " [0.635294  ]\n",
      " [0.63586086]\n",
      " [0.6350152 ]\n",
      " [0.63547975]\n",
      " [0.6346203 ]\n",
      " [0.63516885]\n",
      " [0.63485193]\n",
      " [0.6353414 ]\n",
      " [0.6355597 ]\n",
      " [0.6354693 ]\n",
      " [0.6325131 ]\n",
      " [0.6336982 ]\n",
      " [0.63449657]\n",
      " [0.63389105]\n",
      " [0.63311243]\n",
      " [0.634515  ]\n",
      " [0.6339477 ]\n",
      " [0.6344392 ]\n",
      " [0.63488644]\n",
      " [0.6341768 ]\n",
      " [0.6352597 ]\n",
      " [0.62806034]\n",
      " [0.62989837]\n",
      " [0.6242145 ]\n",
      " [0.6273678 ]\n",
      " [0.6272422 ]\n",
      " [0.62608254]\n",
      " [0.62615776]\n",
      " [0.6249566 ]\n",
      " [0.6270899 ]\n",
      " [0.6272497 ]\n",
      " [0.62875247]\n",
      " [0.628462  ]\n",
      " [0.62898505]\n",
      " [0.6296611 ]\n",
      " [0.62996644]\n",
      " [0.6299739 ]\n",
      " [0.6303025 ]\n",
      " [0.6307574 ]\n",
      " [0.6302337 ]\n",
      " [0.6323262 ]\n",
      " [0.6295481 ]\n",
      " [0.6314243 ]\n",
      " [0.63296   ]\n",
      " [0.63179004]\n",
      " [0.633718  ]\n",
      " [0.6332613 ]\n",
      " [0.632446  ]\n",
      " [0.6333892 ]\n",
      " [0.6331696 ]\n",
      " [0.63267523]\n",
      " [0.6307474 ]\n",
      " [0.63112587]\n",
      " [0.63170224]\n",
      " [0.6323257 ]\n",
      " [0.6323811 ]\n",
      " [0.63195974]\n",
      " [0.631482  ]\n",
      " [0.6318623 ]\n",
      " [0.631361  ]\n",
      " [0.63005054]\n",
      " [0.6303016 ]\n",
      " [0.6310914 ]\n",
      " [0.6301715 ]\n",
      " [0.6297077 ]\n",
      " [0.6306441 ]\n",
      " [0.63048226]\n",
      " [0.6277037 ]\n",
      " [0.6244099 ]\n",
      " [0.62603515]\n",
      " [0.6201788 ]\n",
      " [0.6190207 ]\n",
      " [0.6234589 ]\n",
      " [0.62425107]\n",
      " [0.62327766]\n",
      " [0.62465954]\n",
      " [0.6249853 ]\n",
      " [0.6248703 ]\n",
      " [0.62485003]\n",
      " [0.623149  ]\n",
      " [0.623618  ]\n",
      " [0.6265306 ]\n",
      " [0.6264205 ]\n",
      " [0.6248394 ]\n",
      " [0.6250441 ]\n",
      " [0.62394446]\n",
      " [0.62548167]\n",
      " [0.62608504]\n",
      " [0.62570643]\n",
      " [0.62676775]\n",
      " [0.6256793 ]\n",
      " [0.62725127]\n",
      " [0.6277862 ]\n",
      " [0.62713885]\n",
      " [0.6279043 ]\n",
      " [0.6282267 ]\n",
      " [0.627502  ]\n",
      " [0.6275456 ]\n",
      " [0.62853056]\n",
      " [0.6280702 ]\n",
      " [0.6256007 ]\n",
      " [0.6240161 ]\n",
      " [0.6276116 ]\n",
      " [0.623425  ]\n",
      " [0.62359804]\n",
      " [0.62566334]\n",
      " [0.6217428 ]\n",
      " [0.6244779 ]\n",
      " [0.6236667 ]\n",
      " [0.6215469 ]\n",
      " [0.62058794]\n",
      " [0.6228402 ]\n",
      " [0.62336576]\n",
      " [0.6232588 ]\n",
      " [0.62402576]\n",
      " [0.62156737]\n",
      " [0.6240515 ]\n",
      " [0.6223846 ]\n",
      " [0.62082005]\n",
      " [0.6134736 ]\n",
      " [0.61773604]\n",
      " [0.6190055 ]\n",
      " [0.61942285]\n",
      " [0.61869615]\n",
      " [0.62052387]\n",
      " [0.6209703 ]\n",
      " [0.62257886]\n",
      " [0.6237716 ]\n",
      " [0.61975527]\n",
      " [0.6236306 ]\n",
      " [0.6247196 ]\n",
      " [0.62467396]\n",
      " [0.6267881 ]\n",
      " [0.6271659 ]\n",
      " [0.62807876]\n",
      " [0.62842286]\n",
      " [0.62835085]\n",
      " [0.62710804]\n",
      " [0.6288968 ]\n",
      " [0.6265276 ]\n",
      " [0.6265759 ]\n",
      " [0.629342  ]\n",
      " [0.62919647]\n",
      " [0.6292149 ]\n",
      " [0.62821025]\n",
      " [0.62878513]\n",
      " [0.62588066]\n",
      " [0.62816477]\n",
      " [0.6290988 ]\n",
      " [0.6302282 ]\n",
      " [0.63092494]\n",
      " [0.6297971 ]\n",
      " [0.63019574]\n",
      " [0.6302883 ]\n",
      " [0.6289247 ]\n",
      " [0.62964517]\n",
      " [0.6307664 ]\n",
      " [0.6295218 ]\n",
      " [0.62887573]\n",
      " [0.6295224 ]\n",
      " [0.62837094]\n",
      " [0.62697214]\n",
      " [0.63002616]\n",
      " [0.62743646]\n",
      " [0.62945384]\n",
      " [0.6274537 ]\n",
      " [0.62763655]\n",
      " [0.62505233]\n",
      " [0.62819964]\n",
      " [0.6292408 ]\n",
      " [0.62877506]\n",
      " [0.6288963 ]\n",
      " [0.6296819 ]\n",
      " [0.63108736]\n",
      " [0.6314738 ]\n",
      " [0.63128024]\n",
      " [0.6290006 ]\n",
      " [0.6309252 ]\n",
      " [0.63169533]\n",
      " [0.63161314]\n",
      " [0.63183427]\n",
      " [0.6318075 ]\n",
      " [0.631686  ]\n",
      " [0.63182956]\n",
      " [0.6321152 ]\n",
      " [0.63144225]\n",
      " [0.6321972 ]\n",
      " [0.6318505 ]\n",
      " [0.63214034]\n",
      " [0.63154835]\n",
      " [0.63208985]\n",
      " [0.63257784]\n",
      " [0.63216674]\n",
      " [0.63263273]\n",
      " [0.6313302 ]\n",
      " [0.6321362 ]\n",
      " [0.6329548 ]\n",
      " [0.6325814 ]\n",
      " [0.6308542 ]\n",
      " [0.6309426 ]\n",
      " [0.63179535]\n",
      " [0.6307137 ]\n",
      " [0.63129646]\n",
      " [0.63063   ]\n",
      " [0.63132274]\n",
      " [0.63257194]\n",
      " [0.6319768 ]\n",
      " [0.6314203 ]\n",
      " [0.63258207]\n",
      " [0.63255   ]\n",
      " [0.63240165]\n",
      " [0.6323689 ]\n",
      " [0.6305363 ]\n",
      " [0.6310543 ]\n",
      " [0.63140833]\n",
      " [0.631448  ]\n",
      " [0.63103074]\n",
      " [0.6312576 ]\n",
      " [0.63105804]\n",
      " [0.6305476 ]\n",
      " [0.62971675]\n",
      " [0.62918144]\n",
      " [0.63081425]\n",
      " [0.62902015]\n",
      " [0.6290138 ]\n",
      " [0.62999827]\n",
      " [0.6302141 ]\n",
      " [0.6300309 ]\n",
      " [0.6307562 ]\n",
      " [0.62901175]\n",
      " [0.62995034]\n",
      " [0.6302726 ]\n",
      " [0.62884897]\n",
      " [0.63008785]\n",
      " [0.6277368 ]\n",
      " [0.6290156 ]]\n",
      "[0.6353348  0.6354754  0.63635904 0.63620573 0.63641316 0.6357279\n",
      " 0.63635457 0.63642395 0.6361348  0.6357883  0.6343884  0.6350899\n",
      " 0.6353596  0.6355915  0.63488936 0.635294   0.63586086 0.6350152\n",
      " 0.63547975 0.6346203  0.63516885 0.63485193 0.6353414  0.6355597\n",
      " 0.6354693  0.6325131  0.6336982  0.63449657 0.63389105 0.63311243\n",
      " 0.634515   0.6339477  0.6344392  0.63488644 0.6341768  0.6352597\n",
      " 0.62806034 0.62989837 0.6242145  0.6273678  0.6272422  0.62608254\n",
      " 0.62615776 0.6249566  0.6270899  0.6272497  0.62875247 0.628462\n",
      " 0.62898505 0.6296611  0.62996644 0.6299739  0.6303025  0.6307574\n",
      " 0.6302337  0.6323262  0.6295481  0.6314243  0.63296    0.63179004\n",
      " 0.633718   0.6332613  0.632446   0.6333892  0.6331696  0.63267523\n",
      " 0.6307474  0.63112587 0.63170224 0.6323257  0.6323811  0.63195974\n",
      " 0.631482   0.6318623  0.631361   0.63005054 0.6303016  0.6310914\n",
      " 0.6301715  0.6297077  0.6306441  0.63048226 0.6277037  0.6244099\n",
      " 0.62603515 0.6201788  0.6190207  0.6234589  0.62425107 0.62327766\n",
      " 0.62465954 0.6249853  0.6248703  0.62485003 0.623149   0.623618\n",
      " 0.6265306  0.6264205  0.6248394  0.6250441  0.62394446 0.62548167\n",
      " 0.62608504 0.62570643 0.62676775 0.6256793  0.62725127 0.6277862\n",
      " 0.62713885 0.6279043  0.6282267  0.627502   0.6275456  0.62853056\n",
      " 0.6280702  0.6256007  0.6240161  0.6276116  0.623425   0.62359804\n",
      " 0.62566334 0.6217428  0.6244779  0.6236667  0.6215469  0.62058794\n",
      " 0.6228402  0.62336576 0.6232588  0.62402576 0.62156737 0.6240515\n",
      " 0.6223846  0.62082005 0.6134736  0.61773604 0.6190055  0.61942285\n",
      " 0.61869615 0.62052387 0.6209703  0.62257886 0.6237716  0.61975527\n",
      " 0.6236306  0.6247196  0.62467396 0.6267881  0.6271659  0.62807876\n",
      " 0.62842286 0.62835085 0.62710804 0.6288968  0.6265276  0.6265759\n",
      " 0.629342   0.62919647 0.6292149  0.62821025 0.62878513 0.62588066\n",
      " 0.62816477 0.6290988  0.6302282  0.63092494 0.6297971  0.63019574\n",
      " 0.6302883  0.6289247  0.62964517 0.6307664  0.6295218  0.62887573\n",
      " 0.6295224  0.62837094 0.62697214 0.63002616 0.62743646 0.62945384\n",
      " 0.6274537  0.62763655 0.62505233 0.62819964 0.6292408  0.62877506\n",
      " 0.6288963  0.6296819  0.63108736 0.6314738  0.63128024 0.6290006\n",
      " 0.6309252  0.63169533 0.63161314 0.63183427 0.6318075  0.631686\n",
      " 0.63182956 0.6321152  0.63144225 0.6321972  0.6318505  0.63214034\n",
      " 0.63154835 0.63208985 0.63257784 0.63216674 0.63263273 0.6313302\n",
      " 0.6321362  0.6329548  0.6325814  0.6308542  0.6309426  0.63179535\n",
      " 0.6307137  0.63129646 0.63063    0.63132274 0.63257194 0.6319768\n",
      " 0.6314203  0.63258207 0.63255    0.63240165 0.6323689  0.6305363\n",
      " 0.6310543  0.63140833 0.631448   0.63103074 0.6312576  0.63105804\n",
      " 0.6305476  0.62971675 0.62918144 0.63081425 0.62902015 0.6290138\n",
      " 0.62999827 0.6302141  0.6300309  0.6307562  0.62901175 0.62995034\n",
      " 0.6302726  0.62884897 0.63008785 0.6277368  0.6290156 ]\n",
      "TSM 0.602605138078583\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed\n",
      "            RSI       ATR       Date    Volume   Returns\n",
      "1     68.339799  0.595815 2013-01-03  14937200  0.000772\n",
      "2     70.518633  0.596293 2013-01-04  10376000  0.008134\n",
      "3     72.327521  0.589415 2013-01-07  10242400  0.007119\n",
      "4     74.535760  0.589278 2013-01-08  11800400  0.009267\n",
      "5     77.703212  0.604151 2013-01-09  17552800  0.015134\n",
      "...         ...       ...        ...       ...       ...\n",
      "1505  29.106984  4.241238 2018-12-24   8617700 -0.020571\n",
      "1506  44.326948  4.546150 2018-12-26  13499500  0.067497\n",
      "1507  46.898034  4.613568 2018-12-27  10883000  0.013576\n",
      "1508  45.536634  4.566884 2018-12-28   7381300 -0.008139\n",
      "1509  47.082701  4.398535 2018-12-31   7976000  0.007608\n",
      "\n",
      "[1509 rows x 5 columns]\n",
      "1198\n",
      "[[ 6.83397988e+01  5.95815305e-01  1.49372000e+07  7.71898938e-04]\n",
      " [ 7.05186335e+01  5.96292849e-01  1.03760000e+07  8.13441023e-03]\n",
      " [ 7.23275210e+01  5.89414788e-01  1.02424000e+07  7.11856960e-03]\n",
      " ...\n",
      " [ 5.96391519e+01  1.53336575e+00  7.07310000e+06  9.07461299e-03]\n",
      " [ 6.08386578e+01  1.49026821e+00  6.93710000e+06  2.89023704e-03]\n",
      " [ 5.89518645e+01  1.44596354e+00  5.24900000e+06 -2.89023704e-03]]\n",
      "Length of inputs 1198\n",
      "length of time-series - inputs (1198, 60, 4)\n",
      "length of time-series - outputs (1198,)\n",
      "Training Size (1198, 60, 4) (1198,)\n",
      "Length of inputs 251\n",
      "length of time-series - inputs (251, 60, 4)\n",
      "length of time-series - outputs (251,)\n",
      "Test Size (251, 60, 4) (251,)\n",
      "Reloading Tuner from .\\untitled_project\\tuner0.json\n",
      "WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_23460\\465918227.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_stock.Date = pd.to_datetime(df_stock.Date)\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\base.py:443: UserWarning: X has feature names, but MinMaxScaler was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).layer_with_weights-1.cell.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).layer_with_weights-1.cell.recurrent_kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).layer_with_weights-1.cell.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.19\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.20\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 60, 452)           826256    \n",
      "                                                                 \n",
      " global_average_pooling1d (  (None, 452)               0         \n",
      " GlobalAveragePooling1D)                                         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 60)                27180     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 20)                1220      \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 20)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 21        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 854677 (3.26 MB)\n",
      "Trainable params: 854677 (3.26 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "V None\n",
      "Epoch 1/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 1.2094WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 5s 149ms/step - loss: 1.2094\n",
      "Epoch 2/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0683WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 4s 153ms/step - loss: 0.0683\n",
      "Epoch 3/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0835WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 4s 147ms/step - loss: 0.0835\n",
      "Epoch 4/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0413WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 4s 146ms/step - loss: 0.0413\n",
      "Epoch 5/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0605WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 4s 148ms/step - loss: 0.0605\n",
      "Epoch 6/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0535WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 4s 147ms/step - loss: 0.0535\n",
      "Epoch 7/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0567WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 4s 150ms/step - loss: 0.0567\n",
      "Epoch 8/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0473WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 4s 150ms/step - loss: 0.0473\n",
      "Epoch 9/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0412WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 4s 147ms/step - loss: 0.0412\n",
      "Epoch 10/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0420WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 4s 152ms/step - loss: 0.0420\n",
      "6/6 [==============================] - 1s 50ms/step\n",
      "[[0.35600674]\n",
      " [0.35608864]\n",
      " [0.356126  ]\n",
      " [0.35623574]\n",
      " [0.3561704 ]\n",
      " [0.3561905 ]\n",
      " [0.35628787]\n",
      " [0.35615656]\n",
      " [0.3559536 ]\n",
      " [0.3560214 ]\n",
      " [0.35554558]\n",
      " [0.3557802 ]\n",
      " [0.3558781 ]\n",
      " [0.3557275 ]\n",
      " [0.35563844]\n",
      " [0.35602883]\n",
      " [0.35600293]\n",
      " [0.35615754]\n",
      " [0.3559838 ]\n",
      " [0.3561098 ]\n",
      " [0.3562191 ]\n",
      " [0.35633385]\n",
      " [0.35638115]\n",
      " [0.35647577]\n",
      " [0.35647368]\n",
      " [0.35639244]\n",
      " [0.3563141 ]\n",
      " [0.35603216]\n",
      " [0.3561521 ]\n",
      " [0.35569602]\n",
      " [0.35578877]\n",
      " [0.35543886]\n",
      " [0.3555395 ]\n",
      " [0.35573697]\n",
      " [0.35588282]\n",
      " [0.3559656 ]\n",
      " [0.3558929 ]\n",
      " [0.35601544]\n",
      " [0.354855  ]\n",
      " [0.35497516]\n",
      " [0.35504696]\n",
      " [0.3542387 ]\n",
      " [0.3547237 ]\n",
      " [0.35507256]\n",
      " [0.3552786 ]\n",
      " [0.35543528]\n",
      " [0.3554637 ]\n",
      " [0.35547915]\n",
      " [0.35554123]\n",
      " [0.35538766]\n",
      " [0.35521263]\n",
      " [0.35553157]\n",
      " [0.3553935 ]\n",
      " [0.35539478]\n",
      " [0.35533643]\n",
      " [0.3555311 ]\n",
      " [0.3556682 ]\n",
      " [0.35562167]\n",
      " [0.35572597]\n",
      " [0.35579586]\n",
      " [0.35576367]\n",
      " [0.3557987 ]\n",
      " [0.3558542 ]\n",
      " [0.35583425]\n",
      " [0.35609597]\n",
      " [0.35611802]\n",
      " [0.35604823]\n",
      " [0.35608494]\n",
      " [0.3561776 ]\n",
      " [0.35603887]\n",
      " [0.35604548]\n",
      " [0.35611406]\n",
      " [0.3561712 ]\n",
      " [0.35604486]\n",
      " [0.35632452]\n",
      " [0.35628638]\n",
      " [0.3562467 ]\n",
      " [0.35633618]\n",
      " [0.3562155 ]\n",
      " [0.35590127]\n",
      " [0.35586056]\n",
      " [0.355761  ]\n",
      " [0.3550909 ]\n",
      " [0.35407948]\n",
      " [0.35464495]\n",
      " [0.35502976]\n",
      " [0.35462067]\n",
      " [0.35436338]\n",
      " [0.35485858]\n",
      " [0.35492706]\n",
      " [0.35510272]\n",
      " [0.3553879 ]\n",
      " [0.35539418]\n",
      " [0.3554226 ]\n",
      " [0.35524333]\n",
      " [0.3552356 ]\n",
      " [0.35545802]\n",
      " [0.3556228 ]\n",
      " [0.35551596]\n",
      " [0.35545254]\n",
      " [0.35508698]\n",
      " [0.35509863]\n",
      " [0.35529464]\n",
      " [0.35514805]\n",
      " [0.35524935]\n",
      " [0.35528356]\n",
      " [0.35553813]\n",
      " [0.35550216]\n",
      " [0.35534328]\n",
      " [0.3552934 ]\n",
      " [0.35541454]\n",
      " [0.3553978 ]\n",
      " [0.3550091 ]\n",
      " [0.35550493]\n",
      " [0.35535517]\n",
      " [0.35486698]\n",
      " [0.3544355 ]\n",
      " [0.35491672]\n",
      " [0.35482934]\n",
      " [0.354507  ]\n",
      " [0.3547552 ]\n",
      " [0.35482916]\n",
      " [0.35492855]\n",
      " [0.35496008]\n",
      " [0.35521802]\n",
      " [0.35493818]\n",
      " [0.35488448]\n",
      " [0.3551066 ]\n",
      " [0.35511655]\n",
      " [0.3552325 ]\n",
      " [0.35511452]\n",
      " [0.35527965]\n",
      " [0.35551235]\n",
      " [0.35564125]\n",
      " [0.35553664]\n",
      " [0.35554418]\n",
      " [0.35555544]\n",
      " [0.35511732]\n",
      " [0.3548785 ]\n",
      " [0.35533297]\n",
      " [0.3555835 ]\n",
      " [0.3555848 ]\n",
      " [0.35571158]\n",
      " [0.35553318]\n",
      " [0.3556184 ]\n",
      " [0.35573056]\n",
      " [0.35581958]\n",
      " [0.35589516]\n",
      " [0.355914  ]\n",
      " [0.35594374]\n",
      " [0.3560092 ]\n",
      " [0.3560015 ]\n",
      " [0.3559444 ]\n",
      " [0.35593218]\n",
      " [0.35583845]\n",
      " [0.35575873]\n",
      " [0.35581827]\n",
      " [0.35582185]\n",
      " [0.3558804 ]\n",
      " [0.35595435]\n",
      " [0.35579395]\n",
      " [0.3553729 ]\n",
      " [0.35541147]\n",
      " [0.3554598 ]\n",
      " [0.35558462]\n",
      " [0.3557791 ]\n",
      " [0.3559518 ]\n",
      " [0.35594305]\n",
      " [0.35581183]\n",
      " [0.3558442 ]\n",
      " [0.35570225]\n",
      " [0.35580295]\n",
      " [0.3558244 ]\n",
      " [0.35576034]\n",
      " [0.3556599 ]\n",
      " [0.35577536]\n",
      " [0.3557722 ]\n",
      " [0.35571605]\n",
      " [0.355555  ]\n",
      " [0.35568324]\n",
      " [0.3547877 ]\n",
      " [0.35511184]\n",
      " [0.35492015]\n",
      " [0.35518712]\n",
      " [0.35516912]\n",
      " [0.35519087]\n",
      " [0.35515052]\n",
      " [0.35534662]\n",
      " [0.35545987]\n",
      " [0.3554684 ]\n",
      " [0.3556529 ]\n",
      " [0.35561678]\n",
      " [0.3557123 ]\n",
      " [0.3558973 ]\n",
      " [0.35587174]\n",
      " [0.3558429 ]\n",
      " [0.3558715 ]\n",
      " [0.35582215]\n",
      " [0.35584226]\n",
      " [0.3557634 ]\n",
      " [0.35568875]\n",
      " [0.35578287]\n",
      " [0.35583964]\n",
      " [0.35569176]\n",
      " [0.35487437]\n",
      " [0.35500002]\n",
      " [0.35519606]\n",
      " [0.35517818]\n",
      " [0.35555154]\n",
      " [0.3555196 ]\n",
      " [0.35543013]\n",
      " [0.3556495 ]\n",
      " [0.35556334]\n",
      " [0.35547513]\n",
      " [0.35557848]\n",
      " [0.35561833]\n",
      " [0.3555302 ]\n",
      " [0.3555671 ]\n",
      " [0.35561797]\n",
      " [0.3557138 ]\n",
      " [0.35547322]\n",
      " [0.3556724 ]\n",
      " [0.35578117]\n",
      " [0.35585135]\n",
      " [0.3559349 ]\n",
      " [0.35607404]\n",
      " [0.3560406 ]\n",
      " [0.35602108]\n",
      " [0.35608482]\n",
      " [0.35604715]\n",
      " [0.35520515]\n",
      " [0.35531402]\n",
      " [0.35510898]\n",
      " [0.35545224]\n",
      " [0.35542047]\n",
      " [0.3556496 ]\n",
      " [0.3557914 ]\n",
      " [0.35588   ]\n",
      " [0.3556028 ]\n",
      " [0.35569054]\n",
      " [0.35565752]\n",
      " [0.35569364]\n",
      " [0.35552585]\n",
      " [0.35582042]\n",
      " [0.3558991 ]\n",
      " [0.35582393]\n",
      " [0.3558404 ]\n",
      " [0.35586584]\n",
      " [0.35574436]\n",
      " [0.35572493]\n",
      " [0.35565934]]\n",
      "[0.35600674 0.35608864 0.356126   0.35623574 0.3561704  0.3561905\n",
      " 0.35628787 0.35615656 0.3559536  0.3560214  0.35554558 0.3557802\n",
      " 0.3558781  0.3557275  0.35563844 0.35602883 0.35600293 0.35615754\n",
      " 0.3559838  0.3561098  0.3562191  0.35633385 0.35638115 0.35647577\n",
      " 0.35647368 0.35639244 0.3563141  0.35603216 0.3561521  0.35569602\n",
      " 0.35578877 0.35543886 0.3555395  0.35573697 0.35588282 0.3559656\n",
      " 0.3558929  0.35601544 0.354855   0.35497516 0.35504696 0.3542387\n",
      " 0.3547237  0.35507256 0.3552786  0.35543528 0.3554637  0.35547915\n",
      " 0.35554123 0.35538766 0.35521263 0.35553157 0.3553935  0.35539478\n",
      " 0.35533643 0.3555311  0.3556682  0.35562167 0.35572597 0.35579586\n",
      " 0.35576367 0.3557987  0.3558542  0.35583425 0.35609597 0.35611802\n",
      " 0.35604823 0.35608494 0.3561776  0.35603887 0.35604548 0.35611406\n",
      " 0.3561712  0.35604486 0.35632452 0.35628638 0.3562467  0.35633618\n",
      " 0.3562155  0.35590127 0.35586056 0.355761   0.3550909  0.35407948\n",
      " 0.35464495 0.35502976 0.35462067 0.35436338 0.35485858 0.35492706\n",
      " 0.35510272 0.3553879  0.35539418 0.3554226  0.35524333 0.3552356\n",
      " 0.35545802 0.3556228  0.35551596 0.35545254 0.35508698 0.35509863\n",
      " 0.35529464 0.35514805 0.35524935 0.35528356 0.35553813 0.35550216\n",
      " 0.35534328 0.3552934  0.35541454 0.3553978  0.3550091  0.35550493\n",
      " 0.35535517 0.35486698 0.3544355  0.35491672 0.35482934 0.354507\n",
      " 0.3547552  0.35482916 0.35492855 0.35496008 0.35521802 0.35493818\n",
      " 0.35488448 0.3551066  0.35511655 0.3552325  0.35511452 0.35527965\n",
      " 0.35551235 0.35564125 0.35553664 0.35554418 0.35555544 0.35511732\n",
      " 0.3548785  0.35533297 0.3555835  0.3555848  0.35571158 0.35553318\n",
      " 0.3556184  0.35573056 0.35581958 0.35589516 0.355914   0.35594374\n",
      " 0.3560092  0.3560015  0.3559444  0.35593218 0.35583845 0.35575873\n",
      " 0.35581827 0.35582185 0.3558804  0.35595435 0.35579395 0.3553729\n",
      " 0.35541147 0.3554598  0.35558462 0.3557791  0.3559518  0.35594305\n",
      " 0.35581183 0.3558442  0.35570225 0.35580295 0.3558244  0.35576034\n",
      " 0.3556599  0.35577536 0.3557722  0.35571605 0.355555   0.35568324\n",
      " 0.3547877  0.35511184 0.35492015 0.35518712 0.35516912 0.35519087\n",
      " 0.35515052 0.35534662 0.35545987 0.3554684  0.3556529  0.35561678\n",
      " 0.3557123  0.3558973  0.35587174 0.3558429  0.3558715  0.35582215\n",
      " 0.35584226 0.3557634  0.35568875 0.35578287 0.35583964 0.35569176\n",
      " 0.35487437 0.35500002 0.35519606 0.35517818 0.35555154 0.3555196\n",
      " 0.35543013 0.3556495  0.35556334 0.35547513 0.35557848 0.35561833\n",
      " 0.3555302  0.3555671  0.35561797 0.3557138  0.35547322 0.3556724\n",
      " 0.35578117 0.35585135 0.3559349  0.35607404 0.3560406  0.35602108\n",
      " 0.35608482 0.35604715 0.35520515 0.35531402 0.35510898 0.35545224\n",
      " 0.35542047 0.3556496  0.3557914  0.35588    0.3556028  0.35569054\n",
      " 0.35565752 0.35569364 0.35552585 0.35582042 0.3558991  0.35582393\n",
      " 0.3558404  0.35586584 0.35574436 0.35572493 0.35565934]\n",
      "V 0.7634085516542509\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed\n",
      "            RSI       ATR       Date    Volume   Returns\n",
      "1     36.930554  0.922022 2013-01-03  14413200 -0.047883\n",
      "2     37.696086  0.909020 2013-01-04  10458000  0.001922\n",
      "3     37.696086  0.899805 2013-01-07   7804500  0.000000\n",
      "4     34.358943  0.898390 2013-01-08   8196200 -0.013335\n",
      "5     42.116727  0.912791 2013-01-09   5154300  0.018696\n",
      "...         ...       ...        ...       ...       ...\n",
      "1505  24.997998  7.370625 2018-12-24   3091000 -0.022917\n",
      "1506  36.801839  7.669866 2018-12-26   4159300  0.043720\n",
      "1507  39.387366  7.888447 2018-12-27   4295800  0.010709\n",
      "1508  39.661331  7.680700 2018-12-28   3312700  0.001097\n",
      "1509  42.639502  7.420650 2018-12-31   3123200  0.011628\n",
      "\n",
      "[1509 rows x 5 columns]\n",
      "1198\n",
      "[[ 3.69305541e+01  9.22022155e-01  1.44132000e+07 -4.78830457e-02]\n",
      " [ 3.76960860e+01  9.09020420e-01  1.04580000e+07  1.92162726e-03]\n",
      " [ 3.76960860e+01  8.99804861e-01  7.80450000e+06  0.00000000e+00]\n",
      " ...\n",
      " [ 5.41168650e+01  3.38720948e+00  1.86190000e+06  3.72736936e-03]\n",
      " [ 5.92085279e+01  3.33383733e+00  1.45260000e+06  1.06048675e-02]\n",
      " [ 5.29844759e+01  3.34499111e+00  2.35020000e+06 -1.04235263e-02]]\n",
      "Length of inputs 1198\n",
      "length of time-series - inputs (1198, 60, 4)\n",
      "length of time-series - outputs (1198,)\n",
      "Training Size (1198, 60, 4) (1198,)\n",
      "Length of inputs 251\n",
      "length of time-series - inputs (251, 60, 4)\n",
      "length of time-series - outputs (251,)\n",
      "Test Size (251, 60, 4) (251,)\n",
      "Reloading Tuner from .\\untitled_project\\tuner0.json\n",
      "WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_23460\\465918227.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_stock.Date = pd.to_datetime(df_stock.Date)\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\base.py:443: UserWarning: X has feature names, but MinMaxScaler was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).layer_with_weights-1.cell.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).layer_with_weights-1.cell.recurrent_kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).layer_with_weights-1.cell.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.19\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.20\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 60, 512)           1058816   \n",
      "                                                                 \n",
      " global_average_pooling1d (  (None, 512)               0         \n",
      " GlobalAveragePooling1D)                                         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 60)                30780     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 20)                1220      \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 20)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 21        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1090837 (4.16 MB)\n",
      "Trainable params: 1090837 (4.16 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "UNH None\n",
      "Epoch 1/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.2131WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 6s 202ms/step - loss: 0.2131\n",
      "Epoch 2/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.1749WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 5s 206ms/step - loss: 0.1749\n",
      "Epoch 3/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0937WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 5s 205ms/step - loss: 0.0937\n",
      "Epoch 4/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0447WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 5s 206ms/step - loss: 0.0447\n",
      "Epoch 5/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0416WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 5s 203ms/step - loss: 0.0416\n",
      "Epoch 6/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0436WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 5s 204ms/step - loss: 0.0436\n",
      "Epoch 7/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0444WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 5s 206ms/step - loss: 0.0444\n",
      "Epoch 8/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0446WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 5s 210ms/step - loss: 0.0446\n",
      "Epoch 9/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0447WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 5s 204ms/step - loss: 0.0447\n",
      "Epoch 10/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0447WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 5s 205ms/step - loss: 0.0447\n",
      "6/6 [==============================] - 1s 61ms/step\n",
      "[[0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]\n",
      " [0.36276022]]\n",
      "[0.36276022 0.36276022 0.36276022 0.36276022 0.36276022 0.36276022\n",
      " 0.36276022 0.36276022 0.36276022 0.36276022 0.36276022 0.36276022\n",
      " 0.36276022 0.36276022 0.36276022 0.36276022 0.36276022 0.36276022\n",
      " 0.36276022 0.36276022 0.36276022 0.36276022 0.36276022 0.36276022\n",
      " 0.36276022 0.36276022 0.36276022 0.36276022 0.36276022 0.36276022\n",
      " 0.36276022 0.36276022 0.36276022 0.36276022 0.36276022 0.36276022\n",
      " 0.36276022 0.36276022 0.36276022 0.36276022 0.36276022 0.36276022\n",
      " 0.36276022 0.36276022 0.36276022 0.36276022 0.36276022 0.36276022\n",
      " 0.36276022 0.36276022 0.36276022 0.36276022 0.36276022 0.36276022\n",
      " 0.36276022 0.36276022 0.36276022 0.36276022 0.36276022 0.36276022\n",
      " 0.36276022 0.36276022 0.36276022 0.36276022 0.36276022 0.36276022\n",
      " 0.36276022 0.36276022 0.36276022 0.36276022 0.36276022 0.36276022\n",
      " 0.36276022 0.36276022 0.36276022 0.36276022 0.36276022 0.36276022\n",
      " 0.36276022 0.36276022 0.36276022 0.36276022 0.36276022 0.36276022\n",
      " 0.36276022 0.36276022 0.36276022 0.36276022 0.36276022 0.36276022\n",
      " 0.36276022 0.36276022 0.36276022 0.36276022 0.36276022 0.36276022\n",
      " 0.36276022 0.36276022 0.36276022 0.36276022 0.36276022 0.36276022\n",
      " 0.36276022 0.36276022 0.36276022 0.36276022 0.36276022 0.36276022\n",
      " 0.36276022 0.36276022 0.36276022 0.36276022 0.36276022 0.36276022\n",
      " 0.36276022 0.36276022 0.36276022 0.36276022 0.36276022 0.36276022\n",
      " 0.36276022 0.36276022 0.36276022 0.36276022 0.36276022 0.36276022\n",
      " 0.36276022 0.36276022 0.36276022 0.36276022 0.36276022 0.36276022\n",
      " 0.36276022 0.36276022 0.36276022 0.36276022 0.36276022 0.36276022\n",
      " 0.36276022 0.36276022 0.36276022 0.36276022 0.36276022 0.36276022\n",
      " 0.36276022 0.36276022 0.36276022 0.36276022 0.36276022 0.36276022\n",
      " 0.36276022 0.36276022 0.36276022 0.36276022 0.36276022 0.36276022\n",
      " 0.36276022 0.36276022 0.36276022 0.36276022 0.36276022 0.36276022\n",
      " 0.36276022 0.36276022 0.36276022 0.36276022 0.36276022 0.36276022\n",
      " 0.36276022 0.36276022 0.36276022 0.36276022 0.36276022 0.36276022\n",
      " 0.36276022 0.36276022 0.36276022 0.36276022 0.36276022 0.36276022\n",
      " 0.36276022 0.36276022 0.36276022 0.36276022 0.36276022 0.36276022\n",
      " 0.36276022 0.36276022 0.36276022 0.36276022 0.36276022 0.36276022\n",
      " 0.36276022 0.36276022 0.36276022 0.36276022 0.36276022 0.36276022\n",
      " 0.36276022 0.36276022 0.36276022 0.36276022 0.36276022 0.36276022\n",
      " 0.36276022 0.36276022 0.36276022 0.36276022 0.36276022 0.36276022\n",
      " 0.36276022 0.36276022 0.36276022 0.36276022 0.36276022 0.36276022\n",
      " 0.36276022 0.36276022 0.36276022 0.36276022 0.36276022 0.36276022\n",
      " 0.36276022 0.36276022 0.36276022 0.36276022 0.36276022 0.36276022\n",
      " 0.36276022 0.36276022 0.36276022 0.36276022 0.36276022 0.36276022\n",
      " 0.36276022 0.36276022 0.36276022 0.36276022 0.36276022 0.36276022\n",
      " 0.36276022 0.36276022 0.36276022 0.36276022 0.36276022 0.36276022\n",
      " 0.36276022 0.36276022 0.36276022 0.36276022 0.36276022]\n",
      "UNH 0.7591804822866047\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed\n",
      "            RSI       ATR       Date    Volume   Returns\n",
      "1     52.970481  1.204402 2013-01-03  13268200 -0.001805\n",
      "2     54.794415  1.169802 2013-01-04  11427900  0.004619\n",
      "3     49.590835  1.151959 2013-01-07  11799800 -0.011646\n",
      "4     52.201376  1.154676 2013-01-08  14226400  0.006236\n",
      "5     50.461714  1.128628 2013-01-09  10892200 -0.003850\n",
      "...         ...       ...        ...       ...       ...\n",
      "1505  20.097195  2.067170 2018-12-24  14262800 -0.039068\n",
      "1506  33.197904  2.205944 2018-12-26  24887700  0.046673\n",
      "1507  34.309628  2.252662 2018-12-27  22077000  0.004361\n",
      "1508  32.800779  2.217472 2018-12-28  19710600 -0.011232\n",
      "1509  32.883366  2.165510 2018-12-31  15807000  0.000293\n",
      "\n",
      "[1509 rows x 5 columns]\n",
      "1198\n",
      "[[ 5.29704810e+01  1.20440224e+00  1.32682000e+07 -1.80528488e-03]\n",
      " [ 5.47944151e+01  1.16980162e+00  1.14279000e+07  4.61943771e-03]\n",
      " [ 4.95908354e+01  1.15195852e+00  1.17998000e+07 -1.16460577e-02]\n",
      " ...\n",
      " [ 6.21827747e+01  7.14947103e-01  7.00060000e+06 -9.52692033e-04]\n",
      " [ 6.32129661e+01  6.81736596e-01  7.49530000e+06  1.42909123e-03]\n",
      " [ 5.78393045e+01  6.73040950e-01  8.52340000e+06 -4.53282410e-03]]\n",
      "Length of inputs 1198\n",
      "length of time-series - inputs (1198, 60, 4)\n",
      "length of time-series - outputs (1198,)\n",
      "Training Size (1198, 60, 4) (1198,)\n",
      "Length of inputs 251\n",
      "length of time-series - inputs (251, 60, 4)\n",
      "length of time-series - outputs (251,)\n",
      "Test Size (251, 60, 4) (251,)\n",
      "Reloading Tuner from .\\untitled_project\\tuner0.json\n",
      "WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_23460\\465918227.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_stock.Date = pd.to_datetime(df_stock.Date)\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\base.py:443: UserWarning: X has feature names, but MinMaxScaler was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).layer_with_weights-1.cell.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).layer_with_weights-1.cell.recurrent_kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).layer_with_weights-1.cell.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.19\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.20\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 60, 392)           622496    \n",
      "                                                                 \n",
      " global_average_pooling1d (  (None, 392)               0         \n",
      " GlobalAveragePooling1D)                                         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 60)                23580     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 20)                1220      \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 20)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 21        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 647317 (2.47 MB)\n",
      "Trainable params: 647317 (2.47 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "XOM None\n",
      "Epoch 1/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.7319WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 4s 127ms/step - loss: 0.7319\n",
      "Epoch 2/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0750WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 3s 125ms/step - loss: 0.0750\n",
      "Epoch 3/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0656WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 3s 126ms/step - loss: 0.0656\n",
      "Epoch 4/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.1442WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 3s 128ms/step - loss: 0.1442\n",
      "Epoch 5/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0583WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 3s 128ms/step - loss: 0.0583\n",
      "Epoch 6/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0796WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 3s 127ms/step - loss: 0.0796\n",
      "Epoch 7/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0562WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 3s 126ms/step - loss: 0.0562\n",
      "Epoch 8/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0608WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 3s 126ms/step - loss: 0.0608\n",
      "Epoch 9/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0521WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 3s 126ms/step - loss: 0.0521\n",
      "Epoch 10/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0508WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 3s 127ms/step - loss: 0.0508\n",
      "6/6 [==============================] - 0s 40ms/step\n",
      "[[0.35039926]\n",
      " [0.35034102]\n",
      " [0.3504035 ]\n",
      " [0.350399  ]\n",
      " [0.35039306]\n",
      " [0.3503266 ]\n",
      " [0.35036305]\n",
      " [0.35037267]\n",
      " [0.3503374 ]\n",
      " [0.3503245 ]\n",
      " [0.35036793]\n",
      " [0.35040584]\n",
      " [0.35039574]\n",
      " [0.35036337]\n",
      " [0.35035816]\n",
      " [0.35040444]\n",
      " [0.3505376 ]\n",
      " [0.35043284]\n",
      " [0.3504468 ]\n",
      " [0.35047996]\n",
      " [0.35039508]\n",
      " [0.35038546]\n",
      " [0.3504789 ]\n",
      " [0.35041213]\n",
      " [0.350424  ]\n",
      " [0.35049808]\n",
      " [0.35042807]\n",
      " [0.35038388]\n",
      " [0.35030305]\n",
      " [0.35023373]\n",
      " [0.35025147]\n",
      " [0.35027725]\n",
      " [0.3503756 ]\n",
      " [0.35041127]\n",
      " [0.35041124]\n",
      " [0.35038203]\n",
      " [0.35034585]\n",
      " [0.35048226]\n",
      " [0.35051295]\n",
      " [0.35063446]\n",
      " [0.35060978]\n",
      " [0.35060787]\n",
      " [0.35053682]\n",
      " [0.35057554]\n",
      " [0.35061708]\n",
      " [0.3506026 ]\n",
      " [0.3505844 ]\n",
      " [0.35054576]\n",
      " [0.35058308]\n",
      " [0.35050422]\n",
      " [0.3505812 ]\n",
      " [0.3504953 ]\n",
      " [0.35047916]\n",
      " [0.35058445]\n",
      " [0.35068455]\n",
      " [0.35057858]\n",
      " [0.3505011 ]\n",
      " [0.35048664]\n",
      " [0.3504647 ]\n",
      " [0.35045058]\n",
      " [0.35066757]\n",
      " [0.35075897]\n",
      " [0.35063022]\n",
      " [0.35063824]\n",
      " [0.35063732]\n",
      " [0.35054174]\n",
      " [0.35058624]\n",
      " [0.3507431 ]\n",
      " [0.35069475]\n",
      " [0.35065967]\n",
      " [0.35075405]\n",
      " [0.35064632]\n",
      " [0.3507105 ]\n",
      " [0.35077694]\n",
      " [0.35069442]\n",
      " [0.35068846]\n",
      " [0.35065266]\n",
      " [0.35064706]\n",
      " [0.3505531 ]\n",
      " [0.35060066]\n",
      " [0.35073102]\n",
      " [0.35081637]\n",
      " [0.35053936]\n",
      " [0.35098457]\n",
      " [0.3512228 ]\n",
      " [0.35137737]\n",
      " [0.35134417]\n",
      " [0.35134518]\n",
      " [0.35138428]\n",
      " [0.3513982 ]\n",
      " [0.35135648]\n",
      " [0.3513455 ]\n",
      " [0.35132274]\n",
      " [0.35136515]\n",
      " [0.3513683 ]\n",
      " [0.3513455 ]\n",
      " [0.35135925]\n",
      " [0.35133302]\n",
      " [0.35136443]\n",
      " [0.3514201 ]\n",
      " [0.35142183]\n",
      " [0.35139057]\n",
      " [0.3513536 ]\n",
      " [0.35135007]\n",
      " [0.35127848]\n",
      " [0.35130063]\n",
      " [0.35125715]\n",
      " [0.3512411 ]\n",
      " [0.3512219 ]\n",
      " [0.35121006]\n",
      " [0.3512571 ]\n",
      " [0.35112125]\n",
      " [0.35112226]\n",
      " [0.35112697]\n",
      " [0.351157  ]\n",
      " [0.3510723 ]\n",
      " [0.35111272]\n",
      " [0.35117066]\n",
      " [0.35114592]\n",
      " [0.35112444]\n",
      " [0.35122448]\n",
      " [0.35120255]\n",
      " [0.3512818 ]\n",
      " [0.35126504]\n",
      " [0.35127178]\n",
      " [0.35122865]\n",
      " [0.3512709 ]\n",
      " [0.35133138]\n",
      " [0.35128167]\n",
      " [0.3512421 ]\n",
      " [0.3512313 ]\n",
      " [0.3512126 ]\n",
      " [0.35115355]\n",
      " [0.35118085]\n",
      " [0.35112816]\n",
      " [0.35109314]\n",
      " [0.35110173]\n",
      " [0.35108307]\n",
      " [0.35119778]\n",
      " [0.35117376]\n",
      " [0.35105413]\n",
      " [0.3511688 ]\n",
      " [0.35112864]\n",
      " [0.3511252 ]\n",
      " [0.3511101 ]\n",
      " [0.35114205]\n",
      " [0.3511985 ]\n",
      " [0.35120732]\n",
      " [0.35125569]\n",
      " [0.35124964]\n",
      " [0.35117298]\n",
      " [0.35114345]\n",
      " [0.35107687]\n",
      " [0.35105065]\n",
      " [0.35100847]\n",
      " [0.35095844]\n",
      " [0.35100454]\n",
      " [0.35095385]\n",
      " [0.35102096]\n",
      " [0.35090673]\n",
      " [0.35094416]\n",
      " [0.35103464]\n",
      " [0.35119104]\n",
      " [0.35107914]\n",
      " [0.35108   ]\n",
      " [0.35104817]\n",
      " [0.35107845]\n",
      " [0.3511264 ]\n",
      " [0.35110033]\n",
      " [0.35106915]\n",
      " [0.35098764]\n",
      " [0.35099   ]\n",
      " [0.35097677]\n",
      " [0.35102192]\n",
      " [0.3509211 ]\n",
      " [0.35098588]\n",
      " [0.3509673 ]\n",
      " [0.35093573]\n",
      " [0.350933  ]\n",
      " [0.3510596 ]\n",
      " [0.35097402]\n",
      " [0.3510876 ]\n",
      " [0.35110483]\n",
      " [0.35107347]\n",
      " [0.35108525]\n",
      " [0.35098377]\n",
      " [0.35105854]\n",
      " [0.3510093 ]\n",
      " [0.35099036]\n",
      " [0.35098624]\n",
      " [0.35098055]\n",
      " [0.3509211 ]\n",
      " [0.35096523]\n",
      " [0.3509634 ]\n",
      " [0.3508675 ]\n",
      " [0.35087544]\n",
      " [0.3508598 ]\n",
      " [0.35082862]\n",
      " [0.3507767 ]\n",
      " [0.35081795]\n",
      " [0.35092378]\n",
      " [0.35087785]\n",
      " [0.3508716 ]\n",
      " [0.35073712]\n",
      " [0.35088822]\n",
      " [0.35083568]\n",
      " [0.35073844]\n",
      " [0.3507604 ]\n",
      " [0.35078034]\n",
      " [0.35076103]\n",
      " [0.35083804]\n",
      " [0.35072005]\n",
      " [0.3507204 ]\n",
      " [0.35071817]\n",
      " [0.35066897]\n",
      " [0.35069683]\n",
      " [0.35065603]\n",
      " [0.3508403 ]\n",
      " [0.3507911 ]\n",
      " [0.3508003 ]\n",
      " [0.3507825 ]\n",
      " [0.3508484 ]\n",
      " [0.3507007 ]\n",
      " [0.35080606]\n",
      " [0.35080564]\n",
      " [0.35073993]\n",
      " [0.35077143]\n",
      " [0.35068205]\n",
      " [0.35068417]\n",
      " [0.3507254 ]\n",
      " [0.35084653]\n",
      " [0.35075632]\n",
      " [0.35095164]\n",
      " [0.35086858]\n",
      " [0.3509542 ]\n",
      " [0.3508869 ]\n",
      " [0.350842  ]\n",
      " [0.35090268]\n",
      " [0.35086417]\n",
      " [0.35083437]\n",
      " [0.35087416]\n",
      " [0.35082468]\n",
      " [0.350883  ]\n",
      " [0.35092315]\n",
      " [0.35082144]\n",
      " [0.35077566]\n",
      " [0.3507875 ]\n",
      " [0.35076723]\n",
      " [0.35084832]\n",
      " [0.35082644]\n",
      " [0.35076788]]\n",
      "[0.35039926 0.35034102 0.3504035  0.350399   0.35039306 0.3503266\n",
      " 0.35036305 0.35037267 0.3503374  0.3503245  0.35036793 0.35040584\n",
      " 0.35039574 0.35036337 0.35035816 0.35040444 0.3505376  0.35043284\n",
      " 0.3504468  0.35047996 0.35039508 0.35038546 0.3504789  0.35041213\n",
      " 0.350424   0.35049808 0.35042807 0.35038388 0.35030305 0.35023373\n",
      " 0.35025147 0.35027725 0.3503756  0.35041127 0.35041124 0.35038203\n",
      " 0.35034585 0.35048226 0.35051295 0.35063446 0.35060978 0.35060787\n",
      " 0.35053682 0.35057554 0.35061708 0.3506026  0.3505844  0.35054576\n",
      " 0.35058308 0.35050422 0.3505812  0.3504953  0.35047916 0.35058445\n",
      " 0.35068455 0.35057858 0.3505011  0.35048664 0.3504647  0.35045058\n",
      " 0.35066757 0.35075897 0.35063022 0.35063824 0.35063732 0.35054174\n",
      " 0.35058624 0.3507431  0.35069475 0.35065967 0.35075405 0.35064632\n",
      " 0.3507105  0.35077694 0.35069442 0.35068846 0.35065266 0.35064706\n",
      " 0.3505531  0.35060066 0.35073102 0.35081637 0.35053936 0.35098457\n",
      " 0.3512228  0.35137737 0.35134417 0.35134518 0.35138428 0.3513982\n",
      " 0.35135648 0.3513455  0.35132274 0.35136515 0.3513683  0.3513455\n",
      " 0.35135925 0.35133302 0.35136443 0.3514201  0.35142183 0.35139057\n",
      " 0.3513536  0.35135007 0.35127848 0.35130063 0.35125715 0.3512411\n",
      " 0.3512219  0.35121006 0.3512571  0.35112125 0.35112226 0.35112697\n",
      " 0.351157   0.3510723  0.35111272 0.35117066 0.35114592 0.35112444\n",
      " 0.35122448 0.35120255 0.3512818  0.35126504 0.35127178 0.35122865\n",
      " 0.3512709  0.35133138 0.35128167 0.3512421  0.3512313  0.3512126\n",
      " 0.35115355 0.35118085 0.35112816 0.35109314 0.35110173 0.35108307\n",
      " 0.35119778 0.35117376 0.35105413 0.3511688  0.35112864 0.3511252\n",
      " 0.3511101  0.35114205 0.3511985  0.35120732 0.35125569 0.35124964\n",
      " 0.35117298 0.35114345 0.35107687 0.35105065 0.35100847 0.35095844\n",
      " 0.35100454 0.35095385 0.35102096 0.35090673 0.35094416 0.35103464\n",
      " 0.35119104 0.35107914 0.35108    0.35104817 0.35107845 0.3511264\n",
      " 0.35110033 0.35106915 0.35098764 0.35099    0.35097677 0.35102192\n",
      " 0.3509211  0.35098588 0.3509673  0.35093573 0.350933   0.3510596\n",
      " 0.35097402 0.3510876  0.35110483 0.35107347 0.35108525 0.35098377\n",
      " 0.35105854 0.3510093  0.35099036 0.35098624 0.35098055 0.3509211\n",
      " 0.35096523 0.3509634  0.3508675  0.35087544 0.3508598  0.35082862\n",
      " 0.3507767  0.35081795 0.35092378 0.35087785 0.3508716  0.35073712\n",
      " 0.35088822 0.35083568 0.35073844 0.3507604  0.35078034 0.35076103\n",
      " 0.35083804 0.35072005 0.3507204  0.35071817 0.35066897 0.35069683\n",
      " 0.35065603 0.3508403  0.3507911  0.3508003  0.3507825  0.3508484\n",
      " 0.3507007  0.35080606 0.35080564 0.35073993 0.35077143 0.35068205\n",
      " 0.35068417 0.3507254  0.35084653 0.35075632 0.35095164 0.35086858\n",
      " 0.3509542  0.3508869  0.350842   0.35090268 0.35086417 0.35083437\n",
      " 0.35087416 0.35082468 0.350883   0.35092315 0.35082144 0.35077566\n",
      " 0.3507875  0.35076723 0.35084832 0.35082644 0.35076788]\n",
      "XOM 0.0378532644826093\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed\n",
      "            RSI       ATR       Date    Volume   Returns\n",
      "1     67.421316  0.756418 2013-01-03  24227700 -0.002018\n",
      "2     71.680925  0.773103 2013-01-04  24487700  0.017570\n",
      "3     71.931053  0.751453 2013-01-07  24456900  0.001101\n",
      "4     72.403548  0.736349 2013-01-08  19624200  0.001980\n",
      "5     71.968487  0.759467 2013-01-09  25920600 -0.000660\n",
      "...         ...       ...        ...       ...       ...\n",
      "1505  20.814913  2.744690 2018-12-24  17009300 -0.021792\n",
      "1506  34.730753  2.895069 2018-12-26  22542900  0.040622\n",
      "1507  38.045717  2.945421 2018-12-27  20304700  0.011192\n",
      "1508  37.645381  2.869320 2018-12-28  17963300 -0.002166\n",
      "1509  40.195052  2.810082 2018-12-31  13237200  0.008126\n",
      "\n",
      "[1509 rows x 5 columns]\n",
      "1198\n",
      "[[ 6.74213162e+01  7.56418307e-01  2.42277000e+07 -2.01794426e-03]\n",
      " [ 7.16809254e+01  7.73102834e-01  2.44877000e+07  1.75700690e-02]\n",
      " [ 7.19310528e+01  7.51452719e-01  2.44569000e+07  1.10149659e-03]\n",
      " ...\n",
      " [ 6.26734406e+01  1.62084199e+00  9.49650000e+06  1.86720247e-03]\n",
      " [ 6.45915635e+01  1.55363846e+00  7.44060000e+06  5.30203979e-03]\n",
      " [ 5.96676240e+01  1.53837831e+00  8.92570000e+06 -7.91693193e-03]]\n",
      "Length of inputs 1198\n",
      "length of time-series - inputs (1198, 60, 4)\n",
      "length of time-series - outputs (1198,)\n",
      "Training Size (1198, 60, 4) (1198,)\n",
      "Length of inputs 251\n",
      "length of time-series - inputs (251, 60, 4)\n",
      "length of time-series - outputs (251,)\n",
      "Test Size (251, 60, 4) (251,)\n",
      "Reloading Tuner from .\\untitled_project\\tuner0.json\n",
      "WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_23460\\465918227.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_stock.Date = pd.to_datetime(df_stock.Date)\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\base.py:443: UserWarning: X has feature names, but MinMaxScaler was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).layer_with_weights-1.cell.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).layer_with_weights-1.cell.recurrent_kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).layer_with_weights-1.cell.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.19\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.20\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 60, 452)           826256    \n",
      "                                                                 \n",
      " global_average_pooling1d (  (None, 452)               0         \n",
      " GlobalAveragePooling1D)                                         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 60)                27180     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 20)                1220      \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 20)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 21        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 854677 (3.26 MB)\n",
      "Trainable params: 854677 (3.26 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "JPM None\n",
      "Epoch 1/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 1.3801WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 5s 159ms/step - loss: 1.3801\n",
      "Epoch 2/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0654WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 4s 157ms/step - loss: 0.0654\n",
      "Epoch 3/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0650WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 4s 156ms/step - loss: 0.0650\n",
      "Epoch 4/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0681WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 4s 157ms/step - loss: 0.0681\n",
      "Epoch 5/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0634WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 4s 156ms/step - loss: 0.0634\n",
      "Epoch 6/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0684WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 4s 157ms/step - loss: 0.0684\n",
      "Epoch 7/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0769WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 4s 156ms/step - loss: 0.0769\n",
      "Epoch 8/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0517WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 4s 155ms/step - loss: 0.0517\n",
      "Epoch 9/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0329WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 4s 154ms/step - loss: 0.0329\n",
      "Epoch 10/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0830WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 4s 157ms/step - loss: 0.0830\n",
      "6/6 [==============================] - 1s 55ms/step\n",
      "[[0.4354979 ]\n",
      " [0.43558815]\n",
      " [0.43561023]\n",
      " [0.435562  ]\n",
      " [0.43469203]\n",
      " [0.43345   ]\n",
      " [0.43456957]\n",
      " [0.43565214]\n",
      " [0.435413  ]\n",
      " [0.4355847 ]\n",
      " [0.43570128]\n",
      " [0.4355491 ]\n",
      " [0.4358361 ]\n",
      " [0.43596888]\n",
      " [0.43561542]\n",
      " [0.4359579 ]\n",
      " [0.43607283]\n",
      " [0.43594855]\n",
      " [0.4355748 ]\n",
      " [0.4358237 ]\n",
      " [0.4359811 ]\n",
      " [0.4358561 ]\n",
      " [0.43513268]\n",
      " [0.43149382]\n",
      " [0.43113872]\n",
      " [0.43265224]\n",
      " [0.4333266 ]\n",
      " [0.43365663]\n",
      " [0.43251127]\n",
      " [0.43313056]\n",
      " [0.43400028]\n",
      " [0.43379197]\n",
      " [0.43448353]\n",
      " [0.43358546]\n",
      " [0.43426952]\n",
      " [0.43441126]\n",
      " [0.4326591 ]\n",
      " [0.43481544]\n",
      " [0.4344091 ]\n",
      " [0.43339348]\n",
      " [0.43372115]\n",
      " [0.43381262]\n",
      " [0.43283835]\n",
      " [0.43302923]\n",
      " [0.43352365]\n",
      " [0.4344103 ]\n",
      " [0.43433917]\n",
      " [0.4342104 ]\n",
      " [0.43275785]\n",
      " [0.43272525]\n",
      " [0.43144655]\n",
      " [0.43419042]\n",
      " [0.4336922 ]\n",
      " [0.43348247]\n",
      " [0.43454826]\n",
      " [0.43343338]\n",
      " [0.43451577]\n",
      " [0.4345897 ]\n",
      " [0.43503574]\n",
      " [0.43414378]\n",
      " [0.43421197]\n",
      " [0.43433982]\n",
      " [0.4347183 ]\n",
      " [0.43363312]\n",
      " [0.43426675]\n",
      " [0.434417  ]\n",
      " [0.43449518]\n",
      " [0.43478882]\n",
      " [0.43445992]\n",
      " [0.43295524]\n",
      " [0.43464014]\n",
      " [0.43453798]\n",
      " [0.4336814 ]\n",
      " [0.4351494 ]\n",
      " [0.4348963 ]\n",
      " [0.4348862 ]\n",
      " [0.43496612]\n",
      " [0.43507147]\n",
      " [0.43500084]\n",
      " [0.43382904]\n",
      " [0.4345926 ]\n",
      " [0.4344613 ]\n",
      " [0.4310461 ]\n",
      " [0.42530167]\n",
      " [0.42865586]\n",
      " [0.42941147]\n",
      " [0.42555457]\n",
      " [0.4274875 ]\n",
      " [0.42922676]\n",
      " [0.42943096]\n",
      " [0.43093297]\n",
      " [0.43090665]\n",
      " [0.43038368]\n",
      " [0.43086946]\n",
      " [0.43109655]\n",
      " [0.4305979 ]\n",
      " [0.43261397]\n",
      " [0.432126  ]\n",
      " [0.4307598 ]\n",
      " [0.42979768]\n",
      " [0.42881638]\n",
      " [0.42938387]\n",
      " [0.4310669 ]\n",
      " [0.43114048]\n",
      " [0.43064985]\n",
      " [0.43116298]\n",
      " [0.43248776]\n",
      " [0.43172187]\n",
      " [0.43079326]\n",
      " [0.43065086]\n",
      " [0.43205523]\n",
      " [0.4307305 ]\n",
      " [0.4311005 ]\n",
      " [0.4320444 ]\n",
      " [0.4315592 ]\n",
      " [0.4262097 ]\n",
      " [0.42573604]\n",
      " [0.42955273]\n",
      " [0.4272474 ]\n",
      " [0.4276554 ]\n",
      " [0.42973205]\n",
      " [0.42711312]\n",
      " [0.42932403]\n",
      " [0.42964506]\n",
      " [0.42987406]\n",
      " [0.42753264]\n",
      " [0.4294087 ]\n",
      " [0.43064404]\n",
      " [0.42897674]\n",
      " [0.43057716]\n",
      " [0.42591235]\n",
      " [0.42889258]\n",
      " [0.42925245]\n",
      " [0.42895567]\n",
      " [0.43053856]\n",
      " [0.43007448]\n",
      " [0.43052617]\n",
      " [0.4297855 ]\n",
      " [0.43028414]\n",
      " [0.43122372]\n",
      " [0.43111178]\n",
      " [0.43057448]\n",
      " [0.43142825]\n",
      " [0.43054587]\n",
      " [0.42985553]\n",
      " [0.43179497]\n",
      " [0.43256408]\n",
      " [0.43283516]\n",
      " [0.43375152]\n",
      " [0.433861  ]\n",
      " [0.4333916 ]\n",
      " [0.43378472]\n",
      " [0.43298224]\n",
      " [0.4340575 ]\n",
      " [0.43353173]\n",
      " [0.43203828]\n",
      " [0.43357885]\n",
      " [0.43346703]\n",
      " [0.43284124]\n",
      " [0.43182096]\n",
      " [0.4326121 ]\n",
      " [0.42532313]\n",
      " [0.43161815]\n",
      " [0.42972496]\n",
      " [0.43217325]\n",
      " [0.43247545]\n",
      " [0.43186918]\n",
      " [0.43288898]\n",
      " [0.43275946]\n",
      " [0.4333422 ]\n",
      " [0.43271083]\n",
      " [0.43193325]\n",
      " [0.43204266]\n",
      " [0.42985603]\n",
      " [0.42941576]\n",
      " [0.43251204]\n",
      " [0.43161267]\n",
      " [0.43239325]\n",
      " [0.43192238]\n",
      " [0.4292045 ]\n",
      " [0.42943603]\n",
      " [0.42982188]\n",
      " [0.4284996 ]\n",
      " [0.43146148]\n",
      " [0.42971513]\n",
      " [0.43218967]\n",
      " [0.4312725 ]\n",
      " [0.43195677]\n",
      " [0.43177205]\n",
      " [0.4336411 ]\n",
      " [0.43224722]\n",
      " [0.4329748 ]\n",
      " [0.4328869 ]\n",
      " [0.43080053]\n",
      " [0.43307936]\n",
      " [0.4331792 ]\n",
      " [0.43382576]\n",
      " [0.43214375]\n",
      " [0.4337677 ]\n",
      " [0.43361703]\n",
      " [0.43402955]\n",
      " [0.4343769 ]\n",
      " [0.43425182]\n",
      " [0.43480247]\n",
      " [0.4345868 ]\n",
      " [0.43282557]\n",
      " [0.43415868]\n",
      " [0.43464762]\n",
      " [0.43503216]\n",
      " [0.43508416]\n",
      " [0.435301  ]\n",
      " [0.43554848]\n",
      " [0.4347268 ]\n",
      " [0.43362626]\n",
      " [0.4330278 ]\n",
      " [0.43406194]\n",
      " [0.4332887 ]\n",
      " [0.43433797]\n",
      " [0.43436265]\n",
      " [0.43425608]\n",
      " [0.43438858]\n",
      " [0.43442562]\n",
      " [0.43421763]\n",
      " [0.43432713]\n",
      " [0.43473983]\n",
      " [0.4347022 ]\n",
      " [0.4346886 ]\n",
      " [0.43401766]\n",
      " [0.43300876]\n",
      " [0.43403834]\n",
      " [0.43319666]\n",
      " [0.43322402]\n",
      " [0.43334514]\n",
      " [0.43325835]\n",
      " [0.43370047]\n",
      " [0.43199262]\n",
      " [0.43263078]\n",
      " [0.43288976]\n",
      " [0.43340927]\n",
      " [0.43400222]\n",
      " [0.43462667]\n",
      " [0.43426535]\n",
      " [0.43174404]\n",
      " [0.43314496]\n",
      " [0.4335966 ]\n",
      " [0.43170908]\n",
      " [0.43183398]\n",
      " [0.43007886]\n",
      " [0.43234888]\n",
      " [0.4318819 ]\n",
      " [0.43201944]]\n",
      "[0.4354979  0.43558815 0.43561023 0.435562   0.43469203 0.43345\n",
      " 0.43456957 0.43565214 0.435413   0.4355847  0.43570128 0.4355491\n",
      " 0.4358361  0.43596888 0.43561542 0.4359579  0.43607283 0.43594855\n",
      " 0.4355748  0.4358237  0.4359811  0.4358561  0.43513268 0.43149382\n",
      " 0.43113872 0.43265224 0.4333266  0.43365663 0.43251127 0.43313056\n",
      " 0.43400028 0.43379197 0.43448353 0.43358546 0.43426952 0.43441126\n",
      " 0.4326591  0.43481544 0.4344091  0.43339348 0.43372115 0.43381262\n",
      " 0.43283835 0.43302923 0.43352365 0.4344103  0.43433917 0.4342104\n",
      " 0.43275785 0.43272525 0.43144655 0.43419042 0.4336922  0.43348247\n",
      " 0.43454826 0.43343338 0.43451577 0.4345897  0.43503574 0.43414378\n",
      " 0.43421197 0.43433982 0.4347183  0.43363312 0.43426675 0.434417\n",
      " 0.43449518 0.43478882 0.43445992 0.43295524 0.43464014 0.43453798\n",
      " 0.4336814  0.4351494  0.4348963  0.4348862  0.43496612 0.43507147\n",
      " 0.43500084 0.43382904 0.4345926  0.4344613  0.4310461  0.42530167\n",
      " 0.42865586 0.42941147 0.42555457 0.4274875  0.42922676 0.42943096\n",
      " 0.43093297 0.43090665 0.43038368 0.43086946 0.43109655 0.4305979\n",
      " 0.43261397 0.432126   0.4307598  0.42979768 0.42881638 0.42938387\n",
      " 0.4310669  0.43114048 0.43064985 0.43116298 0.43248776 0.43172187\n",
      " 0.43079326 0.43065086 0.43205523 0.4307305  0.4311005  0.4320444\n",
      " 0.4315592  0.4262097  0.42573604 0.42955273 0.4272474  0.4276554\n",
      " 0.42973205 0.42711312 0.42932403 0.42964506 0.42987406 0.42753264\n",
      " 0.4294087  0.43064404 0.42897674 0.43057716 0.42591235 0.42889258\n",
      " 0.42925245 0.42895567 0.43053856 0.43007448 0.43052617 0.4297855\n",
      " 0.43028414 0.43122372 0.43111178 0.43057448 0.43142825 0.43054587\n",
      " 0.42985553 0.43179497 0.43256408 0.43283516 0.43375152 0.433861\n",
      " 0.4333916  0.43378472 0.43298224 0.4340575  0.43353173 0.43203828\n",
      " 0.43357885 0.43346703 0.43284124 0.43182096 0.4326121  0.42532313\n",
      " 0.43161815 0.42972496 0.43217325 0.43247545 0.43186918 0.43288898\n",
      " 0.43275946 0.4333422  0.43271083 0.43193325 0.43204266 0.42985603\n",
      " 0.42941576 0.43251204 0.43161267 0.43239325 0.43192238 0.4292045\n",
      " 0.42943603 0.42982188 0.4284996  0.43146148 0.42971513 0.43218967\n",
      " 0.4312725  0.43195677 0.43177205 0.4336411  0.43224722 0.4329748\n",
      " 0.4328869  0.43080053 0.43307936 0.4331792  0.43382576 0.43214375\n",
      " 0.4337677  0.43361703 0.43402955 0.4343769  0.43425182 0.43480247\n",
      " 0.4345868  0.43282557 0.43415868 0.43464762 0.43503216 0.43508416\n",
      " 0.435301   0.43554848 0.4347268  0.43362626 0.4330278  0.43406194\n",
      " 0.4332887  0.43433797 0.43436265 0.43425608 0.43438858 0.43442562\n",
      " 0.43421763 0.43432713 0.43473983 0.4347022  0.4346886  0.43401766\n",
      " 0.43300876 0.43403834 0.43319666 0.43322402 0.43334514 0.43325835\n",
      " 0.43370047 0.43199262 0.43263078 0.43288976 0.43340927 0.43400222\n",
      " 0.43462667 0.43426535 0.43174404 0.43314496 0.4335966  0.43170908\n",
      " 0.43183398 0.43007886 0.43234888 0.4318819  0.43201944]\n",
      "JPM 0.5356359395775329\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed\n",
      "            RSI       ATR       Date    Volume   Returns\n",
      "1     45.058527  0.908669 2013-01-03   8910100 -0.006375\n",
      "2     46.929911  0.903765 2013-01-04   6438000  0.003772\n",
      "3     42.932481  0.895639 2013-01-07   6201400 -0.009603\n",
      "4     44.400611  0.880236 2013-01-08   5866900  0.002774\n",
      "5     44.271437  0.852362 2013-01-09   5055200 -0.000292\n",
      "...         ...       ...        ...       ...       ...\n",
      "1505  22.049050  1.980552 2018-12-24   6110300 -0.015149\n",
      "1506  42.246710  2.167655 2018-12-26  10028300  0.052103\n",
      "1507  46.112326  2.239251 2018-12-27   9881500  0.012967\n",
      "1508  47.833182  2.213590 2018-12-28   9874000  0.005879\n",
      "1509  51.015116  2.145477 2018-12-31   7005800  0.011011\n",
      "\n",
      "[1509 rows x 5 columns]\n",
      "1198\n",
      "[[ 4.50585274e+01  9.08669495e-01  8.91010000e+06 -6.37516438e-03]\n",
      " [ 4.69299109e+01  9.03764814e-01  6.43800000e+06  3.77182159e-03]\n",
      " [ 4.29324812e+01  8.95638821e-01  6.20140000e+06 -9.60296427e-03]\n",
      " ...\n",
      " [ 6.94337999e+01  1.28891770e+00  5.14080000e+06  1.00805554e-03]\n",
      " [ 7.00002777e+01  1.23256643e+00  9.76390000e+06  1.40950470e-03]\n",
      " [ 6.40631424e+01  1.21166901e+00  7.14430000e+06 -6.56084255e-03]]\n",
      "Length of inputs 1198\n",
      "length of time-series - inputs (1198, 60, 4)\n",
      "length of time-series - outputs (1198,)\n",
      "Training Size (1198, 60, 4) (1198,)\n",
      "Length of inputs 251\n",
      "length of time-series - inputs (251, 60, 4)\n",
      "length of time-series - outputs (251,)\n",
      "Test Size (251, 60, 4) (251,)\n",
      "Reloading Tuner from .\\untitled_project\\tuner0.json\n",
      "WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_23460\\465918227.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_stock.Date = pd.to_datetime(df_stock.Date)\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\base.py:443: UserWarning: X has feature names, but MinMaxScaler was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).layer_with_weights-1.cell.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).layer_with_weights-1.cell.recurrent_kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).layer_with_weights-1.cell.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.19\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.20\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 60, 32)            4736      \n",
      "                                                                 \n",
      " global_average_pooling1d (  (None, 32)                0         \n",
      " GlobalAveragePooling1D)                                         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 60)                1980      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 20)                1220      \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 20)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 21        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 7957 (31.08 KB)\n",
      "Trainable params: 7957 (31.08 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "WMT None\n",
      "Epoch 1/10\n",
      "19/24 [======================>.......] - ETA: 0s - loss: 0.0701WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 1s 9ms/step - loss: 0.0697\n",
      "Epoch 2/10\n",
      "19/24 [======================>.......] - ETA: 0s - loss: 0.0347WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.0425\n",
      "Epoch 3/10\n",
      "20/24 [========================>.....] - ETA: 0s - loss: 0.0258WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.0307\n",
      "Epoch 4/10\n",
      "20/24 [========================>.....] - ETA: 0s - loss: 0.0253WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.0293\n",
      "Epoch 5/10\n",
      "20/24 [========================>.....] - ETA: 0s - loss: 0.0262WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.0291\n",
      "Epoch 6/10\n",
      "21/24 [=========================>....] - ETA: 0s - loss: 0.0263WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.0283\n",
      "Epoch 7/10\n",
      "20/24 [========================>.....] - ETA: 0s - loss: 0.0245WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.0274\n",
      "Epoch 8/10\n",
      "19/24 [======================>.......] - ETA: 0s - loss: 0.0208WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.0236\n",
      "Epoch 9/10\n",
      "19/24 [======================>.......] - ETA: 0s - loss: 0.0200WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.0223\n",
      "Epoch 10/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0211WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.0211\n",
      "6/6 [==============================] - 0s 3ms/step\n",
      "[[0.42194414]\n",
      " [0.4231845 ]\n",
      " [0.42428893]\n",
      " [0.4247055 ]\n",
      " [0.42448002]\n",
      " [0.4239822 ]\n",
      " [0.42337477]\n",
      " [0.42288047]\n",
      " [0.42254722]\n",
      " [0.42241853]\n",
      " [0.42253226]\n",
      " [0.42304486]\n",
      " [0.42376024]\n",
      " [0.42452765]\n",
      " [0.4253983 ]\n",
      " [0.42636544]\n",
      " [0.4273628 ]\n",
      " [0.4282815 ]\n",
      " [0.42921865]\n",
      " [0.4303125 ]\n",
      " [0.4313832 ]\n",
      " [0.4325236 ]\n",
      " [0.43367362]\n",
      " [0.43482298]\n",
      " [0.4362268 ]\n",
      " [0.43776876]\n",
      " [0.43927664]\n",
      " [0.4403811 ]\n",
      " [0.44119042]\n",
      " [0.44139344]\n",
      " [0.44202298]\n",
      " [0.44303823]\n",
      " [0.44371605]\n",
      " [0.44427472]\n",
      " [0.44451416]\n",
      " [0.4449305 ]\n",
      " [0.44545048]\n",
      " [0.44601393]\n",
      " [0.44649172]\n",
      " [0.44693118]\n",
      " [0.4473979 ]\n",
      " [0.44772923]\n",
      " [0.44812745]\n",
      " [0.44861418]\n",
      " [0.44913447]\n",
      " [0.4497115 ]\n",
      " [0.4504075 ]\n",
      " [0.45119566]\n",
      " [0.4519443 ]\n",
      " [0.4525211 ]\n",
      " [0.45309353]\n",
      " [0.45377308]\n",
      " [0.45436943]\n",
      " [0.45491904]\n",
      " [0.45545518]\n",
      " [0.4561128 ]\n",
      " [0.45690978]\n",
      " [0.45775074]\n",
      " [0.45880586]\n",
      " [0.45990437]\n",
      " [0.46114612]\n",
      " [0.46247756]\n",
      " [0.4635318 ]\n",
      " [0.46477968]\n",
      " [0.46598655]\n",
      " [0.4667145 ]\n",
      " [0.46721858]\n",
      " [0.46781564]\n",
      " [0.46825922]\n",
      " [0.46845108]\n",
      " [0.46863467]\n",
      " [0.46849883]\n",
      " [0.46808636]\n",
      " [0.4673878 ]\n",
      " [0.4661138 ]\n",
      " [0.46407187]\n",
      " [0.4609273 ]\n",
      " [0.4556712 ]\n",
      " [0.4479394 ]\n",
      " [0.4368449 ]\n",
      " [0.42055482]\n",
      " [0.40627038]\n",
      " [0.39969784]\n",
      " [0.39610666]\n",
      " [0.39763057]\n",
      " [0.3974113 ]\n",
      " [0.3977298 ]\n",
      " [0.40001935]\n",
      " [0.4025855 ]\n",
      " [0.40463495]\n",
      " [0.4062966 ]\n",
      " [0.40776592]\n",
      " [0.40829957]\n",
      " [0.40796912]\n",
      " [0.41168582]\n",
      " [0.4128412 ]\n",
      " [0.4144684 ]\n",
      " [0.41662717]\n",
      " [0.4184122 ]\n",
      " [0.4207009 ]\n",
      " [0.4228955 ]\n",
      " [0.424729  ]\n",
      " [0.42620504]\n",
      " [0.42764783]\n",
      " [0.42946947]\n",
      " [0.4313991 ]\n",
      " [0.4326533 ]\n",
      " [0.43341893]\n",
      " [0.43454772]\n",
      " [0.43518585]\n",
      " [0.43579304]\n",
      " [0.43626392]\n",
      " [0.4354729 ]\n",
      " [0.43617177]\n",
      " [0.43579513]\n",
      " [0.43509346]\n",
      " [0.43452996]\n",
      " [0.43471467]\n",
      " [0.43331438]\n",
      " [0.43334234]\n",
      " [0.43209857]\n",
      " [0.43084484]\n",
      " [0.43124038]\n",
      " [0.430925  ]\n",
      " [0.43070465]\n",
      " [0.4302199 ]\n",
      " [0.43009925]\n",
      " [0.43012142]\n",
      " [0.42998433]\n",
      " [0.42991287]\n",
      " [0.43016052]\n",
      " [0.43020242]\n",
      " [0.42985952]\n",
      " [0.4289775 ]\n",
      " [0.42815226]\n",
      " [0.42712587]\n",
      " [0.42643481]\n",
      " [0.42592084]\n",
      " [0.42524898]\n",
      " [0.42430365]\n",
      " [0.4227867 ]\n",
      " [0.42122346]\n",
      " [0.41895252]\n",
      " [0.4168774 ]\n",
      " [0.41501784]\n",
      " [0.4132027 ]\n",
      " [0.4108441 ]\n",
      " [0.40916377]\n",
      " [0.40731782]\n",
      " [0.40697426]\n",
      " [0.40603858]\n",
      " [0.40470392]\n",
      " [0.40294796]\n",
      " [0.400945  ]\n",
      " [0.3987831 ]\n",
      " [0.39748627]\n",
      " [0.39622217]\n",
      " [0.39442366]\n",
      " [0.3941412 ]\n",
      " [0.39439148]\n",
      " [0.3949455 ]\n",
      " [0.39570642]\n",
      " [0.396699  ]\n",
      " [0.39730012]\n",
      " [0.39823294]\n",
      " [0.39910054]\n",
      " [0.39980483]\n",
      " [0.40042412]\n",
      " [0.40121174]\n",
      " [0.40200764]\n",
      " [0.40286863]\n",
      " [0.40391695]\n",
      " [0.4050027 ]\n",
      " [0.40614545]\n",
      " [0.40735996]\n",
      " [0.40868998]\n",
      " [0.41001564]\n",
      " [0.41112703]\n",
      " [0.41210008]\n",
      " [0.4127292 ]\n",
      " [0.4133436 ]\n",
      " [0.41353226]\n",
      " [0.41340017]\n",
      " [0.41269422]\n",
      " [0.4116202 ]\n",
      " [0.4106713 ]\n",
      " [0.40971833]\n",
      " [0.40867752]\n",
      " [0.40750325]\n",
      " [0.4063816 ]\n",
      " [0.4051985 ]\n",
      " [0.4037969 ]\n",
      " [0.40267706]\n",
      " [0.4015895 ]\n",
      " [0.40093422]\n",
      " [0.4007913 ]\n",
      " [0.4008838 ]\n",
      " [0.4012769 ]\n",
      " [0.40208787]\n",
      " [0.4029566 ]\n",
      " [0.40418893]\n",
      " [0.40558243]\n",
      " [0.40705365]\n",
      " [0.40859133]\n",
      " [0.41015184]\n",
      " [0.41182482]\n",
      " [0.4134732 ]\n",
      " [0.41521847]\n",
      " [0.41692197]\n",
      " [0.4187234 ]\n",
      " [0.42065597]\n",
      " [0.42257786]\n",
      " [0.42456007]\n",
      " [0.4263926 ]\n",
      " [0.42779762]\n",
      " [0.4287836 ]\n",
      " [0.4290281 ]\n",
      " [0.42954135]\n",
      " [0.4303704 ]\n",
      " [0.4301998 ]\n",
      " [0.43031353]\n",
      " [0.43036187]\n",
      " [0.43042368]\n",
      " [0.43056023]\n",
      " [0.43087643]\n",
      " [0.43117863]\n",
      " [0.43161142]\n",
      " [0.4320572 ]\n",
      " [0.43240297]\n",
      " [0.43275642]\n",
      " [0.43344426]\n",
      " [0.43409967]\n",
      " [0.4347362 ]\n",
      " [0.43565017]\n",
      " [0.4365986 ]\n",
      " [0.43766612]\n",
      " [0.43886918]\n",
      " [0.44043404]\n",
      " [0.4421566 ]\n",
      " [0.44374043]\n",
      " [0.44531548]\n",
      " [0.44687033]\n",
      " [0.44852674]\n",
      " [0.45025808]\n",
      " [0.45197135]\n",
      " [0.45350766]\n",
      " [0.4552176 ]\n",
      " [0.45695072]\n",
      " [0.45910335]\n",
      " [0.46110755]\n",
      " [0.46256793]]\n",
      "[0.42194414 0.4231845  0.42428893 0.4247055  0.42448002 0.4239822\n",
      " 0.42337477 0.42288047 0.42254722 0.42241853 0.42253226 0.42304486\n",
      " 0.42376024 0.42452765 0.4253983  0.42636544 0.4273628  0.4282815\n",
      " 0.42921865 0.4303125  0.4313832  0.4325236  0.43367362 0.43482298\n",
      " 0.4362268  0.43776876 0.43927664 0.4403811  0.44119042 0.44139344\n",
      " 0.44202298 0.44303823 0.44371605 0.44427472 0.44451416 0.4449305\n",
      " 0.44545048 0.44601393 0.44649172 0.44693118 0.4473979  0.44772923\n",
      " 0.44812745 0.44861418 0.44913447 0.4497115  0.4504075  0.45119566\n",
      " 0.4519443  0.4525211  0.45309353 0.45377308 0.45436943 0.45491904\n",
      " 0.45545518 0.4561128  0.45690978 0.45775074 0.45880586 0.45990437\n",
      " 0.46114612 0.46247756 0.4635318  0.46477968 0.46598655 0.4667145\n",
      " 0.46721858 0.46781564 0.46825922 0.46845108 0.46863467 0.46849883\n",
      " 0.46808636 0.4673878  0.4661138  0.46407187 0.4609273  0.4556712\n",
      " 0.4479394  0.4368449  0.42055482 0.40627038 0.39969784 0.39610666\n",
      " 0.39763057 0.3974113  0.3977298  0.40001935 0.4025855  0.40463495\n",
      " 0.4062966  0.40776592 0.40829957 0.40796912 0.41168582 0.4128412\n",
      " 0.4144684  0.41662717 0.4184122  0.4207009  0.4228955  0.424729\n",
      " 0.42620504 0.42764783 0.42946947 0.4313991  0.4326533  0.43341893\n",
      " 0.43454772 0.43518585 0.43579304 0.43626392 0.4354729  0.43617177\n",
      " 0.43579513 0.43509346 0.43452996 0.43471467 0.43331438 0.43334234\n",
      " 0.43209857 0.43084484 0.43124038 0.430925   0.43070465 0.4302199\n",
      " 0.43009925 0.43012142 0.42998433 0.42991287 0.43016052 0.43020242\n",
      " 0.42985952 0.4289775  0.42815226 0.42712587 0.42643481 0.42592084\n",
      " 0.42524898 0.42430365 0.4227867  0.42122346 0.41895252 0.4168774\n",
      " 0.41501784 0.4132027  0.4108441  0.40916377 0.40731782 0.40697426\n",
      " 0.40603858 0.40470392 0.40294796 0.400945   0.3987831  0.39748627\n",
      " 0.39622217 0.39442366 0.3941412  0.39439148 0.3949455  0.39570642\n",
      " 0.396699   0.39730012 0.39823294 0.39910054 0.39980483 0.40042412\n",
      " 0.40121174 0.40200764 0.40286863 0.40391695 0.4050027  0.40614545\n",
      " 0.40735996 0.40868998 0.41001564 0.41112703 0.41210008 0.4127292\n",
      " 0.4133436  0.41353226 0.41340017 0.41269422 0.4116202  0.4106713\n",
      " 0.40971833 0.40867752 0.40750325 0.4063816  0.4051985  0.4037969\n",
      " 0.40267706 0.4015895  0.40093422 0.4007913  0.4008838  0.4012769\n",
      " 0.40208787 0.4029566  0.40418893 0.40558243 0.40705365 0.40859133\n",
      " 0.41015184 0.41182482 0.4134732  0.41521847 0.41692197 0.4187234\n",
      " 0.42065597 0.42257786 0.42456007 0.4263926  0.42779762 0.4287836\n",
      " 0.4290281  0.42954135 0.4303704  0.4301998  0.43031353 0.43036187\n",
      " 0.43042368 0.43056023 0.43087643 0.43117863 0.43161142 0.4320572\n",
      " 0.43240297 0.43275642 0.43344426 0.43409967 0.4347362  0.43565017\n",
      " 0.4365986  0.43766612 0.43886918 0.44043404 0.4421566  0.44374043\n",
      " 0.44531548 0.44687033 0.44852674 0.45025808 0.45197135 0.45350766\n",
      " 0.4552176  0.45695072 0.45910335 0.46110755 0.46256793]\n",
      "WMT 0.20433687845878237\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed\n",
      "            RSI       ATR       Date   Volume   Returns\n",
      "1     63.715073  0.194377 2013-01-03  2462000 -0.002898\n",
      "2     66.176254  0.189708 2013-01-04  1611000  0.005907\n",
      "3     67.554744  0.192300 2013-01-07  2802000  0.003420\n",
      "4     71.192639  0.191636 2013-01-08  4150000  0.009775\n",
      "5     71.936511  0.189733 2013-01-09  2535000  0.002133\n",
      "...         ...       ...        ...      ...       ...\n",
      "1505  41.594219  0.458917 2018-12-24  1744400 -0.011077\n",
      "1506  49.161106  0.478637 2018-12-26  2354600  0.024254\n",
      "1507  52.361377  0.479448 2018-12-27  2486000  0.011472\n",
      "1508  53.806691  0.470202 2018-12-28  2713600  0.005250\n",
      "1509  55.268182  0.450545 2018-12-31  1450400  0.005223\n",
      "\n",
      "[1509 rows x 5 columns]\n",
      "1198\n",
      "[[ 6.37150733e+01  1.94377381e-01  2.46200000e+06 -2.89752866e-03]\n",
      " [ 6.61762538e+01  1.89707547e-01  1.61100000e+06  5.90724238e-03]\n",
      " [ 6.75547445e+01  1.92299853e-01  2.80200000e+06  3.42024300e-03]\n",
      " ...\n",
      " [ 6.35602110e+01  3.29168476e-01  1.31580000e+06 -1.86557945e-04]\n",
      " [ 6.47029807e+01  3.15656399e-01  8.70800000e+05  2.98232995e-03]\n",
      " [ 6.38936764e+01  3.09180969e-01  1.12360000e+06 -1.11735354e-03]]\n",
      "Length of inputs 1198\n",
      "length of time-series - inputs (1198, 60, 4)\n",
      "length of time-series - outputs (1198,)\n",
      "Training Size (1198, 60, 4) (1198,)\n",
      "Length of inputs 251\n",
      "length of time-series - inputs (251, 60, 4)\n",
      "length of time-series - outputs (251,)\n",
      "Test Size (251, 60, 4) (251,)\n",
      "Reloading Tuner from .\\untitled_project\\tuner0.json\n",
      "WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_23460\\465918227.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_stock.Date = pd.to_datetime(df_stock.Date)\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\base.py:443: UserWarning: X has feature names, but MinMaxScaler was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).layer_with_weights-1.cell.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).layer_with_weights-1.cell.recurrent_kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).layer_with_weights-1.cell.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.19\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.20\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 60, 272)           301376    \n",
      "                                                                 \n",
      " global_average_pooling1d (  (None, 272)               0         \n",
      " GlobalAveragePooling1D)                                         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 60)                16380     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 20)                1220      \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 20)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 21        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 318997 (1.22 MB)\n",
      "Trainable params: 318997 (1.22 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "NVO None\n",
      "Epoch 1/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.5313WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 3s 86ms/step - loss: 0.5313\n",
      "Epoch 2/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0372WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 2s 87ms/step - loss: 0.0372\n",
      "Epoch 3/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0700WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 2s 88ms/step - loss: 0.0700\n",
      "Epoch 4/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.1542WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 2s 86ms/step - loss: 0.1542\n",
      "Epoch 5/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0719WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 2s 86ms/step - loss: 0.0719\n",
      "Epoch 6/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0545WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 2s 86ms/step - loss: 0.0545\n",
      "Epoch 7/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0587WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 2s 89ms/step - loss: 0.0587\n",
      "Epoch 8/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0560WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 2s 85ms/step - loss: 0.0560\n",
      "Epoch 9/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0553WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 2s 86ms/step - loss: 0.0553\n",
      "Epoch 10/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0551WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 2s 85ms/step - loss: 0.0551\n",
      "6/6 [==============================] - 0s 22ms/step\n",
      "[[0.38362285]\n",
      " [0.3837396 ]\n",
      " [0.38370845]\n",
      " [0.38371256]\n",
      " [0.38375202]\n",
      " [0.38366032]\n",
      " [0.38368368]\n",
      " [0.38380858]\n",
      " [0.3837065 ]\n",
      " [0.38364214]\n",
      " [0.38356736]\n",
      " [0.3834768 ]\n",
      " [0.38343352]\n",
      " [0.383434  ]\n",
      " [0.38338968]\n",
      " [0.38336784]\n",
      " [0.3834589 ]\n",
      " [0.38345885]\n",
      " [0.3834641 ]\n",
      " [0.38350883]\n",
      " [0.38353124]\n",
      " [0.38359255]\n",
      " [0.38360825]\n",
      " [0.38371402]\n",
      " [0.38376254]\n",
      " [0.38378668]\n",
      " [0.3839324 ]\n",
      " [0.3840065 ]\n",
      " [0.38414934]\n",
      " [0.38429788]\n",
      " [0.38446423]\n",
      " [0.38455707]\n",
      " [0.38465065]\n",
      " [0.38471457]\n",
      " [0.38477126]\n",
      " [0.3848253 ]\n",
      " [0.3848755 ]\n",
      " [0.3849918 ]\n",
      " [0.38503736]\n",
      " [0.3851664 ]\n",
      " [0.38527513]\n",
      " [0.38532904]\n",
      " [0.38549832]\n",
      " [0.3856305 ]\n",
      " [0.38566414]\n",
      " [0.38578534]\n",
      " [0.3858607 ]\n",
      " [0.385919  ]\n",
      " [0.38595524]\n",
      " [0.38594618]\n",
      " [0.38602197]\n",
      " [0.38609976]\n",
      " [0.3861223 ]\n",
      " [0.38617608]\n",
      " [0.38623968]\n",
      " [0.38624418]\n",
      " [0.38626102]\n",
      " [0.38629285]\n",
      " [0.38634732]\n",
      " [0.38640186]\n",
      " [0.38650107]\n",
      " [0.38655934]\n",
      " [0.3865911 ]\n",
      " [0.3866747 ]\n",
      " [0.38672626]\n",
      " [0.38678452]\n",
      " [0.38683212]\n",
      " [0.38689473]\n",
      " [0.3869565 ]\n",
      " [0.3870307 ]\n",
      " [0.38709867]\n",
      " [0.387049  ]\n",
      " [0.38704997]\n",
      " [0.38694525]\n",
      " [0.38684884]\n",
      " [0.3867089 ]\n",
      " [0.38656422]\n",
      " [0.38643658]\n",
      " [0.38623458]\n",
      " [0.3860232 ]\n",
      " [0.38577572]\n",
      " [0.38545525]\n",
      " [0.38548228]\n",
      " [0.38550544]\n",
      " [0.38555235]\n",
      " [0.38551825]\n",
      " [0.38548118]\n",
      " [0.38555574]\n",
      " [0.38544714]\n",
      " [0.38538244]\n",
      " [0.3853732 ]\n",
      " [0.38532642]\n",
      " [0.38528556]\n",
      " [0.38524672]\n",
      " [0.38525715]\n",
      " [0.38521516]\n",
      " [0.3851074 ]\n",
      " [0.3850033 ]\n",
      " [0.3848585 ]\n",
      " [0.38482645]\n",
      " [0.38481113]\n",
      " [0.38480633]\n",
      " [0.38472965]\n",
      " [0.38467026]\n",
      " [0.3846363 ]\n",
      " [0.3845888 ]\n",
      " [0.3845296 ]\n",
      " [0.38445693]\n",
      " [0.3843938 ]\n",
      " [0.38434488]\n",
      " [0.38427886]\n",
      " [0.38420498]\n",
      " [0.38410458]\n",
      " [0.38401505]\n",
      " [0.38390526]\n",
      " [0.38385293]\n",
      " [0.3838782 ]\n",
      " [0.3837941 ]\n",
      " [0.383651  ]\n",
      " [0.38365275]\n",
      " [0.38355836]\n",
      " [0.38349438]\n",
      " [0.3834699 ]\n",
      " [0.38339156]\n",
      " [0.38334852]\n",
      " [0.38335046]\n",
      " [0.38334975]\n",
      " [0.38326728]\n",
      " [0.3831772 ]\n",
      " [0.3831314 ]\n",
      " [0.3830548 ]\n",
      " [0.3830536 ]\n",
      " [0.38302794]\n",
      " [0.3829938 ]\n",
      " [0.3829931 ]\n",
      " [0.38303357]\n",
      " [0.38302395]\n",
      " [0.38298613]\n",
      " [0.38292575]\n",
      " [0.38294715]\n",
      " [0.38296208]\n",
      " [0.38302124]\n",
      " [0.38313258]\n",
      " [0.3831417 ]\n",
      " [0.38300437]\n",
      " [0.382954  ]\n",
      " [0.38288552]\n",
      " [0.38284704]\n",
      " [0.3827731 ]\n",
      " [0.3827585 ]\n",
      " [0.38274843]\n",
      " [0.38266248]\n",
      " [0.38253054]\n",
      " [0.3825675 ]\n",
      " [0.3824785 ]\n",
      " [0.3824233 ]\n",
      " [0.38239855]\n",
      " [0.38238323]\n",
      " [0.38237745]\n",
      " [0.38241532]\n",
      " [0.38240102]\n",
      " [0.38246024]\n",
      " [0.3825237 ]\n",
      " [0.3824896 ]\n",
      " [0.3825169 ]\n",
      " [0.38254616]\n",
      " [0.38253978]\n",
      " [0.38251755]\n",
      " [0.38242024]\n",
      " [0.38240334]\n",
      " [0.38239327]\n",
      " [0.3823882 ]\n",
      " [0.38248494]\n",
      " [0.38249743]\n",
      " [0.38253853]\n",
      " [0.38256556]\n",
      " [0.3827619 ]\n",
      " [0.38292888]\n",
      " [0.382935  ]\n",
      " [0.38286957]\n",
      " [0.38277602]\n",
      " [0.38263196]\n",
      " [0.38260984]\n",
      " [0.3826583 ]\n",
      " [0.38269985]\n",
      " [0.38269964]\n",
      " [0.38268274]\n",
      " [0.38266042]\n",
      " [0.3826053 ]\n",
      " [0.3825185 ]\n",
      " [0.38248026]\n",
      " [0.38235644]\n",
      " [0.38237786]\n",
      " [0.38232562]\n",
      " [0.38230515]\n",
      " [0.38236502]\n",
      " [0.3823283 ]\n",
      " [0.3823259 ]\n",
      " [0.38238543]\n",
      " [0.3823771 ]\n",
      " [0.38242647]\n",
      " [0.38246444]\n",
      " [0.38249034]\n",
      " [0.38246593]\n",
      " [0.38242418]\n",
      " [0.38232762]\n",
      " [0.38229832]\n",
      " [0.3822918 ]\n",
      " [0.3822862 ]\n",
      " [0.38226724]\n",
      " [0.3822806 ]\n",
      " [0.3821254 ]\n",
      " [0.3823643 ]\n",
      " [0.38247207]\n",
      " [0.3825907 ]\n",
      " [0.38263732]\n",
      " [0.3826453 ]\n",
      " [0.38277745]\n",
      " [0.38289583]\n",
      " [0.38294247]\n",
      " [0.38300103]\n",
      " [0.3829667 ]\n",
      " [0.38298738]\n",
      " [0.3830088 ]\n",
      " [0.3830393 ]\n",
      " [0.38304153]\n",
      " [0.3831149 ]\n",
      " [0.38314646]\n",
      " [0.3832186 ]\n",
      " [0.38328052]\n",
      " [0.38335505]\n",
      " [0.3834893 ]\n",
      " [0.38365465]\n",
      " [0.38382596]\n",
      " [0.38392   ]\n",
      " [0.38404867]\n",
      " [0.3840833 ]\n",
      " [0.38408595]\n",
      " [0.3842107 ]\n",
      " [0.38430592]\n",
      " [0.38443917]\n",
      " [0.38465777]\n",
      " [0.38475686]\n",
      " [0.38486853]\n",
      " [0.38491285]\n",
      " [0.38486576]\n",
      " [0.38488245]\n",
      " [0.38484615]\n",
      " [0.38494915]\n",
      " [0.3851198 ]\n",
      " [0.38498774]]\n",
      "[0.38362285 0.3837396  0.38370845 0.38371256 0.38375202 0.38366032\n",
      " 0.38368368 0.38380858 0.3837065  0.38364214 0.38356736 0.3834768\n",
      " 0.38343352 0.383434   0.38338968 0.38336784 0.3834589  0.38345885\n",
      " 0.3834641  0.38350883 0.38353124 0.38359255 0.38360825 0.38371402\n",
      " 0.38376254 0.38378668 0.3839324  0.3840065  0.38414934 0.38429788\n",
      " 0.38446423 0.38455707 0.38465065 0.38471457 0.38477126 0.3848253\n",
      " 0.3848755  0.3849918  0.38503736 0.3851664  0.38527513 0.38532904\n",
      " 0.38549832 0.3856305  0.38566414 0.38578534 0.3858607  0.385919\n",
      " 0.38595524 0.38594618 0.38602197 0.38609976 0.3861223  0.38617608\n",
      " 0.38623968 0.38624418 0.38626102 0.38629285 0.38634732 0.38640186\n",
      " 0.38650107 0.38655934 0.3865911  0.3866747  0.38672626 0.38678452\n",
      " 0.38683212 0.38689473 0.3869565  0.3870307  0.38709867 0.387049\n",
      " 0.38704997 0.38694525 0.38684884 0.3867089  0.38656422 0.38643658\n",
      " 0.38623458 0.3860232  0.38577572 0.38545525 0.38548228 0.38550544\n",
      " 0.38555235 0.38551825 0.38548118 0.38555574 0.38544714 0.38538244\n",
      " 0.3853732  0.38532642 0.38528556 0.38524672 0.38525715 0.38521516\n",
      " 0.3851074  0.3850033  0.3848585  0.38482645 0.38481113 0.38480633\n",
      " 0.38472965 0.38467026 0.3846363  0.3845888  0.3845296  0.38445693\n",
      " 0.3843938  0.38434488 0.38427886 0.38420498 0.38410458 0.38401505\n",
      " 0.38390526 0.38385293 0.3838782  0.3837941  0.383651   0.38365275\n",
      " 0.38355836 0.38349438 0.3834699  0.38339156 0.38334852 0.38335046\n",
      " 0.38334975 0.38326728 0.3831772  0.3831314  0.3830548  0.3830536\n",
      " 0.38302794 0.3829938  0.3829931  0.38303357 0.38302395 0.38298613\n",
      " 0.38292575 0.38294715 0.38296208 0.38302124 0.38313258 0.3831417\n",
      " 0.38300437 0.382954   0.38288552 0.38284704 0.3827731  0.3827585\n",
      " 0.38274843 0.38266248 0.38253054 0.3825675  0.3824785  0.3824233\n",
      " 0.38239855 0.38238323 0.38237745 0.38241532 0.38240102 0.38246024\n",
      " 0.3825237  0.3824896  0.3825169  0.38254616 0.38253978 0.38251755\n",
      " 0.38242024 0.38240334 0.38239327 0.3823882  0.38248494 0.38249743\n",
      " 0.38253853 0.38256556 0.3827619  0.38292888 0.382935   0.38286957\n",
      " 0.38277602 0.38263196 0.38260984 0.3826583  0.38269985 0.38269964\n",
      " 0.38268274 0.38266042 0.3826053  0.3825185  0.38248026 0.38235644\n",
      " 0.38237786 0.38232562 0.38230515 0.38236502 0.3823283  0.3823259\n",
      " 0.38238543 0.3823771  0.38242647 0.38246444 0.38249034 0.38246593\n",
      " 0.38242418 0.38232762 0.38229832 0.3822918  0.3822862  0.38226724\n",
      " 0.3822806  0.3821254  0.3823643  0.38247207 0.3825907  0.38263732\n",
      " 0.3826453  0.38277745 0.38289583 0.38294247 0.38300103 0.3829667\n",
      " 0.38298738 0.3830088  0.3830393  0.38304153 0.3831149  0.38314646\n",
      " 0.3832186  0.38328052 0.38335505 0.3834893  0.38365465 0.38382596\n",
      " 0.38392    0.38404867 0.3840833  0.38408595 0.3842107  0.38430592\n",
      " 0.38443917 0.38465777 0.38475686 0.38486853 0.38491285 0.38486576\n",
      " 0.38488245 0.38484615 0.38494915 0.3851198  0.38498774]\n",
      "NVO 0.018551511434845337\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed\n",
      "            RSI       ATR       Date    Volume   Returns\n",
      "1     56.994588  0.646496 2013-01-03   9598300 -0.001413\n",
      "2     64.242990  0.671032 2013-01-04  11631800  0.011386\n",
      "3     62.153790  0.647387 2013-01-07   7548800 -0.002098\n",
      "4     62.241926  0.629717 2013-01-08   9825300  0.000140\n",
      "5     65.047577  0.616165 2013-01-09   7672800  0.004471\n",
      "...         ...       ...        ...       ...       ...\n",
      "1505  22.357910  3.818837 2018-12-24   7531900 -0.041851\n",
      "1506  32.109660  3.885349 2018-12-26   9253000  0.031018\n",
      "1507  33.730999  3.917824 2018-12-27   9918700  0.005509\n",
      "1508  33.558349  3.780837 2018-12-28   6537200 -0.001100\n",
      "1509  37.909058  3.673635 2018-12-31   7409900  0.013889\n",
      "\n",
      "[1509 rows x 5 columns]\n",
      "1198\n",
      "[[ 5.69945877e+01  6.46495924e-01  9.59830000e+06 -1.41273000e-03]\n",
      " [ 6.42429899e+01  6.71032322e-01  1.16318000e+07  1.13855158e-02]\n",
      " [ 6.21537904e+01  6.47386894e-01  7.54880000e+06 -2.09842788e-03]\n",
      " ...\n",
      " [ 5.17151319e+01  1.51739114e+00  3.75360000e+06  3.42056884e-03]\n",
      " [ 5.16531204e+01  1.45686307e+00  2.48500000e+06 -7.09196910e-05]\n",
      " [ 4.65922888e+01  1.46923035e+00  4.45290000e+06 -5.99432914e-03]]\n",
      "Length of inputs 1198\n",
      "length of time-series - inputs (1198, 60, 4)\n",
      "length of time-series - outputs (1198,)\n",
      "Training Size (1198, 60, 4) (1198,)\n",
      "Length of inputs 251\n",
      "length of time-series - inputs (251, 60, 4)\n",
      "length of time-series - outputs (251,)\n",
      "Test Size (251, 60, 4) (251,)\n",
      "Reloading Tuner from .\\untitled_project\\tuner0.json\n",
      "WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_23460\\465918227.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_stock.Date = pd.to_datetime(df_stock.Date)\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\base.py:443: UserWarning: X has feature names, but MinMaxScaler was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).layer_with_weights-1.cell.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).layer_with_weights-1.cell.recurrent_kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).layer_with_weights-1.cell.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.19\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.20\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 60, 92)            35696     \n",
      "                                                                 \n",
      " global_average_pooling1d (  (None, 92)                0         \n",
      " GlobalAveragePooling1D)                                         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 60)                5580      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 20)                1220      \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 20)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 21        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 42517 (166.08 KB)\n",
      "Trainable params: 42517 (166.08 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "JNJ None\n",
      "Epoch 1/10\n",
      "22/24 [==========================>...] - ETA: 0s - loss: 0.2365WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 2s 23ms/step - loss: 0.2193\n",
      "Epoch 2/10\n",
      "22/24 [==========================>...] - ETA: 0s - loss: 0.0433WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 1s 23ms/step - loss: 0.0491\n",
      "Epoch 3/10\n",
      "22/24 [==========================>...] - ETA: 0s - loss: 0.0268WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 1s 23ms/step - loss: 0.0281\n",
      "Epoch 4/10\n",
      "22/24 [==========================>...] - ETA: 0s - loss: 0.0273WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 1s 23ms/step - loss: 0.0283\n",
      "Epoch 5/10\n",
      "22/24 [==========================>...] - ETA: 0s - loss: 0.0286WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 1s 23ms/step - loss: 0.0288\n",
      "Epoch 6/10\n",
      "22/24 [==========================>...] - ETA: 0s - loss: 0.0287WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 1s 23ms/step - loss: 0.0277\n",
      "Epoch 7/10\n",
      "22/24 [==========================>...] - ETA: 0s - loss: 0.0292WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 1s 23ms/step - loss: 0.0284\n",
      "Epoch 8/10\n",
      "22/24 [==========================>...] - ETA: 0s - loss: 0.0280WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 1s 23ms/step - loss: 0.0271\n",
      "Epoch 9/10\n",
      "22/24 [==========================>...] - ETA: 0s - loss: 0.0287WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 1s 23ms/step - loss: 0.0276\n",
      "Epoch 10/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0243WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "24/24 [==============================] - 1s 24ms/step - loss: 0.0243\n",
      "6/6 [==============================] - 0s 7ms/step\n",
      "[[0.37151593]\n",
      " [0.37177086]\n",
      " [0.3720268 ]\n",
      " [0.37241966]\n",
      " [0.3729506 ]\n",
      " [0.37252706]\n",
      " [0.37258726]\n",
      " [0.37295282]\n",
      " [0.3735674 ]\n",
      " [0.37263328]\n",
      " [0.37278903]\n",
      " [0.37268376]\n",
      " [0.37266183]\n",
      " [0.37203455]\n",
      " [0.37231112]\n",
      " [0.37276697]\n",
      " [0.37342584]\n",
      " [0.373428  ]\n",
      " [0.37394243]\n",
      " [0.37439948]\n",
      " [0.37463993]\n",
      " [0.3755194 ]\n",
      " [0.37613046]\n",
      " [0.37725246]\n",
      " [0.37955195]\n",
      " [0.3808856 ]\n",
      " [0.38274348]\n",
      " [0.38498354]\n",
      " [0.3865682 ]\n",
      " [0.3882442 ]\n",
      " [0.38991076]\n",
      " [0.3913651 ]\n",
      " [0.39288676]\n",
      " [0.39422715]\n",
      " [0.3956173 ]\n",
      " [0.3975016 ]\n",
      " [0.3989644 ]\n",
      " [0.40040302]\n",
      " [0.40124458]\n",
      " [0.40252483]\n",
      " [0.4042241 ]\n",
      " [0.40552825]\n",
      " [0.40707475]\n",
      " [0.40828496]\n",
      " [0.40914857]\n",
      " [0.41052562]\n",
      " [0.41148448]\n",
      " [0.41253954]\n",
      " [0.41329372]\n",
      " [0.41402805]\n",
      " [0.4152419 ]\n",
      " [0.41624832]\n",
      " [0.41734666]\n",
      " [0.4184422 ]\n",
      " [0.41951537]\n",
      " [0.4204105 ]\n",
      " [0.42192674]\n",
      " [0.42337537]\n",
      " [0.42436123]\n",
      " [0.42539477]\n",
      " [0.4266296 ]\n",
      " [0.42807126]\n",
      " [0.42926478]\n",
      " [0.43057573]\n",
      " [0.43171477]\n",
      " [0.43333173]\n",
      " [0.4344548 ]\n",
      " [0.43611306]\n",
      " [0.43752587]\n",
      " [0.4387895 ]\n",
      " [0.43975413]\n",
      " [0.44099718]\n",
      " [0.44196117]\n",
      " [0.44270557]\n",
      " [0.44198203]\n",
      " [0.44496953]\n",
      " [0.44617003]\n",
      " [0.44615853]\n",
      " [0.44557488]\n",
      " [0.44512212]\n",
      " [0.4439209 ]\n",
      " [0.44506383]\n",
      " [0.44297504]\n",
      " [0.44239086]\n",
      " [0.44360113]\n",
      " [0.44306093]\n",
      " [0.44210398]\n",
      " [0.44249648]\n",
      " [0.44201517]\n",
      " [0.44131476]\n",
      " [0.44055516]\n",
      " [0.4398886 ]\n",
      " [0.43907607]\n",
      " [0.43793988]\n",
      " [0.43752396]\n",
      " [0.43712878]\n",
      " [0.43666434]\n",
      " [0.43575186]\n",
      " [0.43480492]\n",
      " [0.4338295 ]\n",
      " [0.4334224 ]\n",
      " [0.43345982]\n",
      " [0.43295348]\n",
      " [0.4320873 ]\n",
      " [0.4321866 ]\n",
      " [0.43190002]\n",
      " [0.43101388]\n",
      " [0.43005657]\n",
      " [0.4296878 ]\n",
      " [0.42851478]\n",
      " [0.42807353]\n",
      " [0.42681217]\n",
      " [0.42580205]\n",
      " [0.42574853]\n",
      " [0.42435282]\n",
      " [0.42285287]\n",
      " [0.42263728]\n",
      " [0.4223277 ]\n",
      " [0.42108285]\n",
      " [0.42011124]\n",
      " [0.4189344 ]\n",
      " [0.41752952]\n",
      " [0.4176216 ]\n",
      " [0.4164896 ]\n",
      " [0.41507405]\n",
      " [0.41397786]\n",
      " [0.41374713]\n",
      " [0.4125936 ]\n",
      " [0.41137975]\n",
      " [0.4105522 ]\n",
      " [0.40935957]\n",
      " [0.40814   ]\n",
      " [0.40645987]\n",
      " [0.40524352]\n",
      " [0.40480727]\n",
      " [0.4038301 ]\n",
      " [0.40314078]\n",
      " [0.4024139 ]\n",
      " [0.40208066]\n",
      " [0.40155232]\n",
      " [0.4006526 ]\n",
      " [0.39947778]\n",
      " [0.3990311 ]\n",
      " [0.39794827]\n",
      " [0.39769322]\n",
      " [0.39718908]\n",
      " [0.39596987]\n",
      " [0.39502597]\n",
      " [0.39454228]\n",
      " [0.39400232]\n",
      " [0.39317703]\n",
      " [0.39188433]\n",
      " [0.39126742]\n",
      " [0.3906498 ]\n",
      " [0.38959706]\n",
      " [0.38922638]\n",
      " [0.38831043]\n",
      " [0.38753676]\n",
      " [0.38703227]\n",
      " [0.3858148 ]\n",
      " [0.38523763]\n",
      " [0.38405192]\n",
      " [0.38428003]\n",
      " [0.3830558 ]\n",
      " [0.38320774]\n",
      " [0.38240242]\n",
      " [0.38173944]\n",
      " [0.38148713]\n",
      " [0.38078249]\n",
      " [0.3803147 ]\n",
      " [0.37949854]\n",
      " [0.3793884 ]\n",
      " [0.3789124 ]\n",
      " [0.37821573]\n",
      " [0.37737632]\n",
      " [0.3770677 ]\n",
      " [0.37707305]\n",
      " [0.3761115 ]\n",
      " [0.37560016]\n",
      " [0.37523   ]\n",
      " [0.3743214 ]\n",
      " [0.37371874]\n",
      " [0.37290883]\n",
      " [0.37249875]\n",
      " [0.3717817 ]\n",
      " [0.37193054]\n",
      " [0.3721261 ]\n",
      " [0.37221354]\n",
      " [0.37190568]\n",
      " [0.37185198]\n",
      " [0.37200707]\n",
      " [0.37169558]\n",
      " [0.37225652]\n",
      " [0.37135637]\n",
      " [0.37174428]\n",
      " [0.3724224 ]\n",
      " [0.37145668]\n",
      " [0.37208128]\n",
      " [0.37300313]\n",
      " [0.37369096]\n",
      " [0.37431395]\n",
      " [0.37405026]\n",
      " [0.37496495]\n",
      " [0.3752936 ]\n",
      " [0.37538737]\n",
      " [0.37586802]\n",
      " [0.37647003]\n",
      " [0.3770494 ]\n",
      " [0.37855005]\n",
      " [0.37928623]\n",
      " [0.37997735]\n",
      " [0.38107127]\n",
      " [0.38197953]\n",
      " [0.38273668]\n",
      " [0.3834542 ]\n",
      " [0.38418895]\n",
      " [0.38516116]\n",
      " [0.38586688]\n",
      " [0.38645327]\n",
      " [0.38687897]\n",
      " [0.38676143]\n",
      " [0.38762647]\n",
      " [0.38842845]\n",
      " [0.3889351 ]\n",
      " [0.38920218]\n",
      " [0.389764  ]\n",
      " [0.39113647]\n",
      " [0.3919406 ]\n",
      " [0.39277315]\n",
      " [0.39369124]\n",
      " [0.3950773 ]\n",
      " [0.3957181 ]\n",
      " [0.39651376]\n",
      " [0.39738882]\n",
      " [0.3985802 ]\n",
      " [0.3995793 ]\n",
      " [0.40066195]\n",
      " [0.40177   ]\n",
      " [0.4031917 ]\n",
      " [0.40439183]\n",
      " [0.40545905]\n",
      " [0.40932798]\n",
      " [0.41163993]\n",
      " [0.41368878]\n",
      " [0.41649467]\n",
      " [0.41892815]\n",
      " [0.4210959 ]\n",
      " [0.4233213 ]\n",
      " [0.42536485]\n",
      " [0.42698646]\n",
      " [0.428572  ]]\n",
      "[0.37151593 0.37177086 0.3720268  0.37241966 0.3729506  0.37252706\n",
      " 0.37258726 0.37295282 0.3735674  0.37263328 0.37278903 0.37268376\n",
      " 0.37266183 0.37203455 0.37231112 0.37276697 0.37342584 0.373428\n",
      " 0.37394243 0.37439948 0.37463993 0.3755194  0.37613046 0.37725246\n",
      " 0.37955195 0.3808856  0.38274348 0.38498354 0.3865682  0.3882442\n",
      " 0.38991076 0.3913651  0.39288676 0.39422715 0.3956173  0.3975016\n",
      " 0.3989644  0.40040302 0.40124458 0.40252483 0.4042241  0.40552825\n",
      " 0.40707475 0.40828496 0.40914857 0.41052562 0.41148448 0.41253954\n",
      " 0.41329372 0.41402805 0.4152419  0.41624832 0.41734666 0.4184422\n",
      " 0.41951537 0.4204105  0.42192674 0.42337537 0.42436123 0.42539477\n",
      " 0.4266296  0.42807126 0.42926478 0.43057573 0.43171477 0.43333173\n",
      " 0.4344548  0.43611306 0.43752587 0.4387895  0.43975413 0.44099718\n",
      " 0.44196117 0.44270557 0.44198203 0.44496953 0.44617003 0.44615853\n",
      " 0.44557488 0.44512212 0.4439209  0.44506383 0.44297504 0.44239086\n",
      " 0.44360113 0.44306093 0.44210398 0.44249648 0.44201517 0.44131476\n",
      " 0.44055516 0.4398886  0.43907607 0.43793988 0.43752396 0.43712878\n",
      " 0.43666434 0.43575186 0.43480492 0.4338295  0.4334224  0.43345982\n",
      " 0.43295348 0.4320873  0.4321866  0.43190002 0.43101388 0.43005657\n",
      " 0.4296878  0.42851478 0.42807353 0.42681217 0.42580205 0.42574853\n",
      " 0.42435282 0.42285287 0.42263728 0.4223277  0.42108285 0.42011124\n",
      " 0.4189344  0.41752952 0.4176216  0.4164896  0.41507405 0.41397786\n",
      " 0.41374713 0.4125936  0.41137975 0.4105522  0.40935957 0.40814\n",
      " 0.40645987 0.40524352 0.40480727 0.4038301  0.40314078 0.4024139\n",
      " 0.40208066 0.40155232 0.4006526  0.39947778 0.3990311  0.39794827\n",
      " 0.39769322 0.39718908 0.39596987 0.39502597 0.39454228 0.39400232\n",
      " 0.39317703 0.39188433 0.39126742 0.3906498  0.38959706 0.38922638\n",
      " 0.38831043 0.38753676 0.38703227 0.3858148  0.38523763 0.38405192\n",
      " 0.38428003 0.3830558  0.38320774 0.38240242 0.38173944 0.38148713\n",
      " 0.38078249 0.3803147  0.37949854 0.3793884  0.3789124  0.37821573\n",
      " 0.37737632 0.3770677  0.37707305 0.3761115  0.37560016 0.37523\n",
      " 0.3743214  0.37371874 0.37290883 0.37249875 0.3717817  0.37193054\n",
      " 0.3721261  0.37221354 0.37190568 0.37185198 0.37200707 0.37169558\n",
      " 0.37225652 0.37135637 0.37174428 0.3724224  0.37145668 0.37208128\n",
      " 0.37300313 0.37369096 0.37431395 0.37405026 0.37496495 0.3752936\n",
      " 0.37538737 0.37586802 0.37647003 0.3770494  0.37855005 0.37928623\n",
      " 0.37997735 0.38107127 0.38197953 0.38273668 0.3834542  0.38418895\n",
      " 0.38516116 0.38586688 0.38645327 0.38687897 0.38676143 0.38762647\n",
      " 0.38842845 0.3889351  0.38920218 0.389764   0.39113647 0.3919406\n",
      " 0.39277315 0.39369124 0.3950773  0.3957181  0.39651376 0.39738882\n",
      " 0.3985802  0.3995793  0.40066195 0.40177    0.4031917  0.40439183\n",
      " 0.40545905 0.40932798 0.41163993 0.41368878 0.41649467 0.41892815\n",
      " 0.4210959  0.4233213  0.42536485 0.42698646 0.428572  ]\n",
      "JNJ 0.30087121812507783\n"
     ]
    }
   ],
   "source": [
    "mse = {}\n",
    "total_predicted_returns = pd.DataFrame()\n",
    "for ticker in stocks:\n",
    "    df = yf.download(ticker, start='2000-01-01', end=end_date, interval='1d')\n",
    "    df['RSI'] = calculate_rsi(df['Adj Close'])\n",
    "    df['ATR'] = calculate_atr(df[['High', 'Low', 'Close']])\n",
    "    stock_returns = returns[[ticker]].reset_index()\n",
    "    stock_returns['Date'] = stock_returns['Date'].apply(lambda x: x.strftime('%Y-%m-%d'))\n",
    "    df.reset_index(inplace=True)\n",
    "    df_stock = df[['RSI', 'ATR', 'Date', 'Volume']]\n",
    "    stock_returns.Date = pd.to_datetime(stock_returns.Date)\n",
    "    df_stock.Date = pd.to_datetime(df_stock.Date)\n",
    "    data = pd.merge(df_stock[['RSI', 'ATR', 'Date', 'Volume']],  stock_returns , on='Date')[1:].rename(columns={ticker : 'Returns'})\n",
    "    print(data)\n",
    "    # Split the data we try by years first\n",
    "    data = data[data['Date'] < '2019-01-01']\n",
    "    df_train = data[data['Date'] < '2018-01-01']\n",
    "    predicted_period = data[(data['Date'] >= '2018-01-01') & (data['Date'] < '2019-01-01')][['Date']]\n",
    "    print(len(data) - len(predicted_period) - 60)\n",
    "    df_test = data[len(data) - len(predicted_period) - 60:]\n",
    "    train_cols = [\"RSI\", \"ATR\", \"Volume\", \"Returns\"]\n",
    "    x = df_train[train_cols].values\n",
    "    print(x)\n",
    "    \n",
    "    #scaling\n",
    "    min_max_scaler = MinMaxScaler(feature_range = (0, 1))\n",
    "    x_train = min_max_scaler.fit_transform(x)\n",
    "    x_test = min_max_scaler.transform(df_test[train_cols])\n",
    "    \n",
    "    x_t, y_t = build_timeseries(x_train, 1)\n",
    "    print(\"Training Size\", x_t.shape, y_t.shape)\n",
    "    \n",
    "    x_t_test, y_t_test = build_timeseries(x_test, 1)\n",
    "    print(\"Test Size\", x_t_test.shape, y_t_test.shape)\n",
    "    \n",
    "    x_left, x_val = train_test_split(x_t, test_size=0.2, shuffle=False)\n",
    "    y_left, y_val = train_test_split(y_t, test_size=0.2, shuffle=False)\n",
    "    \n",
    "    tuner = kt.BayesianOptimization(\n",
    "        model_builder,\n",
    "        objective='val_loss',\n",
    "        max_trials=5)\n",
    "    tuner.search(x_t, y_t, epochs=5, validation_data=(x_val,y_val))\n",
    "    lstm_model  = tuner.get_best_models()[0]  \n",
    "    print(ticker,lstm_model.summary())\n",
    "    callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\n",
    "    #Model Training\n",
    "    history_lstm = lstm_model.fit(x_t, y_t, epochs=params[\"EPOCHS\"], verbose=1, batch_size=BATCH_SIZE, callbacks=[callback],\n",
    "                                shuffle=False)\n",
    "    y_pred_lstm = lstm_model.predict(x_t_test, batch_size=BATCH_SIZE)\n",
    "    print(y_pred_lstm)\n",
    "    y_pred_lstm = y_pred_lstm.flatten()\n",
    "    print(y_pred_lstm)\n",
    "    error_lstm = mean_squared_error(y_t_test, y_pred_lstm)\n",
    "    mse[ticker] = error_lstm\n",
    "    print(ticker,error_lstm)\n",
    "    \n",
    "    y_pred_lstm_org = (y_pred_lstm * min_max_scaler.data_range_[1]) + min_max_scaler.data_min_[1]   #Inverse Transform \n",
    "    predicted_returns = pd.Series(y_pred_lstm_org).to_frame(ticker)\n",
    "    predicted_period = predicted_period.reset_index().drop(columns=['index'])\n",
    "    result = predicted_returns.join(predicted_period)\n",
    "    if  total_predicted_returns.empty:\n",
    "        total_predicted_returns= result \n",
    "    else: \n",
    "        total_predicted_returns = pd.merge(result, total_predicted_returns, on='Date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a0ac1736",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>JNJ</th>\n",
       "      <th>Date</th>\n",
       "      <th>NVO</th>\n",
       "      <th>WMT</th>\n",
       "      <th>JPM</th>\n",
       "      <th>XOM</th>\n",
       "      <th>UNH</th>\n",
       "      <th>V</th>\n",
       "      <th>TSM</th>\n",
       "      <th>LLY</th>\n",
       "      <th>HSBC</th>\n",
       "      <th>META</th>\n",
       "      <th>TSLA</th>\n",
       "      <th>NVDA</th>\n",
       "      <th>AMZN</th>\n",
       "      <th>GOOGL</th>\n",
       "      <th>GOOG</th>\n",
       "      <th>MSFT</th>\n",
       "      <th>AAPL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.274866</td>\n",
       "      <td>2018-01-02</td>\n",
       "      <td>0.358531</td>\n",
       "      <td>1.163523</td>\n",
       "      <td>1.259298</td>\n",
       "      <td>1.203410</td>\n",
       "      <td>2.03171</td>\n",
       "      <td>1.141620</td>\n",
       "      <td>0.507688</td>\n",
       "      <td>1.366064</td>\n",
       "      <td>0.493111</td>\n",
       "      <td>2.208042</td>\n",
       "      <td>0.540090</td>\n",
       "      <td>0.364042</td>\n",
       "      <td>0.571188</td>\n",
       "      <td>0.579567</td>\n",
       "      <td>0.537375</td>\n",
       "      <td>0.945141</td>\n",
       "      <td>0.506326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.275352</td>\n",
       "      <td>2018-01-03</td>\n",
       "      <td>0.358587</td>\n",
       "      <td>1.165196</td>\n",
       "      <td>1.259418</td>\n",
       "      <td>1.203309</td>\n",
       "      <td>2.03171</td>\n",
       "      <td>1.141765</td>\n",
       "      <td>0.507748</td>\n",
       "      <td>1.366184</td>\n",
       "      <td>0.492839</td>\n",
       "      <td>2.208081</td>\n",
       "      <td>0.539703</td>\n",
       "      <td>0.364003</td>\n",
       "      <td>0.571188</td>\n",
       "      <td>0.579567</td>\n",
       "      <td>0.537229</td>\n",
       "      <td>0.945242</td>\n",
       "      <td>0.505884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.275841</td>\n",
       "      <td>2018-01-04</td>\n",
       "      <td>0.358572</td>\n",
       "      <td>1.166685</td>\n",
       "      <td>1.259448</td>\n",
       "      <td>1.203418</td>\n",
       "      <td>2.03171</td>\n",
       "      <td>1.141832</td>\n",
       "      <td>0.508129</td>\n",
       "      <td>1.366255</td>\n",
       "      <td>0.492638</td>\n",
       "      <td>2.208094</td>\n",
       "      <td>0.542940</td>\n",
       "      <td>0.363995</td>\n",
       "      <td>0.571188</td>\n",
       "      <td>0.579567</td>\n",
       "      <td>0.537173</td>\n",
       "      <td>0.945296</td>\n",
       "      <td>0.505599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.276590</td>\n",
       "      <td>2018-01-05</td>\n",
       "      <td>0.358574</td>\n",
       "      <td>1.167246</td>\n",
       "      <td>1.259383</td>\n",
       "      <td>1.203410</td>\n",
       "      <td>2.03171</td>\n",
       "      <td>1.142026</td>\n",
       "      <td>0.508063</td>\n",
       "      <td>1.366424</td>\n",
       "      <td>0.492708</td>\n",
       "      <td>2.208059</td>\n",
       "      <td>0.538577</td>\n",
       "      <td>0.364034</td>\n",
       "      <td>0.571188</td>\n",
       "      <td>0.579567</td>\n",
       "      <td>0.537259</td>\n",
       "      <td>0.945137</td>\n",
       "      <td>0.505937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.277604</td>\n",
       "      <td>2018-01-08</td>\n",
       "      <td>0.358593</td>\n",
       "      <td>1.166942</td>\n",
       "      <td>1.258220</td>\n",
       "      <td>1.203400</td>\n",
       "      <td>2.03171</td>\n",
       "      <td>1.141910</td>\n",
       "      <td>0.508152</td>\n",
       "      <td>1.366529</td>\n",
       "      <td>0.492614</td>\n",
       "      <td>2.207635</td>\n",
       "      <td>0.539410</td>\n",
       "      <td>0.363993</td>\n",
       "      <td>0.571188</td>\n",
       "      <td>0.579567</td>\n",
       "      <td>0.537431</td>\n",
       "      <td>0.945045</td>\n",
       "      <td>0.505391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>1.369481</td>\n",
       "      <td>2018-12-24</td>\n",
       "      <td>0.359139</td>\n",
       "      <td>1.208380</td>\n",
       "      <td>1.254396</td>\n",
       "      <td>1.204084</td>\n",
       "      <td>2.03171</td>\n",
       "      <td>1.141326</td>\n",
       "      <td>0.505509</td>\n",
       "      <td>1.368345</td>\n",
       "      <td>0.508687</td>\n",
       "      <td>2.215090</td>\n",
       "      <td>0.551294</td>\n",
       "      <td>0.364275</td>\n",
       "      <td>0.571188</td>\n",
       "      <td>0.579567</td>\n",
       "      <td>0.666979</td>\n",
       "      <td>0.947111</td>\n",
       "      <td>0.512949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>1.373728</td>\n",
       "      <td>2018-12-26</td>\n",
       "      <td>0.359121</td>\n",
       "      <td>1.210717</td>\n",
       "      <td>1.252049</td>\n",
       "      <td>1.204049</td>\n",
       "      <td>2.03171</td>\n",
       "      <td>1.141371</td>\n",
       "      <td>0.504897</td>\n",
       "      <td>1.368076</td>\n",
       "      <td>0.508736</td>\n",
       "      <td>2.216480</td>\n",
       "      <td>0.555004</td>\n",
       "      <td>0.364307</td>\n",
       "      <td>0.571188</td>\n",
       "      <td>0.579567</td>\n",
       "      <td>0.668048</td>\n",
       "      <td>0.947098</td>\n",
       "      <td>0.512787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>1.377628</td>\n",
       "      <td>2018-12-27</td>\n",
       "      <td>0.359171</td>\n",
       "      <td>1.213619</td>\n",
       "      <td>1.255085</td>\n",
       "      <td>1.204190</td>\n",
       "      <td>2.03171</td>\n",
       "      <td>1.141156</td>\n",
       "      <td>0.505430</td>\n",
       "      <td>1.367578</td>\n",
       "      <td>0.509059</td>\n",
       "      <td>2.216615</td>\n",
       "      <td>0.548919</td>\n",
       "      <td>0.364326</td>\n",
       "      <td>0.571188</td>\n",
       "      <td>0.579567</td>\n",
       "      <td>0.669074</td>\n",
       "      <td>0.947408</td>\n",
       "      <td>0.512722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>1.380723</td>\n",
       "      <td>2018-12-28</td>\n",
       "      <td>0.359253</td>\n",
       "      <td>1.216321</td>\n",
       "      <td>1.254461</td>\n",
       "      <td>1.204152</td>\n",
       "      <td>2.03171</td>\n",
       "      <td>1.141121</td>\n",
       "      <td>0.504418</td>\n",
       "      <td>1.367179</td>\n",
       "      <td>0.509189</td>\n",
       "      <td>2.216569</td>\n",
       "      <td>0.556297</td>\n",
       "      <td>0.364336</td>\n",
       "      <td>0.571188</td>\n",
       "      <td>0.579567</td>\n",
       "      <td>0.669989</td>\n",
       "      <td>0.947191</td>\n",
       "      <td>0.512639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250</th>\n",
       "      <td>1.383748</td>\n",
       "      <td>2018-12-31</td>\n",
       "      <td>0.359190</td>\n",
       "      <td>1.218289</td>\n",
       "      <td>1.254645</td>\n",
       "      <td>1.204050</td>\n",
       "      <td>2.03171</td>\n",
       "      <td>1.141005</td>\n",
       "      <td>0.504968</td>\n",
       "      <td>1.367136</td>\n",
       "      <td>0.509691</td>\n",
       "      <td>2.216103</td>\n",
       "      <td>0.555784</td>\n",
       "      <td>0.364337</td>\n",
       "      <td>0.571188</td>\n",
       "      <td>0.579567</td>\n",
       "      <td>0.670146</td>\n",
       "      <td>0.947067</td>\n",
       "      <td>0.512486</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>251 rows × 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          JNJ       Date       NVO       WMT       JPM       XOM      UNH  \\\n",
       "0    1.274866 2018-01-02  0.358531  1.163523  1.259298  1.203410  2.03171   \n",
       "1    1.275352 2018-01-03  0.358587  1.165196  1.259418  1.203309  2.03171   \n",
       "2    1.275841 2018-01-04  0.358572  1.166685  1.259448  1.203418  2.03171   \n",
       "3    1.276590 2018-01-05  0.358574  1.167246  1.259383  1.203410  2.03171   \n",
       "4    1.277604 2018-01-08  0.358593  1.166942  1.258220  1.203400  2.03171   \n",
       "..        ...        ...       ...       ...       ...       ...      ...   \n",
       "246  1.369481 2018-12-24  0.359139  1.208380  1.254396  1.204084  2.03171   \n",
       "247  1.373728 2018-12-26  0.359121  1.210717  1.252049  1.204049  2.03171   \n",
       "248  1.377628 2018-12-27  0.359171  1.213619  1.255085  1.204190  2.03171   \n",
       "249  1.380723 2018-12-28  0.359253  1.216321  1.254461  1.204152  2.03171   \n",
       "250  1.383748 2018-12-31  0.359190  1.218289  1.254645  1.204050  2.03171   \n",
       "\n",
       "            V       TSM       LLY      HSBC      META      TSLA      NVDA  \\\n",
       "0    1.141620  0.507688  1.366064  0.493111  2.208042  0.540090  0.364042   \n",
       "1    1.141765  0.507748  1.366184  0.492839  2.208081  0.539703  0.364003   \n",
       "2    1.141832  0.508129  1.366255  0.492638  2.208094  0.542940  0.363995   \n",
       "3    1.142026  0.508063  1.366424  0.492708  2.208059  0.538577  0.364034   \n",
       "4    1.141910  0.508152  1.366529  0.492614  2.207635  0.539410  0.363993   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "246  1.141326  0.505509  1.368345  0.508687  2.215090  0.551294  0.364275   \n",
       "247  1.141371  0.504897  1.368076  0.508736  2.216480  0.555004  0.364307   \n",
       "248  1.141156  0.505430  1.367578  0.509059  2.216615  0.548919  0.364326   \n",
       "249  1.141121  0.504418  1.367179  0.509189  2.216569  0.556297  0.364336   \n",
       "250  1.141005  0.504968  1.367136  0.509691  2.216103  0.555784  0.364337   \n",
       "\n",
       "         AMZN     GOOGL      GOOG      MSFT      AAPL  \n",
       "0    0.571188  0.579567  0.537375  0.945141  0.506326  \n",
       "1    0.571188  0.579567  0.537229  0.945242  0.505884  \n",
       "2    0.571188  0.579567  0.537173  0.945296  0.505599  \n",
       "3    0.571188  0.579567  0.537259  0.945137  0.505937  \n",
       "4    0.571188  0.579567  0.537431  0.945045  0.505391  \n",
       "..        ...       ...       ...       ...       ...  \n",
       "246  0.571188  0.579567  0.666979  0.947111  0.512949  \n",
       "247  0.571188  0.579567  0.668048  0.947098  0.512787  \n",
       "248  0.571188  0.579567  0.669074  0.947408  0.512722  \n",
       "249  0.571188  0.579567  0.669989  0.947191  0.512639  \n",
       "250  0.571188  0.579567  0.670146  0.947067  0.512486  \n",
       "\n",
       "[251 rows x 19 columns]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_predicted_returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7083eac8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
